diff --git a/examples/genericsect_bow_random/genericsect_bow.py b/examples/genericsect_bow_random/genericsect_bow.py
new file mode 100644
index 0000000..a4817f4
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow.py
@@ -0,0 +1,102 @@
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
+import sciwing.constants as constants
+import os
+import torch.optim as optim
+from sciwing.engine.engine import Engine
+import json
+import argparse
+import torch.nn as nn
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+if __name__ == "__main__":
+    # read the hyperparams from config file
+    parser = argparse.ArgumentParser(
+        description="Bag of words linear classifier. "
+        "with initial random word embeddings"
+    )
+
+    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
+    parser.add_argument("--bs", help="batch size", type=int)
+    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
+    parser.add_argument("--emb_type", help="embedding type", type=str)
+    parser.add_argument("--lr", help="learning rate", type=float)
+    parser.add_argument("--epochs", help="number of epochs", type=int)
+    parser.add_argument(
+        "--save_every", help="Save the model every few epochs", type=int
+    )
+    parser.add_argument(
+        "--log_train_metrics_every",
+        help="Log training metrics every few iterations",
+        type=int,
+    )
+
+    parser.add_argument(
+        "--exp_dir_path", help="Directory to store all experiment related information"
+    )
+    parser.add_argument(
+        "--model_save_dir",
+        help="Directory where the checkpoints during model training are stored.",
+    )
+    parser.add_argument(
+        "--sample_proportion", help="Sample proportion for the dataset", type=float
+    )
+
+    args = parser.parse_args()
+
+    DATA_DIR = pathlib.Path(DATA_DIR)
+    train_filename = DATA_DIR.joinpath("genericSect.train")
+    dev_filename = DATA_DIR.joinpath("genericSect.dev")
+    test_filename = DATA_DIR.joinpath("genericSect.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=train_filename,
+        dev_filename=dev_filename,
+        test_filename=test_filename,
+    )
+
+    embedder = WordEmbedder(embedding_type=args.emb_type)
+    encoder = BOW_Encoder(embedder=embedder, dropout_value=0.0, aggregation_type="sum",)
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=50,
+        num_classes=12,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+
+    engine = Engine(
+        datasets_manager=data_manager,
+        model=model,
+        optimizer=optimizer,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
+        use_wandb=True,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
+        track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
+    )
+
+    engine.run()
diff --git a/examples/genericsect_bow_random/genericsect_bow.sh b/examples/genericsect_bow_random/genericsect_bow.sh
new file mode 100755
index 0000000..c40b9ca
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+
+SCRIPT_FILE="genericsect_bow.py"
+
+python ${SCRIPT_FILE} \
+--exp_name "genericsect_bow_random" \
+--exp_dir_path  "./output" \
+--model_save_dir "./output/checkpoints" \
+--bs 32 \
+--emb_type "glove_6B_50" \
+--lr 1e-4 \
+--epochs 1 \
+--save_every 1 \
+--log_train_metrics_every 50 \
+--sample_proportion 0.01
\ No newline at end of file
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.toml b/examples/genericsect_bow_random/genericsect_bow.toml
similarity index 57%
rename from examples/genericsect_bow_random/genericsect_bow_random.toml
rename to examples/genericsect_bow_random/genericsect_bow.toml
index 251f06a..457e94e 100644
--- a/examples/genericsect_bow_random/genericsect_bow_random.toml
+++ b/examples/genericsect_bow_random/genericsect_bow.toml
@@ -3,33 +3,24 @@
     exp_dir = "genericsect_bow_random_toml"
 
 [dataset]
-	class = "GenericSectDataset"
-	train_filename="genericSect.train.data"
-	valid_filename="genericSect.train.data"
-	test_filename="genericSect.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "genericsect_bow_random_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="genericSect.train"
+	dev_filename="genericSect.dev"
+	test_filename="genericSect.test"
 
 [model]
     class="SimpleClassifier"
-    encoding_dim=300
+    encoding_dim=50
     num_classes=12
     classification_layer_bias=true
     [model.encoder]
-        emb_dim = 300
         class="BOW_Encoder"
         dropout_value = 0.5
         aggregation_type="sum"
         [[model.encoder.embedder]]
         class="WordEmbedder"
-        embed="word_vocab"
-        freeze=false
+        embedding_type="glove_6B_50"
+
 
 [engine]
     batch_size=32
@@ -39,6 +30,7 @@
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.01
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/genericsect_bow_random/genericsect_bow_glove.toml b/examples/genericsect_bow_random/genericsect_bow_glove.toml
deleted file mode 100644
index 56754b3..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_glove.toml
+++ /dev/null
@@ -1,46 +0,0 @@
-[experiment]
-    exp_name = "genericsect-bow-random"
-    exp_dir = "genericsect_bow_glove_toml"
-
-[dataset]
-	class = "GenericSectDataset"
-	train_filename="genericSect.train.data"
-	valid_filename="genericSect.train.data"
-	test_filename="genericSect.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "genericsect_bow_glove_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "glove_6B_300"
-	word_embedding_dimension = 300
-
-[model]
-    class="SimpleClassifier"
-    encoding_dim=300
-    num_classes=12
-    classification_layer_bias=true
-    [model.encoder]
-        emb_dim = 300
-        class="BOW_Encoder"
-        dropout_value = 0.5
-        aggregation_type="sum"
-        [[model.encoder.embedder]]
-        class="WordEmbedder"
-        embed="word_vocab"
-        freeze=false
-
-[engine]
-    batch_size=32
-    save_dir="genericsect_bow_glove_toml/checkpoints"
-    num_epochs=1
-    save_every=10
-    log_train_metrics_every=10
-    device="cpu"
-    gradient_norm_clip_value=5.0
-    [engine.metric]
-        class="PrecisionRecallFMeasure"
-    [engine.optimizer]
-        class="Adam"
-        lr=1e-3
\ No newline at end of file
diff --git a/examples/genericsect_bow_random/genericsect_bow_infer.py b/examples/genericsect_bow_random/genericsect_bow_infer.py
new file mode 100644
index 0000000..6906ef4
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow_infer.py
@@ -0,0 +1,56 @@
+import sciwing.constants as constants
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+DATA_DIR = pathlib.Path(DATA_DIR)
+
+
+def build_genericsect_bow_glove_model(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    train_filename = DATA_DIR.joinpath("genericSect.train")
+    dev_filename = DATA_DIR.joinpath("genericSect.dev")
+    test_filename = DATA_DIR.joinpath("genericSect.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=train_filename,
+        dev_filename=dev_filename,
+        test_filename=test_filename,
+    )
+
+    EMBEDDING_TYPE = "glove_6B_50"
+    embedder = WordEmbedder(embedding_type=EMBEDDING_TYPE)
+    encoder = BOW_Encoder(embedder=embedder, dropout_value=0.0, aggregation_type="sum",)
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=50,
+        num_classes=12,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    inference = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+
+    return inference
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_genericsect_bow_glove_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.py b/examples/genericsect_bow_random/genericsect_bow_random.py
deleted file mode 100644
index 6d50017..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_random.py
+++ /dev/null
@@ -1,216 +0,0 @@
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
-from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-import sciwing.constants as constants
-import os
-import torch.optim as optim
-from sciwing.engine.engine import Engine
-import json
-import argparse
-import torch.nn as nn
-import pathlib
-
-FILES = constants.FILES
-PATHS = constants.PATHS
-
-GENERIC_SECTION_TRAIN_FILE = FILES["GENERIC_SECTION_TRAIN_FILE"]
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-
-if __name__ == "__main__":
-    # read the hyperparams from config file
-    parser = argparse.ArgumentParser(
-        description="Bag of words linear classifier. "
-        "with initial random word embeddings"
-    )
-
-    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-    parser.add_argument(
-        "--max_length", help="Maximum length of every sentence", type=int
-    )
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
-    parser.add_argument("--bs", help="batch size", type=int)
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
-    parser.add_argument("--emb_type", help="embedding type", type=str)
-    parser.add_argument("--lr", help="learning rate", type=float)
-    parser.add_argument("--epochs", help="number of epochs", type=int)
-    parser.add_argument(
-        "--save_every", help="Save the model every few epochs", type=int
-    )
-    parser.add_argument(
-        "--log_train_metrics_every",
-        help="Log training metrics every few iterations",
-        type=int,
-    )
-
-    parser.add_argument(
-        "--exp_dir_path", help="Directory to store all experiment related information"
-    )
-    parser.add_argument(
-        "--model_save_dir",
-        help="Directory where the checkpoints during model training are stored.",
-    )
-    parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
-    )
-
-    args = parser.parse_args()
-    config = {
-        "EXP_NAME": args.exp_name,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "MAX_LENGTH": args.max_length,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "EMBEDDING_TYPE": args.emb_type,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    EXP_NAME = config["EXP_NAME"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    GENERIC_SECTION_TRAIN_FILE = "genericSect.train.data"
-
-    train_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset_params = {
-        "filename": GENERIC_SECTION_TRAIN_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    random_embeddings = train_dataset.word_vocab.load_embedding()
-    embedding = nn.Embedding.from_pretrained(random_embeddings, freeze=False)
-
-    embedder = WordEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        dropout_value=0.0,
-        aggregation_type="sum",
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
-
-    engine = Engine(
-        model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
-        optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        metric=metric,
-        use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
-        track_for_best="macro_fscore",
-    )
-
-    engine.run()
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.sh b/examples/genericsect_bow_random/genericsect_bow_random.sh
deleted file mode 100755
index 7614444..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_random.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/usr/bin/env bash
-
-SCRIPT_FILE="genericsect_bow_random.py"
-
-python ${SCRIPT_FILE} \
---exp_name "genericsect_bow_random" \
---exp_dir_path  "./output" \
---model_save_dir "./output/checkpoints" \
---vocab_store_location "./output/vocab.json" \
---max_num_words 3000 \
---max_length 15 \
---debug \
---debug_dataset_proportion 0.01 \
---bs 32 \
---emb_type random \
---emb_dim 50 \
---lr 1e-4 \
---epochs 15 \
---save_every 1 \
---log_train_metrics_every 50
\ No newline at end of file
diff --git a/sciwing/infer/bow_lc_gensect_infer.py b/sciwing/infer/bow_lc_gensect_infer.py
deleted file mode 100644
index f421acb..0000000
--- a/sciwing/infer/bow_lc_gensect_infer.py
+++ /dev/null
@@ -1,61 +0,0 @@
-import json
-import os
-import sciwing.constants as constants
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
-import torch.nn as nn
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-GENERIC_SECTION_TRAIN_FILE = FILES["GENERIC_SECTION_TRAIN_FILE"]
-
-
-def get_bow_lc_gensect_infer(dirname: str):
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-    VOCAB_SIZE = config["VOCAB_SIZE"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)
-    embedder = WordEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        dropout_value=0.0,
-        aggregation_type="sum",
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = GenericSectDataset(**test_dataset_args)
-
-    parsect_inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return parsect_inference

diff --git a/.gitignore b/.gitignore
index 492a86a..6f1a5ec 100644
--- a/.gitignore
+++ b/.gitignore
@@ -159,9 +159,6 @@ Pyre type checker
 .idea
 
 sciwing/vis/*
-sciwing/data/*
-.idea
-*.egg-info
 sciwing/outputs/*
 tests/utils/dummy_folder/*
 tests/utils/dummy_file.txt
diff --git a/README.md b/README.md
index ce3e2c1..44f89ce 100644
--- a/README.md
+++ b/README.md
@@ -50,7 +50,7 @@ ELMO_EMBEDDING_DIMENSION = 1024
 embedding = dataset.word_vocab.load_embedding()
 # initialize a normal embedder with the word embedding 
 # EMBEDDING_DIM is the embedding dimension for the word vectors
-vanilla_embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIM)
+vanilla_embedder = WordEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIM)
 
 # concatenate the vanilla embedding and the elmo embedding to get a new embedding
 final_embedder = ConcatEmbedders([vanilla_embedder, elmo_embedder])
diff --git a/examples/genericsect_bilstm/genericsect_bilstm.py b/examples/genericsect_bilstm/genericsect_bilstm.py
index a80de01..033fb41 100644
--- a/examples/genericsect_bilstm/genericsect_bilstm.py
+++ b/examples/genericsect_bilstm/genericsect_bilstm.py
@@ -1,7 +1,7 @@
 from sciwing.models.simpleclassifier import SimpleClassifier
 from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 import sciwing.constants as constants
 import os
 import torch.nn as nn
@@ -209,7 +209,7 @@ if __name__ == "__main__":
     random_embeddings = train_dataset.word_vocab.load_embedding()
     random_embeddings = nn.Embedding.from_pretrained(random_embeddings, freeze=False)
 
-    embedder = VanillaEmbedder(
+    embedder = WordEmbedder(
         embedding_dim=EMBEDDING_DIMENSION, embedding=random_embeddings
     )
     encoder = LSTM2VecEncoder(
diff --git a/examples/genericsect_bilstm/genericsect_bilstm.toml b/examples/genericsect_bilstm/genericsect_bilstm.toml
index b1f6040..fdc29a6 100644
--- a/examples/genericsect_bilstm/genericsect_bilstm.toml
+++ b/examples/genericsect_bilstm/genericsect_bilstm.toml
@@ -30,7 +30,7 @@
         device="cpu"
         hidden_dim=100
         [[model.encoder.embedder]]
-        class="VanillaEmbedder"
+        class="WordEmbedder"
         embed="word_vocab"
         freeze=false
 
diff --git a/examples/genericsect_bow_random/genericsect_bow.py b/examples/genericsect_bow_random/genericsect_bow.py
new file mode 100644
index 0000000..a4817f4
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow.py
@@ -0,0 +1,102 @@
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
+import sciwing.constants as constants
+import os
+import torch.optim as optim
+from sciwing.engine.engine import Engine
+import json
+import argparse
+import torch.nn as nn
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+if __name__ == "__main__":
+    # read the hyperparams from config file
+    parser = argparse.ArgumentParser(
+        description="Bag of words linear classifier. "
+        "with initial random word embeddings"
+    )
+
+    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
+    parser.add_argument("--bs", help="batch size", type=int)
+    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
+    parser.add_argument("--emb_type", help="embedding type", type=str)
+    parser.add_argument("--lr", help="learning rate", type=float)
+    parser.add_argument("--epochs", help="number of epochs", type=int)
+    parser.add_argument(
+        "--save_every", help="Save the model every few epochs", type=int
+    )
+    parser.add_argument(
+        "--log_train_metrics_every",
+        help="Log training metrics every few iterations",
+        type=int,
+    )
+
+    parser.add_argument(
+        "--exp_dir_path", help="Directory to store all experiment related information"
+    )
+    parser.add_argument(
+        "--model_save_dir",
+        help="Directory where the checkpoints during model training are stored.",
+    )
+    parser.add_argument(
+        "--sample_proportion", help="Sample proportion for the dataset", type=float
+    )
+
+    args = parser.parse_args()
+
+    DATA_DIR = pathlib.Path(DATA_DIR)
+    train_filename = DATA_DIR.joinpath("genericSect.train")
+    dev_filename = DATA_DIR.joinpath("genericSect.dev")
+    test_filename = DATA_DIR.joinpath("genericSect.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=train_filename,
+        dev_filename=dev_filename,
+        test_filename=test_filename,
+    )
+
+    embedder = WordEmbedder(embedding_type=args.emb_type)
+    encoder = BOW_Encoder(embedder=embedder, dropout_value=0.0, aggregation_type="sum",)
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=50,
+        num_classes=12,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+
+    engine = Engine(
+        datasets_manager=data_manager,
+        model=model,
+        optimizer=optimizer,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
+        use_wandb=True,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
+        track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
+    )
+
+    engine.run()
diff --git a/examples/genericsect_bow_random/genericsect_bow.sh b/examples/genericsect_bow_random/genericsect_bow.sh
new file mode 100755
index 0000000..c40b9ca
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+
+SCRIPT_FILE="genericsect_bow.py"
+
+python ${SCRIPT_FILE} \
+--exp_name "genericsect_bow_random" \
+--exp_dir_path  "./output" \
+--model_save_dir "./output/checkpoints" \
+--bs 32 \
+--emb_type "glove_6B_50" \
+--lr 1e-4 \
+--epochs 1 \
+--save_every 1 \
+--log_train_metrics_every 50 \
+--sample_proportion 0.01
\ No newline at end of file
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.toml b/examples/genericsect_bow_random/genericsect_bow.toml
similarity index 54%
rename from examples/genericsect_bow_random/genericsect_bow_random.toml
rename to examples/genericsect_bow_random/genericsect_bow.toml
index 0ef759d..457e94e 100644
--- a/examples/genericsect_bow_random/genericsect_bow_random.toml
+++ b/examples/genericsect_bow_random/genericsect_bow.toml
@@ -3,33 +3,24 @@
     exp_dir = "genericsect_bow_random_toml"
 
 [dataset]
-	class = "GenericSectDataset"
-	train_filename="genericSect.train.data"
-	valid_filename="genericSect.train.data"
-	test_filename="genericSect.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "genericsect_bow_random_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="genericSect.train"
+	dev_filename="genericSect.dev"
+	test_filename="genericSect.test"
 
 [model]
     class="SimpleClassifier"
-    encoding_dim=300
+    encoding_dim=50
     num_classes=12
     classification_layer_bias=true
     [model.encoder]
-        emb_dim = 300
         class="BOW_Encoder"
         dropout_value = 0.5
         aggregation_type="sum"
         [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
+        class="WordEmbedder"
+        embedding_type="glove_6B_50"
+
 
 [engine]
     batch_size=32
@@ -39,6 +30,7 @@
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.01
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/genericsect_bow_random/genericsect_bow_glove.toml b/examples/genericsect_bow_random/genericsect_bow_glove.toml
deleted file mode 100644
index 563220f..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_glove.toml
+++ /dev/null
@@ -1,46 +0,0 @@
-[experiment]
-    exp_name = "genericsect-bow-random"
-    exp_dir = "genericsect_bow_glove_toml"
-
-[dataset]
-	class = "GenericSectDataset"
-	train_filename="genericSect.train.data"
-	valid_filename="genericSect.train.data"
-	test_filename="genericSect.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "genericsect_bow_glove_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "glove_6B_300"
-	word_embedding_dimension = 300
-
-[model]
-    class="SimpleClassifier"
-    encoding_dim=300
-    num_classes=12
-    classification_layer_bias=true
-    [model.encoder]
-        emb_dim = 300
-        class="BOW_Encoder"
-        dropout_value = 0.5
-        aggregation_type="sum"
-        [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
-
-[engine]
-    batch_size=32
-    save_dir="genericsect_bow_glove_toml/checkpoints"
-    num_epochs=1
-    save_every=10
-    log_train_metrics_every=10
-    device="cpu"
-    gradient_norm_clip_value=5.0
-    [engine.metric]
-        class="PrecisionRecallFMeasure"
-    [engine.optimizer]
-        class="Adam"
-        lr=1e-3
\ No newline at end of file
diff --git a/examples/genericsect_bow_random/genericsect_bow_infer.py b/examples/genericsect_bow_random/genericsect_bow_infer.py
new file mode 100644
index 0000000..6906ef4
--- /dev/null
+++ b/examples/genericsect_bow_random/genericsect_bow_infer.py
@@ -0,0 +1,56 @@
+import sciwing.constants as constants
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+DATA_DIR = pathlib.Path(DATA_DIR)
+
+
+def build_genericsect_bow_glove_model(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    train_filename = DATA_DIR.joinpath("genericSect.train")
+    dev_filename = DATA_DIR.joinpath("genericSect.dev")
+    test_filename = DATA_DIR.joinpath("genericSect.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=train_filename,
+        dev_filename=dev_filename,
+        test_filename=test_filename,
+    )
+
+    EMBEDDING_TYPE = "glove_6B_50"
+    embedder = WordEmbedder(embedding_type=EMBEDDING_TYPE)
+    encoder = BOW_Encoder(embedder=embedder, dropout_value=0.0, aggregation_type="sum",)
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=50,
+        num_classes=12,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    inference = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+
+    return inference
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_genericsect_bow_glove_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.py b/examples/genericsect_bow_random/genericsect_bow_random.py
deleted file mode 100644
index be70a9e..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_random.py
+++ /dev/null
@@ -1,216 +0,0 @@
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-import sciwing.constants as constants
-import os
-import torch.optim as optim
-from sciwing.engine.engine import Engine
-import json
-import argparse
-import torch.nn as nn
-import pathlib
-
-FILES = constants.FILES
-PATHS = constants.PATHS
-
-GENERIC_SECTION_TRAIN_FILE = FILES["GENERIC_SECTION_TRAIN_FILE"]
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-
-if __name__ == "__main__":
-    # read the hyperparams from config file
-    parser = argparse.ArgumentParser(
-        description="Bag of words linear classifier. "
-        "with initial random word embeddings"
-    )
-
-    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-    parser.add_argument(
-        "--max_length", help="Maximum length of every sentence", type=int
-    )
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
-    parser.add_argument("--bs", help="batch size", type=int)
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
-    parser.add_argument("--emb_type", help="embedding type", type=str)
-    parser.add_argument("--lr", help="learning rate", type=float)
-    parser.add_argument("--epochs", help="number of epochs", type=int)
-    parser.add_argument(
-        "--save_every", help="Save the model every few epochs", type=int
-    )
-    parser.add_argument(
-        "--log_train_metrics_every",
-        help="Log training metrics every few iterations",
-        type=int,
-    )
-
-    parser.add_argument(
-        "--exp_dir_path", help="Directory to store all experiment related information"
-    )
-    parser.add_argument(
-        "--model_save_dir",
-        help="Directory where the checkpoints during model training are stored.",
-    )
-    parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
-    )
-
-    args = parser.parse_args()
-    config = {
-        "EXP_NAME": args.exp_name,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "MAX_LENGTH": args.max_length,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "EMBEDDING_TYPE": args.emb_type,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    EXP_NAME = config["EXP_NAME"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    GENERIC_SECTION_TRAIN_FILE = "genericSect.train.data"
-
-    train_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset_params = {
-        "filename": GENERIC_SECTION_TRAIN_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    random_embeddings = train_dataset.word_vocab.load_embedding()
-    embedding = nn.Embedding.from_pretrained(random_embeddings, freeze=False)
-
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        dropout_value=0.0,
-        aggregation_type="sum",
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
-
-    engine = Engine(
-        model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
-        optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        metric=metric,
-        use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
-        track_for_best="macro_fscore",
-    )
-
-    engine.run()
diff --git a/examples/genericsect_bow_random/genericsect_bow_random.sh b/examples/genericsect_bow_random/genericsect_bow_random.sh
deleted file mode 100755
index 7614444..0000000
--- a/examples/genericsect_bow_random/genericsect_bow_random.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/usr/bin/env bash
-
-SCRIPT_FILE="genericsect_bow_random.py"
-
-python ${SCRIPT_FILE} \
---exp_name "genericsect_bow_random" \
---exp_dir_path  "./output" \
---model_save_dir "./output/checkpoints" \
---vocab_store_location "./output/vocab.json" \
---max_num_words 3000 \
---max_length 15 \
---debug \
---debug_dataset_proportion 0.01 \
---bs 32 \
---emb_type random \
---emb_dim 50 \
---lr 1e-4 \
---epochs 15 \
---save_every 1 \
---log_train_metrics_every 50
\ No newline at end of file
diff --git a/examples/parscit/parscit_client.py b/examples/parscit/parscit_client.py
index fc13c20..25ad412 100644
--- a/examples/parscit/parscit_client.py
+++ b/examples/parscit/parscit_client.py
@@ -1,7 +1,7 @@
 from sciwing.models.parscit_tagger import ParscitTagger
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.datasets.seq_labeling.parscit_dataset import ParscitDataset
 from sciwing.metrics.token_cls_accuracy import TokenClassificationAccuracy
@@ -262,12 +262,10 @@ if __name__ == "__main__":
         char_embedding = train_dataset.char_vocab.load_embedding()
         char_embedding = nn.Embedding.from_pretrained(char_embedding, freeze=False)
 
-        embedder = VanillaEmbedder(
-            embedding=embedding, embedding_dim=EMBEDDING_DIMENSION
-        )
+        embedder = WordEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
 
         if USE_CHAR_ENCODER:
-            char_embedder = VanillaEmbedder(
+            char_embedder = WordEmbedder(
                 embedding=char_embedding, embedding_dim=CHAR_EMBEDDING_DIMENSION
             )
             char_encoder = CharLSTMEncoder(
diff --git a/examples/science_ie/science_ie.py b/examples/science_ie/science_ie.py
index 99c42fd..f5ac1fb 100644
--- a/examples/science_ie/science_ie.py
+++ b/examples/science_ie/science_ie.py
@@ -1,7 +1,7 @@
 from sciwing.models.science_ie_tagger import ScienceIETagger
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.datasets.seq_labeling.science_ie_dataset import ScienceIEDataset
 from sciwing.metrics.token_cls_accuracy import TokenClassificationAccuracy
@@ -289,10 +289,10 @@ if __name__ == "__main__":
         constraint_type="BIOUL", labels=material_idx2classnames
     )
 
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
+    embedder = WordEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
 
     if USE_CHAR_ENCODER:
-        char_embedder = VanillaEmbedder(
+        char_embedder = WordEmbedder(
             embedding=char_embedding, embedding_dim=CHAR_EMBEDDING_DIMENSION
         )
         char_encoder = CharLSTMEncoder(
diff --git a/examples/science_ie/science_ie.toml b/examples/science_ie/science_ie.toml
index 036a5e8..f89136f 100644
--- a/examples/science_ie/science_ie.toml
+++ b/examples/science_ie/science_ie.toml
@@ -35,7 +35,7 @@
         combine_strategy="concat"
         rnn_bias=true
         [[model.rnn2seqencoder.embedder]]
-        class="VanillaEmbedder"
+        class="WordEmbedder"
         embed="word_vocab"
         freeze=false
         [[model.rnn2seqencoder.embedder]]
@@ -44,7 +44,7 @@
         hidden_dim=25
         bidirectional=true
         [model.rnn2seqencoder.embedder.char_embedder]
-        class="VanillaEmbedder"
+        class="WordEmbedder"
         embed="char_vocab"
         freeze=false
 
diff --git a/examples/sectlabel_bilstm/sectlabel_bilstm.py b/examples/sectlabel_bilstm/sectlabel_bilstm.py
index 7029c1e..734cb2b 100644
--- a/examples/sectlabel_bilstm/sectlabel_bilstm.py
+++ b/examples/sectlabel_bilstm/sectlabel_bilstm.py
@@ -1,21 +1,19 @@
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 import sciwing.constants as constants
-import os
-import torch.nn as nn
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-import pathlib
 import torch.optim as optim
 from sciwing.engine.engine import Engine
-import json
 import argparse
 import torch
+import pathlib
 
-FILES = constants.FILES
 PATHS = constants.PATHS
-
+DATA_DIR = PATHS["DATA_DIR"]
 
 if __name__ == "__main__":
     # read the hyperparams from config file
@@ -28,23 +26,6 @@ if __name__ == "__main__":
     parser.add_argument(
         "--device", help="Adding which device to run the experiment on", type=str
     )
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
     parser.add_argument("--bs", help="batch size", type=int)
     parser.add_argument("--lr", help="learning rate", type=float)
     parser.add_argument("--epochs", help="number of epochs", type=int)
@@ -56,7 +37,6 @@ if __name__ == "__main__":
         help="Log training metrics every few iterations",
         type=int,
     )
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
     parser.add_argument(
         "--emb_type",
         help="The type of glove embedding you want. The allowed types are glove_6B_50, glove_6B_100, "
@@ -65,14 +45,6 @@ if __name__ == "__main__":
     parser.add_argument(
         "--hidden_dim", help="Hidden dimension of the LSTM network", type=int
     )
-    parser.add_argument(
-        "--max_length", help="Maximum length of the inputs to the encoder", type=int
-    )
-    parser.add_argument(
-        "--return_instances",
-        help="Return instances or tokens from sciwing dataset",
-        action="store_true",
-    )
     parser.add_argument(
         "--bidirectional",
         help="Specify Whether the lstm is bidirectional or uni-directional",
@@ -92,168 +64,68 @@ if __name__ == "__main__":
         help="Directory where the checkpoints during model training are stored.",
     )
     parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
+        "--sample_proportion", help="Sample proportion for the dataset", type=float
     )
 
     args = parser.parse_args()
-    config = {
-        "EXP_NAME": args.exp_name,
-        "DEVICE": args.device,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "MAX_LENGTH": args.max_length,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_TYPE": args.emb_type,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "HIDDEN_DIMENSION": args.hidden_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "RETURN_INSTANCES": args.return_instances,
-        "BIDIRECTIONAL": bool(args.bidirectional),
-        "COMBINE_STRATEGY": args.combine_strategy,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    EXP_NAME = config["EXP_NAME"]
-    DEVICE = config["DEVICE"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    HIDDEN_DIMENSION = config["HIDDEN_DIMENSION"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    RETURN_INSTANCES = config["RETURN_INSTANCES"]
-    BIDIRECTIONAL = config["BIDIRECTIONAL"]
-    COMBINE_STRATEGY = config["COMBINE_STRATEGY"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    # instantiate the dataset
-    # fi you do not have the data in the local file run `sciwing download data --task sectlabel`
-    SECT_LABEL_FILE = "sectLabel.train.data"
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset_params = {
-        "filename": SECT_LABEL_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
 
     # saving the test dataset params
     # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-
-    # save the config in a file to help later during testing/inference
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
 
-    # save the parameters of the test dataset
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
 
-    random_embeddings = train_dataset.word_vocab.load_embedding()
-    random_embeddings = nn.Embedding.from_pretrained(random_embeddings, freeze=False)
-
-    embedder = VanillaEmbedder(
-        embedding=random_embeddings, embedding_dim=EMBEDDING_DIMENSION
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
     )
+
+    embedder = WordEmbedder(embedding_type=args.emb_type)
     encoder = LSTM2VecEncoder(
-        emb_dim=EMBEDDING_DIMENSION,
         embedder=embedder,
-        dropout_value=0.0,
-        hidden_dim=HIDDEN_DIMENSION,
-        combine_strategy=COMBINE_STRATEGY,
-        bidirectional=BIDIRECTIONAL,
-        device=torch.device(DEVICE),
+        hidden_dim=args.hidden_dim,
+        combine_strategy=args.combine_strategy,
+        bidirectional=args.bidirectional,
+        device=torch.device(args.device),
     )
 
-    classiier_encoding_dim = 2 * HIDDEN_DIMENSION if BIDIRECTIONAL else HIDDEN_DIMENSION
+    classiier_encoding_dim = (
+        2 * args.hidden_dim if args.bidirectional else args.hidden_dim
+    )
     model = SimpleClassifier(
         encoder=encoder,
         encoding_dim=classiier_encoding_dim,
-        num_classes=NUM_CLASSES,
+        num_classes=23,
         classification_layer_bias=True,
+        datasets_manager=data_manager,
     )
 
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(train_dataset.idx2classname)
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
 
     engine = Engine(
         model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
+        datasets_manager=data_manager,
         optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        device=torch.device(DEVICE),
-        metric=metric,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        device=torch.device(args.device),
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
         use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
         track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
     )
 
     engine.run()
diff --git a/examples/sectlabel_bilstm/sectlabel_bilstm.sh b/examples/sectlabel_bilstm/sectlabel_bilstm.sh
index 08c03ef..43fb959 100755
--- a/examples/sectlabel_bilstm/sectlabel_bilstm.sh
+++ b/examples/sectlabel_bilstm/sectlabel_bilstm.sh
@@ -5,22 +5,17 @@ SCRIPT_FILE="sectlabel_bilstm.py"
 
 
 python ${SCRIPT_FILE} \
---exp_name "parsect_bi_lstm_lc" \
+--exp_name "sectlabel_bilstm" \
 --exp_dir_path  "./output" \
 --model_save_dir "./output/checkpoints" \
---vocab_store_location "./output/vocab.json" \
+--emb_type "glove_6B_50" \
 --device cpu \
---max_num_words 1000 \
---max_length 10 \
---debug \
---debug_dataset_proportion 0.01 \
 --bs 32 \
---emb_type random \
---emb_dim 300 \
 --hidden_dim 512 \
 --lr 1e-3 \
 --bidirectional \
 --combine_strategy concat \
 --epochs 1 \
 --save_every 5 \
---log_train_metrics_every 50
+--log_train_metrics_every 50 \
+--sample_proportion 0.01
diff --git a/examples/sectlabel_bilstm/sectlabel_bilstm.toml b/examples/sectlabel_bilstm/sectlabel_bilstm.toml
index 33b30e6..56958cf 100644
--- a/examples/sectlabel_bilstm/sectlabel_bilstm.toml
+++ b/examples/sectlabel_bilstm/sectlabel_bilstm.toml
@@ -1,20 +1,12 @@
 [experiment]
-    exp_name = "sectlabel-bow-random"
-    exp_dir = "sectlabel_bow_random_toml"
+    exp_name = "sectlabel-bilstm"
+    exp_dir = "sectlabel_bilstm_toml"
 
 [dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_random_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="sectLabel.train"
+	dev_filename="sectLabel.dev"
+	test_filename="sectLabel.test"
 
 [model]
     class="SimpleClassifier"
@@ -22,7 +14,6 @@
     num_classes=23
     classification_layer_bias=true
     [model.encoder]
-        emb_dim = 300
         class="LSTM2VecEncoder"
         dropout_value = 0.5
         combine_strategy="concat"
@@ -30,18 +21,18 @@
         device="cpu"
         hidden_dim=100
         [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
+        class="WordEmbedder"
+        embedding_type="glove_6B_50"
 
 [engine]
     batch_size=32
-    save_dir="sectlabel_bow_random_toml/checkpoints"
+    save_dir="sectlabel_bilstm_toml/checkpoints"
     num_epochs=1
     save_every=10
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.01
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/sectlabel_bilstm/sectlabel_bilstm_infer.py b/examples/sectlabel_bilstm/sectlabel_bilstm_infer.py
new file mode 100644
index 0000000..a35d73b
--- /dev/null
+++ b/examples/sectlabel_bilstm/sectlabel_bilstm_infer.py
@@ -0,0 +1,68 @@
+import os
+import sciwing.constants as constants
+from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+def build_sectlabel_bilstm_model(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    HIDDEN_DIM = 512
+    BIDIRECTIONAL = True
+    COMBINE_STRATEGY = "concat"
+
+    classifier_encoding_dim = 2 * HIDDEN_DIM if BIDIRECTIONAL else HIDDEN_DIM
+
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
+
+    encoder = LSTM2VecEncoder(
+        embedder=embedder,
+        hidden_dim=HIDDEN_DIM,
+        combine_strategy=COMBINE_STRATEGY,
+        bidirectional=BIDIRECTIONAL,
+    )
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=classifier_encoding_dim,
+        num_classes=23,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    inference = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+
+    return inference
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_sectlabel_bilstm_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/sectlabel_bow_bert/sectlabel_bow_bert.py b/examples/sectlabel_bow_bert/sectlabel_bow_bert.py
index c9371c8..a6ba57b 100644
--- a/examples/sectlabel_bow_bert/sectlabel_bow_bert.py
+++ b/examples/sectlabel_bow_bert/sectlabel_bow_bert.py
@@ -1,16 +1,19 @@
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
 from sciwing.modules.embedders.bert_embedder import BertEmbedder
 from sciwing.modules.bow_encoder import BOW_Encoder
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-import os
-import pathlib
+import sciwing.constants as constants
 import torch.optim as optim
 from sciwing.engine.engine import Engine
-import json
+import pathlib
 import argparse
 import torch
 
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
 
 if __name__ == "__main__":
     # read the hyperparams from config file
@@ -21,24 +24,6 @@ if __name__ == "__main__":
 
     parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
     parser.add_argument("--device", help="specify the device to run on.", type=str)
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-
-    parser.add_argument("--max_length", help="max length of sentences", type=int)
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
     parser.add_argument("--bs", help="batch size", type=int)
     parser.add_argument("--lr", help="learning rate", type=float)
     parser.add_argument("--epochs", help="number of epochs", type=int)
@@ -50,17 +35,6 @@ if __name__ == "__main__":
         help="Log training metrics every few iterations",
         type=int,
     )
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
-    parser.add_argument(
-        "--emb_type",
-        help="The type of glove embedding you want. The allowed types are glove_6B_50, glove_6B_100, "
-        "glove_6B_200, glove_6B_300",
-    )
-    parser.add_argument(
-        "--return_instances",
-        help="Set this if the dataset has to return instances",
-        action="store_true",
-    )
     parser.add_argument(
         "--bert_type",
         help="Specify the bert model to be used. One of bert-base-uncased, bert-base-cased, "
@@ -75,159 +49,62 @@ if __name__ == "__main__":
         help="Directory where the checkpoints during model training are stored.",
     )
     parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
+        "--sample_proportion", help="Sample proportion for the dataset", type=float
     )
 
     args = parser.parse_args()
 
-    config = {
-        "EXP_NAME": args.exp_name,
-        "DEVICE": args.device,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "EMBEDDING_TYPE": args.emb_type,
-        "RETURN_INSTANCES": args.return_instances,
-        "BERT_TYPE": args.bert_type,
-        "MAX_LENGTH": args.max_length,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    EXP_NAME = config["EXP_NAME"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    RETURN_INSTANCES = config["RETURN_INSTANCES"]
-    TENSORBOARD_LOGDIR = os.path.join(".", "runs", EXP_NAME)
-    BERT_TYPE = config["BERT_TYPE"]
-    DEVICE = config["DEVICE"]
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    SECT_LABEL_FILE = "sectLabel.train.data"
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
+    DATA_PATH = pathlib.Path(DATA_DIR)
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
 
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
     )
 
-    test_dataset_params = {
-        "filename": SECT_LABEL_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
     embedder = BertEmbedder(
-        emb_dim=EMBEDDING_DIMENSION,
         dropout_value=0.0,
         aggregation_type="average",
-        bert_type=BERT_TYPE,
-        device=torch.device(DEVICE),
+        bert_type=args.bert_type,
+        device=torch.device(args.device),
     )
 
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION, embedder=embedder, aggregation_type="average"
-    )
+    encoder = BOW_Encoder(embedder=embedder, aggregation_type="average")
 
     model = SimpleClassifier(
         encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
+        encoding_dim=768,
+        num_classes=23,
         classification_layer_bias=True,
+        datasets_manager=data_manager,
     )
 
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
 
     engine = Engine(
         model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
+        datasets_manager=data_manager,
         optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        tensorboard_logdir=TENSORBOARD_LOGDIR,
-        device=torch.device(DEVICE),
-        metric=metric,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        device=torch.device(args.device),
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
         use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
         track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
     )
 
     engine.run()
diff --git a/examples/sectlabel_bow_bert/sectlabel_bow_bert.sh b/examples/sectlabel_bow_bert/sectlabel_bow_bert.sh
index 024ff1e..f8a955d 100755
--- a/examples/sectlabel_bow_bert/sectlabel_bow_bert.sh
+++ b/examples/sectlabel_bow_bert/sectlabel_bow_bert.sh
@@ -7,17 +7,11 @@ python ${SCRIPT_FILE} \
 --exp_name "sectlabel_bow_bert" \
 --exp_dir_path  "./output" \
 --model_save_dir "./output/checkpoints" \
---vocab_store_location "./output/vocab.json" \
 --device cpu \
---bert_type "bert-large-cased" \
---max_length 20 \
---max_num_words 15000 \
---debug \
---debug_dataset_proportion 0.01 \
---return_instances \
+--bert_type "bert-base-uncased" \
 --bs 32 \
---emb_dim 1024 \
 --lr 1e-2 \
 --epochs 1 \
 --save_every 1 \
---log_train_metrics_every 50
\ No newline at end of file
+--log_train_metrics_every 50 \
+--sample_proportion 1.0
\ No newline at end of file
diff --git a/examples/sectlabel_bow_bert/sectlabel_bow_bert.toml b/examples/sectlabel_bow_bert/sectlabel_bow_bert.toml
index cc68ddf..ef49b55 100644
--- a/examples/sectlabel_bow_bert/sectlabel_bow_bert.toml
+++ b/examples/sectlabel_bow_bert/sectlabel_bow_bert.toml
@@ -1,20 +1,12 @@
 [experiment]
-    exp_name = "sectlabel-bow-elmo"
-    exp_dir = "sectlabel_bow_elmo_toml"
+    exp_name = "sectlabel-bow-bert"
+    exp_dir = "sectlabel_bow_bert_toml"
 
 [dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_elmo_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="sectLabel.train"
+	dev_filename="sectLabel.dev"
+	test_filename="sectLabel.test"
 
 [model]
     class="SimpleClassifier"
@@ -25,19 +17,20 @@
         class="BOW_Encoder"
         dropout_value = 0.5
         aggregation_type="sum"
-        emb_dim = 1024
         [[model.encoder.embedder]]
         class="BertEmbedder"
-        emb_dim=768
+        bert_type="bert-base-cased"
+        aggregation_type="average"
 
 [engine]
     batch_size=32
-    save_dir="sectlabel_bow_elmo_toml/checkpoints"
+    save_dir="sectlabel_bow_bert_toml/checkpoints"
     num_epochs=1
     save_every=10
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.01
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/sectlabel_bow_bert/sectlabel_bow_bert_infer.py b/examples/sectlabel_bow_bert/sectlabel_bow_bert_infer.py
new file mode 100644
index 0000000..36a1e63
--- /dev/null
+++ b/examples/sectlabel_bow_bert/sectlabel_bow_bert_infer.py
@@ -0,0 +1,63 @@
+import json
+import os
+import sciwing.constants as constants
+from sciwing.modules.embedders.bert_embedder import BertEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+import torch
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+def build_sectlabel_bow_bert(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    embedder = BertEmbedder(
+        dropout_value=0.0,
+        aggregation_type="average",
+        bert_type="bert-base-uncased",
+        device=torch.device("cpu"),
+    )
+
+    encoder = BOW_Encoder(embedder=embedder, aggregation_type="average")
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=768,
+        num_classes=23,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    parsect_inference = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+
+    return parsect_inference
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_sectlabel_bow_bert(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.py b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.py
index c607001..b491218 100644
--- a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.py
+++ b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.py
@@ -1,24 +1,21 @@
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
 from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
 from sciwing.modules.bow_encoder import BOW_Encoder
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
 import sciwing.constants as constants
-import os
 import torch.optim as optim
 from sciwing.engine.engine import Engine
-import json
 import argparse
 import torch
 import re
 import pathlib
 
-FILES = constants.FILES
-PATHS = constants.PATHS
 
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
 
 if __name__ == "__main__":
     # read the hyperparams from config file
@@ -28,26 +25,12 @@ if __name__ == "__main__":
     )
 
     parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
-    parser.add_argument(
-        "--max_length", help="Specify the maximum length of input", type=int
-    )
+
     parser.add_argument(
         "--device",
         help="Specify the device on which models and tensors reside",
         type=str,
     )
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
 
     parser.add_argument(
         "--layer_aggregation", help="Layer aggregation strategy", type=str
@@ -56,11 +39,6 @@ if __name__ == "__main__":
     parser.add_argument(
         "--word_aggregation", help="word aggregation strategy", type=str
     )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
     parser.add_argument("--bs", help="batch size", type=int)
     parser.add_argument("--lr", help="learning rate", type=float)
     parser.add_argument("--epochs", help="number of epochs", type=int)
@@ -72,7 +50,6 @@ if __name__ == "__main__":
         help="Log training metrics every few iterations",
         type=int,
     )
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
     parser.add_argument(
         "--emb_type",
         help="The type of glove embedding you want. The allowed types are glove_6B_50, glove_6B_100, "
@@ -89,159 +66,62 @@ if __name__ == "__main__":
     parser.add_argument(
         "--vocab_store_location", help="File in which the vocab is stored"
     )
+    parser.add_argument(
+        "--sample_proportion", help="Sample proportion for debugging", type=float
+    )
 
     args = parser.parse_args()
 
-    config = {
-        "EXP_NAME": args.exp_name,
-        "MAX_LENGTH": args.max_length,
-        "DEVICE": args.device,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "EMBEDDING_TYPE": args.emb_type,
-        "LAYER_AGGREGATION": args.layer_aggregation,
-        "WORD_AGGREGATION": args.word_aggregation,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    MAX_LENGTH = config["MAX_LENGTH"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    DEVICE = config["DEVICE"]
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    LAYER_AGGREGATION = config["LAYER_AGGREGATION"]
-    WORD_AGGREGATION = config["WORD_AGGREGATION"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    EXP_NAME = config["EXP_NAME"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    SECT_LABEL_FILE = "sectLabel.train.data"
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
+    DATA_PATH = pathlib.Path(DATA_DIR)
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
 
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
     )
 
-    test_dataset_params = {
-        "filename": SECT_LABEL_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-
-    # store anything that helps later in instantiation
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
     # BowElmoEmbedder embeds sentences using ELMO
     embedder = BowElmoEmbedder(
-        emb_dim=EMBEDDING_DIMENSION,
-        layer_aggregation=LAYER_AGGREGATION,
-        cuda_device_id=0 if re.match("cuda", DEVICE) else -1,
+        layer_aggregation=args.layer_aggregation,
+        cuda_device_id=0 if re.match("cuda", args.device) else -1,
     )
 
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        aggregation_type=WORD_AGGREGATION,
-    )
+    encoder = BOW_Encoder(embedder=embedder, aggregation_type=args.word_aggregation)
 
     model = SimpleClassifier(
         encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
+        encoding_dim=1024,
+        num_classes=data_manager.num_labels["label"],
         classification_layer_bias=True,
+        datasets_manager=data_manager,
     )
 
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
 
     engine = Engine(
         model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
+        datasets_manager=data_manager,
         optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        device=torch.device(DEVICE),
-        metric=metric,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        device=torch.device(args.device),
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
         use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
         track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
     )
 
     engine.run()
diff --git a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.sh b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.sh
index 6c9abf6..012532d 100644
--- a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.sh
+++ b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.sh
@@ -7,16 +7,12 @@ python ${SCRIPT_FILE} \
 --exp_dir_path  "./output" \
 --model_save_dir "./output/checkpoints" \
 --vocab_store_location "./output/vocab.json" \
---max_length 15 \
---max_num_words 15000 \
 --device cpu \
---debug \
 --layer_aggregation last \
 --word_aggregation sum \
---debug_dataset_proportion 0.01 \
 --bs 10 \
---emb_dim 1024 \
 --lr 1e-4 \
 --epochs 1 \
 --save_every 5 \
---log_train_metrics_every 10
+--log_train_metrics_every 10 \
+--sample_proportion 0.1
diff --git a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.toml b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.toml
index 152585f..093df50 100644
--- a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.toml
+++ b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo.toml
@@ -3,18 +3,10 @@
     exp_dir = "sectlabel_bow_elmo_toml"
 
 [dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_elmo_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="sectLabel.train"
+	dev_filename="sectLabel.dev"
+	test_filename="sectLabel.test"
 
 [model]
     class="SimpleClassifier"
@@ -25,10 +17,8 @@
         class="BOW_Encoder"
         dropout_value = 0.5
         aggregation_type="sum"
-        emb_dim = 1024
         [[model.encoder.embedder]]
         class="BowElmoEmbedder"
-        emb_dim=1024
 
 [engine]
     batch_size=32
@@ -38,6 +28,7 @@
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.1
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/sectlabel_bow_elmo/sectlabel_bow_elmo_infer.py b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo_infer.py
new file mode 100644
index 0000000..844eaac
--- /dev/null
+++ b/examples/sectlabel_bow_elmo/sectlabel_bow_elmo_infer.py
@@ -0,0 +1,55 @@
+import sciwing.constants as constants
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+def build_sectlabel_bow_elmo_model(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    embedder = BowElmoEmbedder(layer_aggregation="last")
+    encoder = BOW_Encoder(aggregation_type="sum", embedder=embedder)
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=1024,
+        num_classes=data_manager.num_labels["label"],
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    infer_client = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+
+    return infer_client
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_sectlabel_bow_elmo_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/sectlabel_bow_random/sectlabel_bow.py b/examples/sectlabel_bow_random/sectlabel_bow.py
new file mode 100644
index 0000000..ba32e3a
--- /dev/null
+++ b/examples/sectlabel_bow_random/sectlabel_bow.py
@@ -0,0 +1,104 @@
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import sciwing.constants as constants
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+import torch.optim as optim
+from sciwing.engine.engine import Engine
+import argparse
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+if __name__ == "__main__":
+    # read the hyperparams from config file
+
+    parser = argparse.ArgumentParser(
+        description="Bag of words linear classifier. "
+        "with initial random word embeddings"
+    )
+    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
+    parser.add_argument("--bs", help="batch size", type=int)
+    parser.add_argument("--emb_type", help="embedding type", type=str)
+    parser.add_argument("--lr", help="learning rate", type=float)
+    parser.add_argument("--epochs", help="number of epochs", type=int)
+    parser.add_argument(
+        "--save_every", help="Save the model every few epochs", type=int
+    )
+    parser.add_argument(
+        "--log_train_metrics_every",
+        help="Log training metrics every few iterations",
+        type=int,
+    )
+    parser.add_argument(
+        "--exp_dir_path", help="Directory to store all experiment related information"
+    )
+    parser.add_argument(
+        "--model_save_dir",
+        help="Directory where the checkpoints during model training are stored.",
+    )
+    parser.add_argument(
+        "--vocab_store_location", help="File in which the vocab is stored"
+    )
+
+    args = parser.parse_args()
+
+    DATA_PATH = pathlib.Path(DATA_DIR)
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    embedder = WordEmbedder(embedding_type=args.emb_type)
+
+    # initialize a bag of word emcoder
+    encoder = BOW_Encoder(embedder=embedder)
+
+    # Instantiate a simple classifier
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=embedder.get_embedding_dimension(),
+        classification_layer_bias=True,
+        num_classes=data_manager.num_labels["label"],
+        datasets_manager=data_manager,
+    )
+
+    # you get to use any optimizer from Pytorch
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+
+    # Instantiate the PrecisionRecallFMeasure
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+
+    # Get the engine worked up
+    engine = Engine(
+        model=model,
+        datasets_manager=data_manager,
+        optimizer=optimizer,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        log_train_metrics_every=args.log_train_metrics_every,
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
+        use_wandb=True,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
+        track_for_best="macro_fscore",
+        sample_proportion=0.01,
+    )
+
+    # Run the engine
+    engine.run()
diff --git a/examples/sectlabel_bow_random/sectlabel_bow_random.sh b/examples/sectlabel_bow_random/sectlabel_bow.sh
similarity index 54%
rename from examples/sectlabel_bow_random/sectlabel_bow_random.sh
rename to examples/sectlabel_bow_random/sectlabel_bow.sh
index a15de63..932d18d 100644
--- a/examples/sectlabel_bow_random/sectlabel_bow_random.sh
+++ b/examples/sectlabel_bow_random/sectlabel_bow.sh
@@ -1,20 +1,15 @@
 #!/usr/bin/env bash
 
-SCRIPT_FILE="sectlabel_bow_random.py"
+SCRIPT_FILE="sectlabel_bow.py"
 
 python ${SCRIPT_FILE} \
 --exp_name "sectlabel_bow_random" \
 --exp_dir_path  "./output" \
 --model_save_dir "./output/checkpoints" \
 --vocab_store_location "./output/vocab.json" \
---max_num_words 3000 \
---max_length 15 \
---debug \
---debug_dataset_proportion 0.01 \
 --bs 32 \
---emb_type random \
---emb_dim 50 \
+--emb_type glove_6B_50 \
 --lr 1e-4 \
---epochs 15 \
+--epochs 2 \
 --save_every 1 \
 --log_train_metrics_every 50
\ No newline at end of file
diff --git a/examples/sectlabel_bow_random/sectlabel_bow_random.toml b/examples/sectlabel_bow_random/sectlabel_bow.toml
similarity index 54%
rename from examples/sectlabel_bow_random/sectlabel_bow_random.toml
rename to examples/sectlabel_bow_random/sectlabel_bow.toml
index 4d7e5ca..6dfa529 100644
--- a/examples/sectlabel_bow_random/sectlabel_bow_random.toml
+++ b/examples/sectlabel_bow_random/sectlabel_bow.toml
@@ -3,33 +3,23 @@
     exp_dir = "sectlabel_bow_random_toml"
 
 [dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_random_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="sectLabel.train"
+	dev_filename="sectLabel.dev"
+	test_filename="sectLabel.test"
 
 [model]
     class="SimpleClassifier"
-    encoding_dim=300
+    encoding_dim=50
     num_classes=23
     classification_layer_bias=true
     [model.encoder]
-        emb_dim = 300
         class="BOW_Encoder"
-        dropout_value = 0.5
         aggregation_type="sum"
         [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
+        class="WordEmbedder"
+        embedding_type="glove_6B_50"
+
 
 [engine]
     batch_size=32
@@ -40,6 +30,7 @@
     tensorboard_logdir="debug_parsect_dataset"
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.1
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/sectlabel_bow_random/sectlabel_bow_glove.toml b/examples/sectlabel_bow_random/sectlabel_bow_glove.toml
deleted file mode 100644
index d973948..0000000
--- a/examples/sectlabel_bow_random/sectlabel_bow_glove.toml
+++ /dev/null
@@ -1,47 +0,0 @@
-[experiment]
-    exp_name = "sectlabel-bow-glove"
-    exp_dir = "sectlabel_bow_glove_toml"
-
-[dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_glove_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "glove_6B_100"
-	word_embedding_dimension = 100
-
-[model]
-    class="SimpleClassifier"
-    encoding_dim=100
-    num_classes=23
-    classification_layer_bias=true
-    [model.encoder]
-        emb_dim = 100
-        class="BOW_Encoder"
-        dropout_value = 0.5
-        aggregation_type="sum"
-        [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
-
-[engine]
-    batch_size=32
-    save_dir="sectlabel_bow_glove_toml/checkpoints"
-    num_epochs=1
-    save_every=10
-    log_train_metrics_every=10
-    tensorboard_logdir="debug_parsect_dataset"
-    device="cpu"
-    gradient_norm_clip_value=5.0
-    [engine.metric]
-        class="PrecisionRecallFMeasure"
-    [engine.optimizer]
-        class="Adam"
-        lr=1e-3
\ No newline at end of file
diff --git a/examples/sectlabel_bow_random/sectlabel_bow_infer.py b/examples/sectlabel_bow_random/sectlabel_bow_infer.py
new file mode 100644
index 0000000..5f2db1d
--- /dev/null
+++ b/examples/sectlabel_bow_random/sectlabel_bow_infer.py
@@ -0,0 +1,68 @@
+import sciwing.constants as constants
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import pathlib
+
+PATHS = constants.PATHS
+FILES = constants.FILES
+OUTPUT_DIR = PATHS["OUTPUT_DIR"]
+CONFIGS_DIR = PATHS["CONFIGS_DIR"]
+SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+def build_sectlabel_bow_model(dirname: str):
+    """
+
+    Parameters
+    ----------
+    dirname : The directory where sciwing stores your outputs for the model
+
+    Returns
+    -------
+
+
+    """
+    exp_dirpath = pathlib.Path(dirname)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
+    encoder = BOW_Encoder(embedder=embedder)
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=embedder.get_embedding_dimension(),
+        num_classes=data_manager.num_labels["label"],
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    infer = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+    return infer
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_sectlabel_bow_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/examples/sectlabel_bow_random/sectlabel_bow_random.py b/examples/sectlabel_bow_random/sectlabel_bow_random.py
deleted file mode 100644
index d15b954..0000000
--- a/examples/sectlabel_bow_random/sectlabel_bow_random.py
+++ /dev/null
@@ -1,223 +0,0 @@
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import sciwing.constants as constants
-import os
-import torch.optim as optim
-from sciwing.engine.engine import Engine
-import json
-import argparse
-import torch.nn as nn
-import pathlib
-
-
-if __name__ == "__main__":
-    # read the hyperparams from config file
-
-    parser = argparse.ArgumentParser(
-        description="Bag of words linear classifier. "
-        "with initial random word embeddings"
-    )
-
-    parser.add_argument("--exp_name", help="Specify an experiment name", type=str)
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words considered in the vocab",
-        type=int,
-    )
-    parser.add_argument(
-        "--max_length", help="Maximum Length of Sentence (used for padding)", type=int
-    )
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is a debug run. The dataset used will be small",
-        action="store_true",
-        default=False,
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-        default=0.1,
-    )
-    parser.add_argument("--bs", help="batch size", type=int)
-    parser.add_argument("--emb_type", help="embedding type", type=str)
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
-    parser.add_argument("--lr", help="learning rate", type=float)
-    parser.add_argument("--epochs", help="number of epochs", type=int)
-    parser.add_argument(
-        "--save_every", help="Save the model every few epochs", type=int
-    )
-    parser.add_argument(
-        "--log_train_metrics_every",
-        help="Log training metrics every few iterations",
-        type=int,
-    )
-    parser.add_argument(
-        "--exp_dir_path", help="Directory to store all experiment related information"
-    )
-    parser.add_argument(
-        "--model_save_dir",
-        help="Directory where the checkpoints during model training are stored.",
-    )
-    parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
-    )
-
-    args = parser.parse_args()
-
-    # store the config in a dictionary
-    # this can be used to load the model later during inference/testing
-    config = {
-        "EXP_NAME": args.exp_name,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "MAX_LENGTH": args.max_length,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "EMBEDDING_TYPE": args.emb_type,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    EXP_NAME = config["EXP_NAME"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    # get the path to the sectLabel.train.data in the folder
-    SECT_LABEL_FILE = "sectLabel.train.data"
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
-
-    # store the configuration of the datasets into a dictionary
-    # which can be used to instantiate the dataset later during inference/testing
-    test_dataset_params = {
-        "filename": SECT_LABEL_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-
-    # store anything that helps later in instantiation
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
-    # you can load the embeddings from the word vocab
-    # This returns the FloatTensor that can be then used with Pytorch Embedding
-    random_embeddings = train_dataset.word_vocab.load_embedding()
-    embedding = nn.Embedding.from_pretrained(random_embeddings, freeze=False)
-
-    # A vanilla embedder that maps indices to embeddings
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-
-    # initialize a bag of word emcoder
-    encoder = BOW_Encoder(emb_dim=EMBEDDING_DIMENSION, embedder=embedder)
-
-    # Instantiate a simple classifier
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    # you get to use any optimizer from Pytorch
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-
-    # Instantiate the PrecisionRecallFMeasure
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
-
-    # Get the engine worked up
-    engine = Engine(
-        model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
-        optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        metric=metric,
-        use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
-        track_for_best="macro_fscore",
-    )
-
-    # Run the engine
-    engine.run()
diff --git a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.py b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.py
index 3de81e9..b05612b 100644
--- a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.py
+++ b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.py
@@ -1,19 +1,21 @@
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
 from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
 from sciwing.models.simpleclassifier import SimpleClassifier
 import pathlib
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-import os
-import torch.nn as nn
-
 import torch.optim as optim
 from sciwing.engine.engine import Engine
-import json
 import argparse
 import torch
+import sciwing.constants as constants
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
 
 
 if __name__ == "__main__":
@@ -28,23 +30,6 @@ if __name__ == "__main__":
         "--device", help="Specify the device where the model is run", type=str
     )
 
-    parser.add_argument(
-        "--max_num_words",
-        help="Maximum number of words to be considered " "in the vocab",
-        type=int,
-    )
-
-    parser.add_argument(
-        "--debug",
-        help="Specify whether this is run on a debug options. The "
-        "dataset considered will be small",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--debug_dataset_proportion",
-        help="The proportion of the dataset " "that will be used if debug is true",
-        type=float,
-    )
     parser.add_argument("--bs", help="batch size", type=int)
     parser.add_argument("--lr", help="learning rate", type=float)
     parser.add_argument("--epochs", help="number of epochs", type=int)
@@ -56,7 +41,6 @@ if __name__ == "__main__":
         help="Log training metrics every few iterations",
         type=int,
     )
-    parser.add_argument("--emb_dim", help="embedding dimension", type=int)
     parser.add_argument(
         "--emb_type",
         help="The type of glove embedding you want. The allowed types are glove_6B_50, glove_6B_100, "
@@ -65,10 +49,6 @@ if __name__ == "__main__":
     parser.add_argument(
         "--hidden_dim", help="Hidden dimension of the LSTM network", type=int
     )
-    parser.add_argument(
-        "--max_length", help="Maximum length of the inputs to the encoder", type=int
-    )
-
     parser.add_argument(
         "--bidirectional",
         help="Specify Whether the lstm is bidirectional or uni-directional",
@@ -88,184 +68,81 @@ if __name__ == "__main__":
         help="Directory where the checkpoints during model training are stored.",
     )
     parser.add_argument(
-        "--vocab_store_location", help="File in which the vocab is stored"
+        "--sample_proportion", help="Sample proportion for the dataset", type=float
     )
 
     args = parser.parse_args()
-    config = {
-        "EXP_NAME": args.exp_name,
-        "DEVICE": args.device,
-        "MAX_NUM_WORDS": args.max_num_words,
-        "MAX_LENGTH": args.max_length,
-        "DEBUG": args.debug,
-        "DEBUG_DATASET_PROPORTION": args.debug_dataset_proportion,
-        "BATCH_SIZE": args.bs,
-        "EMBEDDING_TYPE": args.emb_type,
-        "EMBEDDING_DIMENSION": args.emb_dim,
-        "HIDDEN_DIMENSION": args.hidden_dim,
-        "LEARNING_RATE": args.lr,
-        "NUM_EPOCHS": args.epochs,
-        "SAVE_EVERY": args.save_every,
-        "LOG_TRAIN_METRICS_EVERY": args.log_train_metrics_every,
-        "BIDIRECTIONAL": bool(args.bidirectional),
-        "COMBINE_STRATEGY": args.combine_strategy,
-        "ELMO_EMBEDDING_DIMENSION": 1024,
-        "EXP_DIR_PATH": args.exp_dir_path,
-        "MODEL_SAVE_DIR": args.model_save_dir,
-        "VOCAB_STORE_LOCATION": args.vocab_store_location,
-    }
-
-    EXP_NAME = config["EXP_NAME"]
-    DEVICE = config["DEVICE"]
-    VOCAB_STORE_LOCATION = config["VOCAB_STORE_LOCATION"]
-    MAX_NUM_WORDS = config["MAX_NUM_WORDS"]
-    MAX_LENGTH = config["MAX_LENGTH"]
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    EMBEDDING_TYPE = config["EMBEDDING_TYPE"]
-    DEBUG = config["DEBUG"]
-    DEBUG_DATASET_PROPORTION = config["DEBUG_DATASET_PROPORTION"]
-    BATCH_SIZE = config["BATCH_SIZE"]
-    HIDDEN_DIMENSION = config["HIDDEN_DIMENSION"]
-    LEARNING_RATE = config["LEARNING_RATE"]
-    NUM_EPOCHS = config["NUM_EPOCHS"]
-    SAVE_EVERY = config["SAVE_EVERY"]
-    LOG_TRAIN_METRICS_EVERY = config["LOG_TRAIN_METRICS_EVERY"]
-    TENSORBOARD_LOGDIR = os.path.join(".", "runs", EXP_NAME)
-    BIDIRECTIONAL = config["BIDIRECTIONAL"]
-    COMBINE_STRATEGY = config["COMBINE_STRATEGY"]
-    ELMO_EMBEDDING_DIMENSION = config["ELMO_EMBEDDING_DIMENSION"]
-    EXP_DIR_PATH = config["EXP_DIR_PATH"]
-    EXP_DIR_PATH = pathlib.Path(EXP_DIR_PATH)
-    EXP_NAME = config["EXP_NAME"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    SECT_LABEL_FILE = "sectLabel.train.data"
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
 
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
-    )
+    DATA_PATH = pathlib.Path(DATA_DIR)
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
 
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=VOCAB_STORE_LOCATION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
     )
 
-    test_dataset_params = {
-        "filename": SECT_LABEL_FILE,
-        "dataset_type": "test",
-        "max_num_words": MAX_NUM_WORDS,
-        "max_instance_length": MAX_LENGTH,
-        "word_vocab_store_location": VOCAB_STORE_LOCATION,
-        "debug": DEBUG,
-        "debug_dataset_proportion": DEBUG_DATASET_PROPORTION,
-        "word_embedding_type": EMBEDDING_TYPE,
-        "word_embedding_dimension": EMBEDDING_DIMENSION,
-    }
-
-    # saving the test dataset params
-    # lets save the test dataset params for the experiment
-    if not EXP_DIR_PATH.is_dir():
-        EXP_DIR_PATH.mkdir()
-
-    with open(os.path.join(EXP_DIR_PATH, "test_dataset_params.json"), "w") as fp:
-        json.dump(test_dataset_params, fp)
-
-    VOCAB_SIZE = train_dataset.word_vocab.get_vocab_len()
-    NUM_CLASSES = train_dataset.get_num_classes()
-
-    # store anything that helps later in instantiation
-    config["VOCAB_SIZE"] = VOCAB_SIZE
-    config["NUM_CLASSES"] = NUM_CLASSES
-    with open(os.path.join(EXP_DIR_PATH, "config.json"), "w") as fp:
-        json.dump(config, fp)
-
-    # load the word embeddings
-    embeddings = train_dataset.word_vocab.load_embedding()
-    embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)
-
     # instantiate the elmo embedder
     elmo_embedder = BowElmoEmbedder(
         layer_aggregation="sum",
-        cuda_device_id=-1 if DEVICE == "cpu" else int(DEVICE.split("cuda:")[1]),
+        cuda_device_id=-1
+        if args.device == "cpu"
+        else int(args.device.split("cuda:")[1]),
     )
 
     # instantiate the vanilla embedder
-    vanilla_embedder = VanillaEmbedder(
-        embedding=embeddings, embedding_dim=EMBEDDING_DIMENSION
-    )
+    vanilla_embedder = WordEmbedder(embedding_type=args.emb_type)
 
     # concat the embeddings
     embedder = ConcatEmbedders([vanilla_embedder, elmo_embedder])
 
     encoder = LSTM2VecEncoder(
-        emb_dim=EMBEDDING_DIMENSION + 1024,
         embedder=embedder,
-        hidden_dim=HIDDEN_DIMENSION,
-        bidirectional=BIDIRECTIONAL,
-        combine_strategy=COMBINE_STRATEGY,
-        device=torch.device(DEVICE),
+        hidden_dim=args.hidden_dim,
+        bidirectional=args.bidirectional,
+        combine_strategy=args.combine_strategy,
+        device=torch.device(args.device),
     )
 
     encoding_dim = (
-        2 * HIDDEN_DIMENSION
-        if BIDIRECTIONAL and COMBINE_STRATEGY == "concat"
-        else HIDDEN_DIMENSION
+        2 * args.hidden_dim
+        if args.bidirectional and args.combine_strategy == "concat"
+        else args.hidden_dim
     )
 
     model = SimpleClassifier(
         encoder=encoder,
         encoding_dim=encoding_dim,
-        num_classes=NUM_CLASSES,
+        num_classes=23,
         classification_layer_bias=True,
+        datasets_manager=data_manager,
     )
 
-    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
+    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)
+    train_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    dev_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=data_manager)
 
     engine = Engine(
         model=model,
-        train_dataset=train_dataset,
-        validation_dataset=validation_dataset,
-        test_dataset=test_dataset,
+        datasets_manager=data_manager,
         optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=MODEL_SAVE_DIR,
-        num_epochs=NUM_EPOCHS,
-        save_every=SAVE_EVERY,
-        log_train_metrics_every=LOG_TRAIN_METRICS_EVERY,
-        tensorboard_logdir=TENSORBOARD_LOGDIR,
-        device=torch.device(DEVICE),
-        metric=metric,
+        batch_size=args.bs,
+        save_dir=args.model_save_dir,
+        num_epochs=args.epochs,
+        save_every=args.save_every,
+        train_metric=train_metric,
+        validation_metric=dev_metric,
+        test_metric=test_metric,
+        log_train_metrics_every=args.log_train_metrics_every,
+        device=torch.device(args.device),
         use_wandb=True,
-        experiment_name=EXP_NAME,
-        experiment_hyperparams=config,
+        experiment_name=args.exp_name,
+        experiment_hyperparams=vars(args),
         track_for_best="macro_fscore",
+        sample_proportion=args.sample_proportion,
     )
 
     engine.run()
diff --git a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.sh b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.sh
index e453cfb..7f2470b 100755
--- a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.sh
+++ b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.sh
@@ -6,23 +6,17 @@ python ${SCRIPT_FILE} \
 --exp_name "parsect_elmo_bi_lstm_lc" \
 --exp_dir_path  "./output" \
 --model_save_dir "./output/checkpoints" \
---vocab_store_location "./output/vocab.json" \
 --device cpu \
---max_num_words 1000 \
---max_length 10 \
---debug \
---debug_dataset_proportion 0.01 \
 --bs 10 \
---emb_type random \
---emb_dim 300 \
+--emb_type "glove_6B_50" \
 --hidden_dim 512 \
 --lr 1e-3 \
 --bidirectional \
 --combine_strategy concat \
 --epochs 1 \
 --save_every 5 \
---log_train_metrics_every 5
-
+--log_train_metrics_every 5 \
+--sample_proportion 0.01
 
 
 
diff --git a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.toml b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.toml
index 65119cd..2566735 100644
--- a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.toml
+++ b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm.toml
@@ -1,20 +1,12 @@
 [experiment]
-    exp_name = "sectlabel-bow-random"
-    exp_dir = "sectlabel_bow_random_toml"
+    exp_name = "sectlabel-elmo-bilstm"
+    exp_dir = "sectlabel_elmo_bilstm"
 
 [dataset]
-	class = "SectLabelDataset"
-	train_filename="sectLabel.train.data"
-	valid_filename="sectLabel.train.data"
-	test_filename="sectLabel.train.data"
-	[dataset.args]
-	max_num_words = 1000
-	max_instance_length = 100
-	word_vocab_store_location = "sectlabel_bow_random_toml/vocab.json"
-	debug = true
-	debug_dataset_proportion = 0.1
-	word_embedding_type = "random"
-	word_embedding_dimension = 300
+	class = "TextClassificationDatasetManager"
+	train_filename="sectLabel.train"
+	dev_filename="sectLabel.dev"
+	test_filename="sectLabel.test"
 
 [model]
     class="SimpleClassifier"
@@ -22,27 +14,26 @@
     num_classes=23
     classification_layer_bias=true
     [model.encoder]
-        emb_dim = 1324
         class="LSTM2VecEncoder"
         dropout_value = 0.5
         hidden_dim=100
         combine_strategy="concat"
         bidirectional=true
         [[model.encoder.embedder]]
-        class="VanillaEmbedder"
-        embed="word_vocab"
-        freeze=false
+        class="WordEmbedder"
+        embedding_type="glove_6B_50"
         [[model.encoder.embedder]]
         class="BowElmoEmbedder"
 
 [engine]
     batch_size=32
-    save_dir="sectlabel_bow_random_toml/checkpoints"
+    save_dir="sectlabel_elmo_bilstm/checkpoints"
     num_epochs=1
     save_every=10
     log_train_metrics_every=10
     device="cpu"
     gradient_norm_clip_value=5.0
+    sample_proportion=0.01
     [engine.metric]
         class="PrecisionRecallFMeasure"
     [engine.optimizer]
diff --git a/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm_infer.py b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm_infer.py
new file mode 100644
index 0000000..685063d
--- /dev/null
+++ b/examples/sectlabel_elmo_bilstm/sectlabel_elmo_bilstm_infer.py
@@ -0,0 +1,79 @@
+import sciwing.constants as constants
+from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
+)
+import torch
+import pathlib
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
+
+
+def build_sectlabel_elmobilstm_model(dirname: str):
+    exp_dirpath = pathlib.Path(dirname)
+    DATA_PATH = pathlib.Path(DATA_DIR)
+
+    train_file = DATA_PATH.joinpath("sectLabel.train")
+    dev_file = DATA_PATH.joinpath("sectLabel.dev")
+    test_file = DATA_PATH.joinpath("sectLabel.test")
+
+    data_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+    DEVICE = "cpu"
+    EMBEDDING_TYPE = "glove_6B_50"
+    HIDDEN_DIM = 512
+    BIDIRECTIONAL = True
+    COMBINE_STRATEGY = "concat"
+
+    elmo_embedder = BowElmoEmbedder(
+        cuda_device_id=-1 if DEVICE == "cpu" else int(DEVICE.split("cuda:")[1])
+    )
+
+    vanilla_embedder = WordEmbedder(embedding_type=EMBEDDING_TYPE)
+
+    embedders = ConcatEmbedders([vanilla_embedder, elmo_embedder])
+
+    encoder = LSTM2VecEncoder(
+        embedder=embedders,
+        hidden_dim=HIDDEN_DIM,
+        bidirectional=BIDIRECTIONAL,
+        combine_strategy=COMBINE_STRATEGY,
+        device=torch.device(DEVICE),
+    )
+
+    encoding_dim = (
+        2 * HIDDEN_DIM if BIDIRECTIONAL and COMBINE_STRATEGY == "concat" else HIDDEN_DIM
+    )
+
+    model = SimpleClassifier(
+        encoder=encoder,
+        encoding_dim=encoding_dim,
+        num_classes=23,
+        classification_layer_bias=True,
+        datasets_manager=data_manager,
+    )
+
+    inference = ClassificationInference(
+        model=model,
+        model_filepath=str(exp_dirpath.joinpath("checkpoints", "best_model.pt")),
+        datasets_manager=data_manager,
+    )
+    return inference
+
+
+if __name__ == "__main__":
+    dirname = pathlib.Path(".", "output")
+    infer = build_sectlabel_elmobilstm_model(str(dirname))
+    infer.run_inference()
+    infer.report_metrics()
diff --git a/sciwing/commands/test.py b/sciwing/commands/test.py
index a8a13d5..53fbaf0 100644
--- a/sciwing/commands/test.py
+++ b/sciwing/commands/test.py
@@ -42,11 +42,11 @@ def test(toml_filename):
     exp_dirpath = pathlib.Path(exp_dirpath)
     model_filepath = exp_dirpath.joinpath("checkpoints", "best_model.pt")
     model = sciwing_toml_runner.model
-    dataset = sciwing_toml_runner.all_datasets["test"]
+    data_manager = sciwing_toml_runner.datasets_manager
     model_class = sciwing_toml_runner.model_section.get("class")
     inference_cls = class_infer_client_mapping.get(model_class)
     inference_client = inference_cls(
-        model=model, model_filepath=model_filepath, dataset=dataset
+        model=model, model_filepath=model_filepath, datasets_manager=data_manager
     )
     inference_client.run_test()
     inference_client.report_metrics()
diff --git a/sciwing/numericalizer/__init__.py b/sciwing/data/__init__.py
similarity index 100%
rename from sciwing/numericalizer/__init__.py
rename to sciwing/data/__init__.py
diff --git a/sciwing/data/datasets_manager.py b/sciwing/data/datasets_manager.py
new file mode 100644
index 0000000..fa8f431
--- /dev/null
+++ b/sciwing/data/datasets_manager.py
@@ -0,0 +1,220 @@
+"""
+The dataset manager orchestrates different datasets for a problem
+It is a container for the train dev and test datasets
+"""
+from torch.utils.data import Dataset
+from sciwing.vocab.vocab import Vocab
+from sciwing.numericalizers.base_numericalizer import BaseNumericalizer
+from sciwing.data.line import Line
+from typing import Dict, List, Any
+from collections import defaultdict
+import wasabi
+
+
+class DatasetsManager:
+    def __init__(
+        self,
+        train_dataset: Dataset,
+        dev_dataset: Dataset = None,
+        test_dataset: Dataset = None,
+        namespace_vocab_options: Dict[str, Dict[str, Any]] = None,
+        namespace_numericalizer_map: Dict[str, BaseNumericalizer] = None,
+        batch_size: int = 32,
+    ):
+        """
+
+        Parameters
+        ----------
+        train_dataset : Dataset
+            A pytorch dataset that represents training data
+        dev_dataset : Dataset
+            A pytorch dataset that represents validation data
+        test_dataset : Dataset
+            A pytorch dataset that represents test data
+        namespace_vocab_options : Dict[str, Dict[str, Any]]
+            For every namespace you can give a set of options that will
+            be passed down to Vocab.
+        namespace_numericalizer_map: Dict[str, Dict[str, Any]]
+            For every namespace, you can give a set of options here that will
+            be passed down to the Numericalizer Instances
+        batch_size: int
+            Batch size for loading the datasets
+        """
+        self.train_dataset = train_dataset
+        self.dev_dataset = dev_dataset
+        self.test_dataset = test_dataset
+        self.label_namespaces: List[str] = None  # Holds the label namespaces
+        self.msg_printer = wasabi.Printer()
+
+        if namespace_vocab_options is None:
+            self.namespace_vocab_options = {}
+        else:
+            self.namespace_vocab_options = namespace_vocab_options
+
+        self.batch_size = batch_size
+
+        self.namespace_to_numericalizer: Dict[
+            str, BaseNumericalizer
+        ] = namespace_numericalizer_map
+
+        # Build vocab using the datasets passed
+        self.namespace_to_vocab: Dict[str, Vocab] = self.build_vocab()
+
+        # sets the vocab for the appropriate numericalizers
+        self.namespace_to_numericalizer = self.build_numericalizers()
+        self.namespaces = list(self.namespace_to_vocab.keys())
+        self.num_labels = {}
+        for namespace in self.label_namespaces:
+            self.num_labels[namespace] = self.namespace_to_vocab[
+                namespace
+            ].get_vocab_len()
+
+    def build_vocab(self) -> Dict[str, Vocab]:
+        """ Returns a vocab for each of the namespace
+        The namespace identifies the kind of tokens
+        Some tokens correspond to words
+        Some tokens may correspond to characters.
+        Some tokens may correspond to Bert style tokens
+
+        Returns
+        -------
+        Dict[str, Vocab]
+            A vocab corresponding to each of the
+
+        """
+        lines = self.train_dataset.lines
+        labels = self.train_dataset.labels
+
+        namespace_to_instances: Dict[str, List[List[str]]] = defaultdict(list)
+        for line in lines:
+            namespace_tokens = line.tokens
+            for namespace, tokens in namespace_tokens.items():
+                tokens = [tok.text for tok in tokens]
+                namespace_to_instances[namespace].append(tokens)
+        for label in labels:
+            namespace_tokens = label.tokens
+            for namespace, tokens in namespace_tokens.items():
+                tokens = [tok.text for tok in tokens]
+                namespace_to_instances[namespace].append(tokens)
+
+        self.label_namespaces = list(labels[0].tokens.keys())
+
+        namespace_to_vocab: Dict[str, Vocab] = {}
+
+        # This always builds a vocab from instances
+        for namespace, instances in namespace_to_instances.items():
+            namespace_to_vocab[namespace] = Vocab(
+                instances=instances, **self.namespace_vocab_options.get(namespace, {})
+            )
+            namespace_to_vocab[namespace].build_vocab()
+        return namespace_to_vocab
+
+    def print_stats(self):
+        """ Print different stats with respect to the train, dev and test datasets
+
+        Returns
+        -------
+        None
+
+        """
+        len_train_dataset = len(self.train_dataset)
+        len_dev_dataset = len(self.dev_dataset)
+        len_test_dataset = len(self.test_dataset)
+
+        self.msg_printer.info(f"Num of training examples: {len_train_dataset}")
+        self.msg_printer.info(f"Num of dev examples {len_dev_dataset}")
+        self.msg_printer.info(f"Num of test examples {len_test_dataset}")
+
+        # print namespace to vocab stats
+        for namespace in self.namespaces:
+            vocab = self.namespace_to_vocab[namespace]
+            self.msg_printer.divider(text=f"Namespace {namespace}")
+            vocab.print_stats()
+
+    def build_numericalizers(self):
+        namespace_numericalizer_map: Dict[str, BaseNumericalizer] = {}
+        for namespace, numericalizer in self.namespace_to_numericalizer.items():
+            numericalizer.vocabulary = self.namespace_to_vocab[namespace]
+            namespace_numericalizer_map[namespace] = numericalizer
+
+        return namespace_numericalizer_map
+
+    def get_idx_label_mapping(self, label_namespace: str):
+        label_vocab = self.namespace_to_vocab[label_namespace]
+        return label_vocab.idx2token
+
+    def get_label_idx_mapping(self, label_namespace: str):
+        label_vocab = self.namespace_to_vocab[label_namespace]
+        return label_vocab.token2idx
+
+    def make_line(self, line: str):
+        """ Makes a line object from string, having some characteristics as the lines used
+        by the datasets
+
+        Parameters
+        ----------
+        line : str
+
+        Returns
+        -------
+        Line
+
+        """
+        line_ = Line(text=line, tokenizers=self.train_dataset.tokenizers)
+        return line_
+
+    @property
+    def train_dataset(self):
+        return self._train_dataset
+
+    @train_dataset.setter
+    def train_dataset(self, value):
+        self._train_dataset = value
+
+    @property
+    def dev_dataset(self):
+        return self._dev_dataset
+
+    @dev_dataset.setter
+    def dev_dataset(self, value):
+        self._dev_dataset = value
+
+    @property
+    def test_dataset(self):
+        return self._test_dataset
+
+    @test_dataset.setter
+    def test_dataset(self, value):
+        self._test_dataset = value
+
+    @property
+    def num_labels(self):
+        return self._num_labels
+
+    @num_labels.setter
+    def num_labels(self, value):
+        self._num_labels = value
+
+    @property
+    def namespace_to_vocab(self):
+        return self._namespace_to_vocab
+
+    @namespace_to_vocab.setter
+    def namespace_to_vocab(self, value):
+        self._namespace_to_vocab = value
+
+    @property
+    def namespaces(self):
+        return self._namespaces
+
+    @namespaces.setter
+    def namespaces(self, value):
+        self._namespaces = value
+
+    @property
+    def label_namespaces(self):
+        return self._label_namespaces
+
+    @label_namespaces.setter
+    def label_namespaces(self, value):
+        self._label_namespaces = value
diff --git a/sciwing/data/label.py b/sciwing/data/label.py
new file mode 100644
index 0000000..2b220c2
--- /dev/null
+++ b/sciwing/data/label.py
@@ -0,0 +1,56 @@
+from sciwing.data.token import Token
+from typing import Union, List, Dict
+from collections import defaultdict
+
+
+class Label:
+    def __init__(self, text: str, namespace: str = "label"):
+        """ Defines a single label for an example
+        We will only consider one namespace for this class
+        Also we will only consider a single token name for ever label
+
+        label = Label(text="title")
+
+        Parameters
+        ----------
+        text : str
+        namespace : str
+        """
+        self.text = text
+        self.namespace = namespace
+        self.tokens: Dict[str, List[Token]] = defaultdict(list)
+        self.add_token(token=self.text, namespace=namespace)
+
+    @property
+    def text(self):
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        self._text = value
+
+    @property
+    def namespace(self):
+        return self._namespace
+
+    @namespace.setter
+    def namespace(self, value):
+        self._namespace = value
+
+    @property
+    def tokens(self):
+        return self._tokens
+
+    @tokens.setter
+    def tokens(self, value):
+        self._tokens = value
+
+    def add_token(self, token: Union[Token, str], namespace: str):
+        if isinstance(token, str):
+            token = Token(token)
+
+        self.tokens[namespace].append(token)
+
+    def add_tokens(self, tokens: Union[List[str], List[Token]], namespace: str):
+        for token in tokens:
+            self.add_token(token, namespace=namespace)
diff --git a/sciwing/data/line.py b/sciwing/data/line.py
new file mode 100644
index 0000000..79c3a69
--- /dev/null
+++ b/sciwing/data/line.py
@@ -0,0 +1,47 @@
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
+from sciwing.data.token import Token
+from typing import Union, List, Dict
+from collections import defaultdict
+import copy
+
+
+class Line:
+    def __init__(self, text: str, tokenizers: Dict[str, BaseTokenizer] = None):
+        if tokenizers is None:
+            tokenizers = {"tokens": WordTokenizer()}
+        self.text = text
+        self.tokenizers = tokenizers
+        self.tokens: Dict[str, List[Token]] = defaultdict(list)
+        self.namespaces = list(tokenizers.keys())
+
+        for namespace, tokenizer in tokenizers.items():
+            tokens = tokenizer.tokenize(text)
+            for token in tokens:
+                self.add_token(token=token, namespace=namespace)
+
+    def add_token(self, token: Union[Token, str], namespace: str):
+        if isinstance(token, str):
+            token = Token(token)
+
+        self.tokens[namespace].append(token)
+
+    def add_tokens(self, tokens: Union[List[str], List[Token]], namespace: str):
+        for token in tokens:
+            self.add_token(token, namespace=namespace)
+
+    @property
+    def tokens(self):
+        return self._tokens
+
+    @tokens.setter
+    def tokens(self, value):
+        self._tokens = value
+
+    @property
+    def namespaces(self):
+        return self._namespaces
+
+    @namespaces.setter
+    def namespaces(self, value):
+        self._namespaces = value
diff --git a/sciwing/data/sciwing_data_loader.py b/sciwing/data/sciwing_data_loader.py
new file mode 100644
index 0000000..f2d82d2
--- /dev/null
+++ b/sciwing/data/sciwing_data_loader.py
@@ -0,0 +1,44 @@
+from torch.utils.data.dataloader import DataLoader
+
+
+class SciwingDataLoader(DataLoader):
+    def __init__(
+        self,
+        dataset,
+        batch_size=1,
+        shuffle=False,
+        sampler=None,
+        batch_sampler=None,
+        num_workers=8,
+        drop_last=False,
+        timeout=0,
+        worker_init_fn=None,
+    ):
+        """ This is an extension of the PyTorch DataLoader
+        The collate function is always a list. The rest of the parameters
+        can be sent by the user
+
+        Parameters
+        ----------
+        dataset : Dataset
+        batch_size : int
+        shuffle
+        sampler: torch.utils.data.Sampler
+        batch_sampler
+        num_workers: int
+        drop_last : bool
+        timeout : int
+        worker_init_fn
+        """
+        super(SciwingDataLoader, self).__init__(
+            dataset=dataset,
+            batch_size=batch_size,
+            shuffle=shuffle,
+            batch_sampler=batch_sampler,
+            num_workers=num_workers,
+            drop_last=drop_last,
+            timeout=timeout,
+            worker_init_fn=worker_init_fn,
+            collate_fn=list,
+            sampler=sampler,
+        )
diff --git a/sciwing/data/seq_label.py b/sciwing/data/seq_label.py
new file mode 100644
index 0000000..f7540f6
--- /dev/null
+++ b/sciwing/data/seq_label.py
@@ -0,0 +1,56 @@
+from typing import List, Dict, Union
+from sciwing.data.token import Token
+from collections import defaultdict
+
+
+class SeqLabel:
+    def __init__(self, labels: List[str], namespace="seq_label"):
+        """ Sequential Labels are used to label every token in a line. This mostly gets used for sequential
+
+        Parameters
+        ----------
+        labels : List[str]
+            A list of labels
+        namespace : str
+            The namespace used for this label
+        """
+        self.labels = labels
+        self.namespace = namespace
+        self.tokens: Dict[str, List[Token]] = defaultdict(list)
+
+        for label in labels:
+            self.add_token(token=label, namespace=self.namespace)
+
+    @property
+    def labels(self):
+        return self._labels
+
+    @labels.setter
+    def labels(self, value):
+        self._labels = value
+
+    @property
+    def namespace(self):
+        return self._namespace
+
+    @namespace.setter
+    def namespace(self, value):
+        self._namespace = value
+
+    @property
+    def tokens(self):
+        return self._tokens
+
+    @tokens.setter
+    def tokens(self, value):
+        self._tokens = value
+
+    def add_token(self, token: Union[str, Token], namespace=namespace):
+        if isinstance(token, str):
+            token = Token(token)
+
+        self.tokens[namespace].append(token)
+
+    def add_tokens(self, tokens: Union[List[str], List[Token]], namespace: str):
+        for token in tokens:
+            self.add_token(token, namespace=namespace)
diff --git a/sciwing/data/token.py b/sciwing/data/token.py
new file mode 100644
index 0000000..87820cb
--- /dev/null
+++ b/sciwing/data/token.py
@@ -0,0 +1,38 @@
+from typing import Dict
+import torch
+
+
+class Token:
+    def __init__(self, text: str):
+        self.text = text
+        self.sub_tokens = []
+
+        # a token can hold different kinds of embeddings
+        # this is a mapping from the embedding_type to the embedding itself
+        self._embedding: Dict[str, torch.FloatTensor] = {}
+
+    @property
+    def text(self):
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        self._text = value
+
+    @property
+    def len(self):
+        return len(self.text)
+
+    def set_embedding(self, name: str, value: torch.FloatTensor):
+        self._embedding[name] = value
+
+    def get_embedding(self, name: str):
+        return self._embedding[name]
+
+    @property
+    def sub_tokens(self):
+        return self._subtokens
+
+    @sub_tokens.setter
+    def sub_tokens(self, value):
+        self._subtokens = value
diff --git a/sciwing/datasets/__init__.py b/sciwing/datasets/__init__.py
index 5d6c0aa..ff9d718 100644
--- a/sciwing/datasets/__init__.py
+++ b/sciwing/datasets/__init__.py
@@ -1,3 +1,6 @@
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
-from sciwing.datasets.seq_labeling.science_ie_dataset import ScienceIEDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDataset,
+)
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
diff --git a/sciwing/datasets/classification/base_text_classification.py b/sciwing/datasets/classification/base_text_classification.py
index 5ceb6eb..37bd367 100644
--- a/sciwing/datasets/classification/base_text_classification.py
+++ b/sciwing/datasets/classification/base_text_classification.py
@@ -1,132 +1,26 @@
 from abc import ABCMeta, abstractmethod
-from typing import Union, Dict, List, Optional
-from sciwing.tokenizers.word_tokenizer import WordTokenizer
-from sklearn.model_selection import StratifiedShuffleSplit
-import numpy as np
+from typing import Dict, List
+from sciwing.data.line import Line
+from sciwing.data.label import Label
+from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
 
 
 class BaseTextClassification(metaclass=ABCMeta):
-    def __init__(
-        self,
-        filename: str,
-        dataset_type: str,
-        max_num_words: int,
-        max_instance_length: int,
-        word_vocab_store_location: str,
-        debug: bool = False,
-        debug_dataset_proportion: float = 0.1,
-        word_embedding_type: Union[str, None] = None,
-        word_embedding_dimension: Union[int, None] = None,
-        word_start_token: str = "<SOS>",
-        word_end_token: str = "<EOS>",
-        word_pad_token: str = "<PAD>",
-        word_unk_token: str = "<UNK>",
-        train_size: float = 0.8,
-        test_size: float = 0.2,
-        validation_size: float = 0.5,
-        word_tokenizer=WordTokenizer(),
-        word_tokenization_type="vanilla",
-    ):
+    def __init__(self, filename: str, tokenizers: Dict[str, BaseTokenizer]):
         """ Base Text Classification Dataset to be inherited by all text classification datasets
 
         Parameters
         ----------
-        filename : str
-            Path of file where the text classification dataset is stored. Ideally this should have
-            an example text and label separated by space. But it is left to the specific dataset to
-            handle the different ways in which file could be structured
-        dataset_type : str
-            One of ``[train, valid, test]``
-        max_num_words : int
-            The top ``max_num_words`` will be considered for building vocab
-        max_instance_length : int
-            Every instance in the dataset will be padded to or curtailed to ``max_length`` number of
-            tokens
-        word_vocab_store_location : str
-            Vocabulary once built will be stored in this location
-            If the vocabulary already exists then it will be loaded from the filepath
-        debug : bool
-            Useful to build a small dataset for debugging purposes. If ``True``, then a smaller
-            random version of the dataset should be returned. If ``True`` then
-            ``debug_dataset_proportion`` will be the proportion of the dataset that will be returned
-        debug_dataset_proportion : int
-            Percent of dataset that will be returned for debug purposes. Should be between 0 and 1
-        word_embedding_type : str
-            The kind of word embedding that will be associated with the words in the database
-            Any of the ``allowed_types`` in vocab.EmbeddingLoader is allowed here
-        word_embedding_dimension : int
-            Dimension of word embedding
-        word_start_token : str
-            Start token appended at the beginning of every instance
-        word_end_token : str
-            End token appended at the end of every instance
-        word_pad_token : str
-            Pad token to be used for padding
-        word_unk_token : str
-            All OOV words (if they are less frequent than ``max_words`` or word is in
-            test but not in train) will be mapped to ``unk_token``
-        train_size : str
-            Percentage of the instances to be used for training
-        test_size : str
-            Remaining percentage that will be used for testing
-        validation_size : str
-            Percentage of test data that will be used for validation
-        word_tokenizer : WordTokenizer
-            Word Tokenizer to be used for the dataset. You can reference
-            ``tokenizers.WordTokenizer`` for more information
-        word_tokenization_type : str
-            The type of word tokenization that the word tokenizer represents
-        """
-        pass
-
-    @classmethod
-    @abstractmethod
-    def get_classname2idx(cls) -> Dict[str, int]:
-        """ Returns the mapping from classname to idx
-
-        Returns
-        -------
-        Dict[str, int]
-            The mapping between class name to idx.
-
-        """
-        pass
-
-    @abstractmethod
-    def get_num_classes(self) -> int:
-        """ Return the number of classes in the dataset
-
-        Returns
-        -------
-        int
-            Number of classes in the dataset
-        """
-        pass
-
-    @abstractmethod
-    def get_class_names_from_indices(self, indices: List[int]) -> List[str]:
-        """ Return a set of class names from indices. Utility method useful for display purposes
-
-        Parameters
-        ----------
-        indices : List[int]
-            List of indices where every index should be between [0, ``num_classes``)
-
-        Returns
-        -------
-        List[str]
-            List of class names for ``indices``
-        """
-        pass
+        filename: str
+            Full path of the filename where classification dataset is stored
+        tokenizers: Dict[str, BaseTokenizer]
+            The mapping between namespace and a tokenizer
 
-    @abstractmethod
-    def print_stats(self):
-        """ Free form method to print some stats for the dataset
         """
         pass
 
     @abstractmethod
-    def get_lines_labels(self, filename: str) -> (List[str], List[str]):
+    def get_lines_labels(self) -> (List[Line], List[Label]):
         """ A list of lines from the file and a list of corresponding labels
 
         This method is to be implemented by a new dataset. The decision on
@@ -135,8 +29,7 @@ class BaseTextClassification(metaclass=ABCMeta):
 
         Parameters
         ---------
-        filename
-            The filename where the dataset is stored
+
 
         Returns
         -------
@@ -144,104 +37,5 @@ class BaseTextClassification(metaclass=ABCMeta):
             Returns a list of text examples and corresponding labels
 
         """
-        pass
-
-    def get_train_valid_test_stratified_split(
-        self, lines: List[str], labels: List[str], classname2idx: Dict[str, int]
-    ) -> ((List[str], List[str]), (List[str], List[str]), (List[str], List[str])):
-        len_lines = len(lines)
-        len_labels = len(labels)
-
-        assert len_lines == len_labels
 
-        train_test_spliiter = StratifiedShuffleSplit(
-            n_splits=1,
-            test_size=self.test_size,
-            train_size=self.train_size,
-            random_state=1729,
-        )
-
-        features = np.random.rand(len_lines)
-        labels_idx_array = np.array([classname2idx[label] for label in labels])
-
-        splits = list(train_test_spliiter.split(features, labels_idx_array))
-        train_indices, test_valid_indices = splits[0]
-
-        train_lines = [lines[idx] for idx in train_indices]
-        train_labels = [labels[idx] for idx in train_indices]
-
-        test_valid_lines = [lines[idx] for idx in test_valid_indices]
-        test_valid_labels = [labels[idx] for idx in test_valid_indices]
-
-        validation_test_splitter = StratifiedShuffleSplit(
-            n_splits=1,
-            test_size=self.validation_size,
-            train_size=1 - self.validation_size,
-            random_state=1729,
-        )
-
-        len_test_valid_lines = len(test_valid_lines)
-        len_test_valid_labels = len(test_valid_labels)
-
-        assert len_test_valid_labels == len_test_valid_lines
-
-        test_valid_features = np.random.rand(len_test_valid_lines)
-        test_valid_labels_idx_array = np.array(
-            [classname2idx[label] for label in test_valid_labels]
-        )
-
-        test_valid_splits = list(
-            validation_test_splitter.split(
-                test_valid_features, test_valid_labels_idx_array
-            )
-        )
-        test_indices, validation_indices = test_valid_splits[0]
-
-        test_lines = [test_valid_lines[idx] for idx in test_indices]
-        test_labels = [test_valid_labels[idx] for idx in test_indices]
-
-        validation_lines = [test_valid_lines[idx] for idx in validation_indices]
-        validation_labels = [test_valid_labels[idx] for idx in validation_indices]
-
-        return (
-            (train_lines, train_labels),
-            (validation_lines, validation_labels),
-            (test_lines, test_labels),
-        )
-
-    @classmethod
-    @abstractmethod
-    def emits_keys(cls) -> Dict[str, str]:
-        """ Specify the keys that will be emitted in the ``instance_dict`` fo the dataset
-
-        The ``instance_dict`` is a dictionary of string to tensors. The ``instance_dict`` can
-        contain various keys depending on the dataset that is being used and the model that
-        is built using the dataset. The function should provides means to inspect the different
-        keys emitted by the classification dataset and the description of what they mean
-
-        Returns
-        -------
-        Dict[str, str]
-            A dictionary representing different keys emitted and their corresponding human
-            readable descriptions
-        """
-        pass
-
-    def get_iter_dict(self, lines: List[str], labels: Optional[List[str]]):
-        """ Given the lines and labels returns the ``iter_dict`` for the dataset
-
-        Parameters
-        ----------
-        lines : List[str]
-         List of lines to process
-
-        labels : List[str]
-        The corresponding set of labels
-
-        Returns
-        -------
-        dict[str, Any]
-            Returns the ``iter_dict`` of the dataset
-
-        """
-        pass
+    pass
diff --git a/sciwing/datasets/classification/generic_sect_dataset.py b/sciwing/datasets/classification/generic_sect_dataset.py
index 28588f9..a01cc9a 100644
--- a/sciwing/datasets/classification/generic_sect_dataset.py
+++ b/sciwing/datasets/classification/generic_sect_dataset.py
@@ -3,7 +3,7 @@ from sciwing.utils.common import convert_generic_sect_to_json
 import wasabi
 from sciwing.tokenizers.word_tokenizer import WordTokenizer
 from sciwing.vocab.vocab import Vocab
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 from sciwing.utils.common import pack_to_length
 import torch
 from sciwing.datasets.classification.base_text_classification import (
@@ -118,9 +118,10 @@ class GenericSectDataset(BaseTextClassification, ClassNursery):
             headers.append(header)
             labels.append(label)
 
-        (train_headers, train_labels), (valid_headers, valid_labels), (
-            test_headers,
-            test_labels,
+        (
+            (train_headers, train_labels),
+            (valid_headers, valid_labels),
+            (test_headers, test_labels),
         ) = self.get_train_valid_test_stratified_split(
             headers, labels, self.classname2idx
         )
diff --git a/sciwing/datasets/classification/sectlabel_dataset.py b/sciwing/datasets/classification/sectlabel_dataset.py
index 3a00d97..55f0a9e 100644
--- a/sciwing/datasets/classification/sectlabel_dataset.py
+++ b/sciwing/datasets/classification/sectlabel_dataset.py
@@ -117,9 +117,10 @@ class SectLabelDataset(BaseTextClassification, ClassNursery):
             texts.append(text)
             labels.append(label)
 
-        (train_lines, train_labels), (validation_lines, validation_labels), (
-            test_lines,
-            test_labels,
+        (
+            (train_lines, train_labels),
+            (validation_lines, validation_labels),
+            (test_lines, test_labels),
         ) = self.get_train_valid_test_stratified_split(
             texts, labels, self.classname2idx
         )
diff --git a/sciwing/datasets/classification/text_classification_dataset.py b/sciwing/datasets/classification/text_classification_dataset.py
new file mode 100644
index 0000000..00c3f33
--- /dev/null
+++ b/sciwing/datasets/classification/text_classification_dataset.py
@@ -0,0 +1,126 @@
+from typing import Dict, List, Any
+from sciwing.data.line import Line
+from sciwing.data.label import Label
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
+from torch.utils.data import Dataset
+from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
+from sciwing.numericalizers.base_numericalizer import BaseNumericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
+from sciwing.datasets.classification.base_text_classification import (
+    BaseTextClassification,
+)
+from sciwing.data.datasets_manager import DatasetsManager
+from sciwing.utils.class_nursery import ClassNursery
+
+
+class TextClassificationDataset(BaseTextClassification, Dataset):
+    """ This represents a dataset that is of the form
+    line1###label1
+    line2###label2
+    line3###label3
+    .
+    .
+    .
+    """
+
+    def __init__(
+        self, filename: str, tokenizers: Dict[str, BaseTokenizer] = WordTokenizer()
+    ):
+        super().__init__(filename, tokenizers)
+        self.filename = filename
+        self.tokenizers = tokenizers
+        self.lines, self.labels = self.get_lines_labels()
+
+    def get_lines_labels(self) -> (List[Line], List[Label]):
+        lines: List[Line] = []
+        labels: List[Label] = []
+
+        with open(self.filename) as fp:
+            for line in fp:
+                line, label = line.split("###")
+                line = line.strip()
+                label = label.strip()
+                line_instance = Line(text=line, tokenizers=self.tokenizers)
+                label_instance = Label(text=label)
+                lines.append(line_instance)
+                labels.append(label_instance)
+
+        return lines, labels
+
+    def __len__(self):
+        return len(self.lines)
+
+    def __getitem__(self, idx) -> (Line, Label):
+        line, label = self.lines[idx], self.labels[idx]
+        return line, label
+
+    @property
+    def lines(self):
+        return self._lines
+
+    @lines.setter
+    def lines(self, value):
+        self._lines = value
+
+    @property
+    def labels(self):
+        return self._labels
+
+    @labels.setter
+    def labels(self, value):
+        self._labels = value
+
+
+class TextClassificationDatasetManager(DatasetsManager, ClassNursery):
+    def __init__(
+        self,
+        train_filename: str,
+        dev_filename: str,
+        test_filename: str,
+        tokenizers: Dict[str, BaseTokenizer] = None,
+        namespace_vocab_options: Dict[str, Dict[str, Any]] = None,
+        namespace_numericalizer_map: Dict[str, BaseNumericalizer] = None,
+        batch_size: int = 10,
+    ):
+        self.train_filename = train_filename
+        self.dev_filename = dev_filename
+        self.test_filename = test_filename
+        self.tokenizers = tokenizers or {
+            "tokens": WordTokenizer(),
+            "char_tokens": CharacterTokenizer(),
+        }
+        self.namespace_vocab_options = namespace_vocab_options or {
+            "char_tokens": {
+                "start_token": " ",
+                "end_token": " ",
+                "pad_token": " ",
+                "unk_token": " ",
+            },
+            "label": {"include_special_vocab": False},
+        }
+        self.namespace_numericalizer_map = namespace_numericalizer_map or {
+            "tokens": Numericalizer(),
+            "char_tokens": Numericalizer(),
+        }
+        self.namespace_numericalizer_map["label"] = Numericalizer()
+        self.batch_size = batch_size
+
+        self.train_dataset = TextClassificationDataset(
+            filename=self.train_filename, tokenizers=self.tokenizers
+        )
+        self.dev_dataset = TextClassificationDataset(
+            filename=self.dev_filename, tokenizers=self.tokenizers
+        )
+        self.test_dataset = TextClassificationDataset(
+            filename=self.test_filename, tokenizers=self.tokenizers
+        )
+
+        super(TextClassificationDatasetManager, self).__init__(
+            train_dataset=self.train_dataset,
+            dev_dataset=self.dev_dataset,
+            test_dataset=self.test_dataset,
+            namespace_vocab_options=self.namespace_vocab_options,
+            namespace_numericalizer_map=self.namespace_numericalizer_map,
+            batch_size=batch_size,
+        )
diff --git a/sciwing/datasets/seq_labeling/base_seq_labeling.py b/sciwing/datasets/seq_labeling/base_seq_labeling.py
index f65c960..716aaa5 100644
--- a/sciwing/datasets/seq_labeling/base_seq_labeling.py
+++ b/sciwing/datasets/seq_labeling/base_seq_labeling.py
@@ -1,33 +1,12 @@
 from abc import ABCMeta, abstractmethod
-from typing import Union, Dict, List, Optional
-from sciwing.tokenizers.word_tokenizer import WordTokenizer
-import wasabi
-from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
+from typing import Dict, List
+from sciwing.data.line import Line
+from sciwing.data.label import Label
+from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
 
 
 class BaseSeqLabelingDataset(metaclass=ABCMeta):
-    def __init__(
-        self,
-        filename: str,
-        dataset_type: str,
-        max_num_words: int,
-        max_instance_length: int,
-        word_vocab_store_location: str,
-        debug: bool = False,
-        debug_dataset_proportion: float = 0.1,
-        word_embedding_type: Union[str, None] = None,
-        word_embedding_dimension: Union[int, None] = None,
-        word_start_token: str = "<SOS>",
-        word_end_token: str = "<EOS>",
-        word_pad_token: str = "<PAD>",
-        word_unk_token: str = "<UNK>",
-        train_size: float = 0.8,
-        test_size: float = 0.2,
-        validation_size: float = 0.5,
-        word_tokenizer=WordTokenizer(),
-        word_tokenization_type="vanilla",
-        character_tokenizer=CharacterTokenizer(),
-    ):
+    def __init__(self, filename: str, tokenizers: Dict[str, BaseTokenizer]):
         """ Base Text Classification Dataset to be inherited by all text classification datasets
 
         Parameters
@@ -36,142 +15,19 @@ class BaseSeqLabelingDataset(metaclass=ABCMeta):
             Path of file where the text classification dataset is stored. Ideally this should have
             an example text and label separated by space. But it is left to the specific dataset to
             handle the different ways in which file could be structured
-        dataset_type : str
-            One of ``[train, valid, test]``
-        max_num_words : int
-            The top ``max_num_words`` will be considered for building vocab
-        max_instance_length : int
-            Every instance in the dataset will be padded to or curtailed to ``max_length`` number of
-            tokens
-        word_vocab_store_location : str
-            Vocabulary once built will be stored in this location
-            If the vocabulary already exists then it will be loaded from the filepath
-        debug : bool
-            Useful to build a small dataset for debugging purposes. If ``True``, then a smaller
-            random version of the dataset should be returned. If ``True`` then
-            ``debug_dataset_proportion`` will be the proportion of the dataset that will be returned
-        debug_dataset_proportion : int
-            Percent of dataset that will be returned for debug purposes. Should be between 0 and 1
-        word_embedding_type : str
-            The kind of word embedding that will be associated with the words in the database
-            Any of the ``allowed_types`` in vocab.EmbeddingLoader is allowed here
-        word_embedding_dimension : int
-            Dimension of word embedding
-        word_start_token : str
-            Start token appended at the beginning of every instance
-        word_end_token : str
-            End token appended at the end of every instance
-        word_pad_token : str
-            Pad token to be used for padding
-        word_unk_token : str
-            All OOV words (if they are less frequent than ``max_words`` or word is in
-            test but not in train) will be mapped to ``unk_token``
-        train_size : str
-            Percentage of the instances to be used for training
-        test_size : str
-            Remaining percentage that will be used for testing
-        validation_size : str
-            Percentage of test data that will be used for validation
-        word_tokenizer : WordTokenizer
-            Word Tokenizer to be used for the dataset. You can reference
-            ``tokenizers.WordTokenizer`` for more information
-        word_tokenization_type : str
-            The type of word tokenization that the word tokenizer represents
-        character_tokenizer : str
-            Any of the ``tokenizer.CharacterTokenizer`` that can be used for character
-            tokenization
+        tokenizers : Dict[str, BaseTokeizer]
         """
-        self.filename = filename
-        self.dataset_type = dataset_type
-        self.max_num_words = max_num_words
-        self.max_length = max_instance_length
-        self.store_location = word_vocab_store_location
-        self.debug = debug
-        self.debug_dataset_proportion = debug_dataset_proportion
-        self.embedding_type = word_embedding_type
-        self.embedding_dimension = word_embedding_dimension
-        self.start_token = word_start_token
-        self.end_token = word_end_token
-        self.pad_token = word_pad_token
-        self.unk_token = word_unk_token
-        self.train_size = train_size
-        self.validation_size = validation_size
-        self.test_size = test_size
-        self.word_tokenizer = word_tokenizer
-        self.word_tokenization_type = word_tokenization_type
-        self.char_tokenizer = character_tokenizer
-        self.msg_printer = wasabi.Printer()
-        self.allowable_dataset_types = ["train", "valid", "test"]
-
-        self.msg_printer.divider("{0} DATASET".format(self.dataset_type.upper()))
-
-        assert self.dataset_type in self.allowable_dataset_types, (
-            "You can Pass one of these "
-            "for dataset types: {0}".format(self.allowable_dataset_types)
-        )
-
-    @classmethod
-    @abstractmethod
-    def get_classname2idx(cls) -> Dict[str, int]:
-        """ Mapping between classnames and index
-
-        Returns
-        -------
-        Dict[str, int]
-            A mapping between class names and idx
-        """
-        pass
-
-    @abstractmethod
-    def get_num_classes(self) -> int:
-        """ Return the number of classes in the dataset
-
-        In sequential labeling, the tagging scheme can be different.
-        For example in BIO tagging scheme for NER, the beginning of an
-        entity like Person can be B-PER. B-PER counts for one class
-
-        Returns
-        -------
-        int
-            Number of classes in the dataset
-        """
-        pass
-
-    @abstractmethod
-    def get_class_names_from_indices(self, indices: List[int]) -> List[str]:
-        """ Return a set of class names from indices. Utility method useful for display purposes
-
-        Parameters
-        ----------
-        indices : List[int]
-            List of indices where every index should be between [0, ``num_classes``)
-
-        Returns
-        -------
-        List[str]
-            List of class names for indices
-        """
-        pass
 
     @abstractmethod
-    def print_stats(self):
-        """ Free form printing of useful stats about the dataset
-
-        Returns
-        -------
-        None
-        """
-        pass
-
-    @abstractmethod
-    def get_lines_labels(self, filename: str) -> (List[str], List[str]):
+    def get_lines_labels(self) -> (List[Line], List[Label]):
         """ A list of lines from the file and a list of corresponding labels
 
         This method is to be implemented by a new dataset. The decision on
-        the implementation logic is left to the inheriting class. Datasets come in all
+        the implementation logic is left to the new class. Datasets come in all
         shapes and sizes.
 
-        For example return ["NUS is a national school", "B-ORG O O O"] for NER
+        Parameters
+        ---------
 
 
         Returns
@@ -181,41 +37,3 @@ class BaseSeqLabelingDataset(metaclass=ABCMeta):
 
         """
         pass
-
-    @classmethod
-    @abstractmethod
-    def emits_keys(cls) -> Dict[str, str]:
-        """ Specify the keys that will be emitted in the instance dict fo the dataset
-
-        The ``instance_dict`` is a dictionary of string to tensors. The ``instance_dict`` can
-        contain various keys depending on the dataset that is being used and the model that
-        is built using the dataset. The function should provides means to inspect the different
-        keys emitted by the classification dataset and the description of what they mean
-
-        Returns
-        -------
-        Dict[str, str]
-            A dictionary representing different keys emitted and their corresponding human
-            readable description
-        """
-        pass
-
-    def get_iter_dict(self, line: str, label: Optional[List[str]] = None):
-        """ Given a line and an optional label, this method returns
-        the ``iter_dict``. These methods are especially useful during
-        inference when the line is available, and the dataset emits
-        the corresponding ``iter_dict``
-
-        Parameters
-        ----------
-        line : str
-            A string representing the sequence
-        label : str
-            The corresponding set of labels associated with the string
-
-        Returns
-        -------
-        Dict[str, Any]
-            ``iter_dict`` of the dataset
-
-        """
diff --git a/sciwing/datasets/seq_labeling/parscit_dataset.py b/sciwing/datasets/seq_labeling/parscit_dataset.py
index 04e94c9..a10fdb0 100644
--- a/sciwing/datasets/seq_labeling/parscit_dataset.py
+++ b/sciwing/datasets/seq_labeling/parscit_dataset.py
@@ -5,7 +5,7 @@ from sciwing.datasets.seq_labeling.base_seq_labeling import BaseSeqLabelingDatas
 from sciwing.utils.vis_seq_tags import VisTagging
 import wasabi
 from sciwing.vocab.vocab import Vocab
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 from sciwing.utils.common import pack_to_length
 import numpy as np
 import torch
diff --git a/sciwing/datasets/seq_labeling/science_ie_dataset.py b/sciwing/datasets/seq_labeling/science_ie_dataset.py
index f96163e..461f2d3 100644
--- a/sciwing/datasets/seq_labeling/science_ie_dataset.py
+++ b/sciwing/datasets/seq_labeling/science_ie_dataset.py
@@ -4,7 +4,7 @@ from sciwing.datasets.seq_labeling.base_seq_labeling import BaseSeqLabelingDatas
 from sciwing.tokenizers.word_tokenizer import WordTokenizer
 from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
 from sciwing.vocab.vocab import Vocab
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 from sciwing.utils.vis_seq_tags import VisTagging
 from sciwing.utils.common import pack_to_length
 from typing import Union
diff --git a/sciwing/datasets/seq_labeling/seq_labelling_dataset.py b/sciwing/datasets/seq_labeling/seq_labelling_dataset.py
new file mode 100644
index 0000000..acf8d0b
--- /dev/null
+++ b/sciwing/datasets/seq_labeling/seq_labelling_dataset.py
@@ -0,0 +1,115 @@
+from sciwing.datasets.seq_labeling.base_seq_labeling import BaseSeqLabelingDataset
+from torch.utils.data import Dataset
+from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
+from sciwing.numericalizers.base_numericalizer import BaseNumericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
+from typing import Dict, List, Any
+from sciwing.data.line import Line
+from sciwing.data.seq_label import SeqLabel
+from sciwing.data.datasets_manager import DatasetsManager
+
+
+class SeqLabellingDataset(BaseSeqLabelingDataset, Dataset):
+    """ This represents a dataset that is of the form
+        word1###label1 word2###label2 word3###label3
+        word1###label1 word2###label2 word3###label3
+        word1###label1 word2###label2 word3###label3
+        .
+        .
+        .
+    """
+
+    def __init__(self, filename: str, tokenizers: Dict[str, BaseTokenizer]):
+        super().__init__(filename, tokenizers)
+        self.filename = filename
+        self.tokenizers = tokenizers
+        self.lines, self.labels = self.get_lines_labels()
+
+    def get_lines_labels(self) -> (List[Line], List[SeqLabel]):
+        lines: List[Line] = []
+        labels: List[SeqLabel] = []
+
+        with open(self.filename, "r") as fp:
+            for line in fp:
+                lines_and_labels = line.split(" ")
+                words: List[str] = []
+                word_labels: List[str] = []
+                for word_line_labels in lines_and_labels:
+                    word, word_label = word_line_labels.split("###")
+                    word = word.strip()
+                    word_label = word_label.strip()
+                    words.append(word)
+                    word_labels.append(word_label)
+
+                line = Line(text=" ".join(words), tokenizers=self.tokenizers)
+                label = SeqLabel(labels=word_labels)
+                lines.append(line)
+                labels.append(label)
+
+        return lines, labels
+
+    def __len__(self):
+        return len(self.lines)
+
+    def __getitem__(self, idx) -> (Line, SeqLabel):
+        line, label = self.lines[idx], self.labels[idx]
+        return line, label
+
+
+class SeqLabellingDatasetManager(DatasetsManager):
+    def __init__(
+        self,
+        train_filename: str,
+        dev_filename: str,
+        test_filename: str,
+        tokenizers: Dict[str, BaseTokenizer] = None,
+        namespace_vocab_options: Dict[str, Dict[str, Any]] = None,
+        namespace_numericalizer_map: Dict[str, BaseNumericalizer] = None,
+        batch_size: int = 10,
+    ):
+
+        self.train_filename = train_filename
+        self.dev_filename = dev_filename
+        self.test_filename = test_filename
+        self.tokenizers = tokenizers or {
+            "tokens": WordTokenizer(),
+            "char_tokens": CharacterTokenizer(),
+        }
+        self.namespace_vocab_options = namespace_vocab_options or {
+            "char_tokens": {
+                "start_token": " ",
+                "end_token": " ",
+                "pad_token": " ",
+                "unk_token": " ",
+            }
+        }
+        self.namespace_numericalizer_map = namespace_numericalizer_map or {
+            "tokens": Numericalizer(),
+            "char_tokens": Numericalizer(),
+        }
+        self.namespace_numericalizer_map["seq_label"] = Numericalizer()
+
+        self.batch_size = batch_size
+
+        self.train_dataset = SeqLabellingDataset(
+            filename=self.train_filename, tokenizers=self.tokenizers
+        )
+
+        self.dev_dataset = SeqLabellingDataset(
+            filename=self.dev_filename, tokenizers=self.tokenizers
+        )
+
+        self.test_dataset = SeqLabellingDataset(
+            filename=self.test_filename, tokenizers=self.tokenizers
+        )
+
+        super(SeqLabellingDatasetManager, self).__init__(
+            train_dataset=self.train_dataset,
+            dev_dataset=self.dev_dataset,
+            test_dataset=self.test_dataset,
+            namespace_vocab_options=self.namespace_vocab_options,
+            namespace_numericalizer_map=self.namespace_numericalizer_map,
+            batch_size=batch_size,
+        )
diff --git a/sciwing/datasets/sprinkle_dataset.py b/sciwing/datasets/sprinkle_dataset.py
index c5371e3..bebcc3d 100644
--- a/sciwing/datasets/sprinkle_dataset.py
+++ b/sciwing/datasets/sprinkle_dataset.py
@@ -4,7 +4,7 @@ import inspect
 from sciwing.datasets.classification.base_text_classification import (
     BaseTextClassification,
 )
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 from sciwing.vocab.vocab import Vocab
 from sciwing.tokenizers.word_tokenizer import WordTokenizer
 from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
diff --git a/sciwing/engine/engine.py b/sciwing/engine/engine.py
index bc76eb9..89cf757 100644
--- a/sciwing/engine/engine.py
+++ b/sciwing/engine/engine.py
@@ -3,17 +3,16 @@ from torch.utils.data import DataLoader
 import torch.nn as nn
 import torch.optim as optim
 from wasabi import Printer
-from typing import Iterator, Callable, Any, List, Optional, Dict
+from typing import Iterator, Any, Optional, Dict
 from sciwing.meters.loss_meter import LossMeter
+from sciwing.data.datasets_manager import DatasetsManager
 from tensorboardX import SummaryWriter
 from sciwing.metrics.BaseMetric import BaseMetric
 import numpy as np
 import time
 import logging
-from torch.utils.data._utils.collate import default_collate
 import torch
-from sciwing.utils.tensor_utils import move_to_device
-from copy import deepcopy
+from torch.utils.data.sampler import SubsetRandomSampler
 from sciwing.utils.class_nursery import ClassNursery
 import logzero
 import hashlib
@@ -29,25 +28,26 @@ class Engine(ClassNursery):
     def __init__(
         self,
         model: nn.Module,
-        train_dataset: Dataset,
-        validation_dataset: Dataset,
-        test_dataset: Dataset,
+        datasets_manager: DatasetsManager,
         optimizer: optim,
         batch_size: int,
         save_dir: str,
         num_epochs: int,
         save_every: int,
         log_train_metrics_every: int,
-        metric: BaseMetric,
+        train_metric: BaseMetric,
+        validation_metric: BaseMetric,
+        test_metric: BaseMetric,
         experiment_name: Optional[str] = None,
         experiment_hyperparams: Optional[Dict[str, Any]] = None,
         tensorboard_logdir: str = None,
         track_for_best: str = "loss",
-        collate_fn: Callable[[List[Any]], List[Any]] = default_collate,
+        collate_fn=list,
         device=torch.device("cpu"),
         gradient_norm_clip_value: Optional[float] = 5.0,
         lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
         use_wandb: bool = False,
+        sample_proportion: float = 1.0,
     ):
         """ Engine runs the models end to end. It iterates through the train dataset and passes
         it through the model. During training it helps in tracking a lot of parameters for the run
@@ -58,18 +58,8 @@ class Engine(ClassNursery):
         ----------
         model : nn.Module
             A pytorch module defining a model to be run
-        train_dataset : Dataset
-            This represents the training dataset.
-            Anything that confirms to the Dataset protocol. This can be any class that implements
-            the ``__get_item__`` and ``__len__`` as required by pytorch dataset
-        validation_dataset : Dataset
-            This represents the validation dataset.
-            Anything that confirms to the Dataset protocol. This can be any class that implements
-            the ``__get_item__`` and ``__len__`` as required by pytorch dataset
-        test_dataset : Dataset
-            This represents the test dataset
-            Anything that confirms to the Dataset protocol. This can be any class that implements
-            the ``__get_item__`` and ``__len__`` as required by pytorch dataset
+        datasets_manager : DatasetsManager
+            A datasets manager that handles all the different datasets
         optimizer : torch.optim
             Any Optimizer object instantiated using  ``torch.optim``
         batch_size : int
@@ -85,8 +75,12 @@ class Engine(ClassNursery):
         log_train_metrics_every : int
             The train metrics will be reported every ``log_train_metrics_every`` iterations
             during training
-        metric : BaseMetric
-            Anything that is an instance of ``BaseMetric``
+        train_metric : BaseMetric
+            Anything that is an instance of ``BaseMetric`` for calculating training metrics
+        validation_metric : BaseMetric
+            Anything that is an instance of ``BaseMetric`` for calculating validation metrics
+        test_metric : BaseMetric
+            Anything that is an instance of ``BaseMetric`` for calculating test metrics
         experiment_name : str
             The experiment should be given a name for ease of tracking. Instead experiment
             name is not given, we generate a unique 10 digit sha for the experiment.
@@ -126,9 +120,10 @@ class Engine(ClassNursery):
             device = torch.device(device)
 
         self.model = model
-        self.train_dataset = train_dataset
-        self.validation_dataset = validation_dataset
-        self.test_dataset = test_dataset
+        self.datasets_manager = datasets_manager
+        self.train_dataset = self.datasets_manager.train_dataset
+        self.validation_dataset = self.datasets_manager.dev_dataset
+        self.test_dataset = self.datasets_manager.test_dataset
         self.optimizer = optimizer
         self.batch_size = batch_size
         self.save_dir = pathlib.Path(save_dir)
@@ -137,7 +132,9 @@ class Engine(ClassNursery):
         self.save_every = save_every
         self.log_train_metrics_every = log_train_metrics_every
         self.tensorboard_logdir = tensorboard_logdir
-        self.metric = metric
+        self.train_metric_calc = train_metric
+        self.validation_metric_calc = validation_metric
+        self.test_metric_calc = test_metric
         self.summaryWriter = SummaryWriter(log_dir=tensorboard_logdir)
         self.track_for_best = track_for_best
         self.collate_fn = collate_fn
@@ -150,6 +147,8 @@ class Engine(ClassNursery):
             self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau
         )
         self.use_wandb = wandb and use_wandb
+        self.sample_proportion = sample_proportion
+        self.datasets_manager.print_stats()
 
         if experiment_name is None:
             hash_ = hashlib.sha1()
@@ -170,7 +169,7 @@ class Engine(ClassNursery):
         if not self.save_dir.is_dir():
             self.save_dir.mkdir(parents=True)
 
-        self.num_workers = 0
+        self.num_workers = 1
         self.model.to(self.device)
 
         self.train_loader = self.get_loader(self.train_dataset)
@@ -186,19 +185,7 @@ class Engine(ClassNursery):
         self.train_loss_meter = LossMeter()
         self.validation_loss_meter = LossMeter()
 
-        # get metric calculators
-        self.train_metric_calc = deepcopy(metric)
-        self.validation_metric_calc = deepcopy(metric)
-        self.test_metric_calc = deepcopy(metric)
-
         self.msg_printer.divider("ENGINE STARTING")
-        self.msg_printer.info(f"Number of training examples {len(self.train_dataset)}")
-        self.msg_printer.info(
-            f"Number of validation examples {len(self.validation_dataset)}"
-        )
-        self.msg_printer.info(
-            f"Number of test examples {0}".format(len(self.test_dataset))
-        )
         time.sleep(3)
 
         # get the loggers ready
@@ -251,12 +238,17 @@ class Engine(ClassNursery):
             A pytorch DataLoader
 
         """
+        dataset_size = len(dataset)
+        sample_size = int(np.floor(dataset_size * self.sample_proportion))
+        indices = np.random.choice(range(dataset_size), size=sample_size, replace=False)
+        sampler = SubsetRandomSampler(indices=indices)
         loader = DataLoader(
             dataset=dataset,
             batch_size=self.batch_size,
             num_workers=self.num_workers,
             collate_fn=self.collate_fn,
             pin_memory=True,
+            sampler=sampler,
         )
         return loader
 
@@ -340,16 +332,21 @@ class Engine(ClassNursery):
         while True:
             try:
                 # N*T, N * 1, N * 1
-                iter_dict = next(train_iter)
-                iter_dict = move_to_device(obj=iter_dict, cuda_device=self.device)
-                labels = iter_dict["label"]
-                batch_size = labels.size()[0]
+                lines_labels = next(train_iter)
+                lines_labels = list(zip(*lines_labels))
+                lines = lines_labels[0]
+                labels = lines_labels[1]
+                batch_size = len(lines)
 
                 model_forward_out = self.model(
-                    iter_dict, is_training=True, is_validation=False, is_test=False
+                    lines=lines,
+                    labels=labels,
+                    is_training=True,
+                    is_validation=False,
+                    is_test=False,
                 )
                 self.train_metric_calc.calc_metric(
-                    iter_dict=iter_dict, model_forward_dict=model_forward_out
+                    lines=lines, labels=labels, model_forward_dict=model_forward_out
                 )
 
                 try:
@@ -439,19 +436,24 @@ class Engine(ClassNursery):
 
         while True:
             try:
-                iter_dict = next(valid_iter)
-                iter_dict = move_to_device(obj=iter_dict, cuda_device=self.device)
-                labels = iter_dict["label"]
-                batch_size = labels.size(0)
+                lines_labels = next(valid_iter)
+                lines_labels = list(zip(*lines_labels))
+                lines = lines_labels[0]
+                labels = lines_labels[1]
+                batch_size = len(lines)
 
                 with torch.no_grad():
                     model_forward_out = self.model(
-                        iter_dict, is_training=False, is_validation=True, is_test=False
+                        lines=lines,
+                        labels=labels,
+                        is_training=False,
+                        is_validation=True,
+                        is_test=False,
                     )
                 loss = model_forward_out["loss"]
                 self.validation_loss_meter.add_loss(loss, batch_size)
                 self.validation_metric_calc.calc_metric(
-                    iter_dict=iter_dict, model_forward_dict=model_forward_out
+                    lines=lines, labels=labels, model_forward_dict=model_forward_out
                 )
             except StopIteration:
                 self.validation_epoch_end(epoch_num)
@@ -540,15 +542,21 @@ class Engine(ClassNursery):
         test_iter = iter(self.test_loader)
         while True:
             try:
-                iter_dict = next(test_iter)
-                iter_dict = move_to_device(obj=iter_dict, cuda_device=self.device)
+                lines_labels = next(test_iter)
+                lines_labels = list(zip(*lines_labels))
+                lines = lines_labels[0]
+                labels = lines_labels[1]
 
                 with torch.no_grad():
                     model_forward_out = self.model(
-                        iter_dict, is_training=False, is_validation=False, is_test=True
+                        lines=lines,
+                        labels=labels,
+                        is_training=False,
+                        is_validation=False,
+                        is_test=True,
                     )
                 self.test_metric_calc.calc_metric(
-                    iter_dict=iter_dict, model_forward_dict=model_forward_out
+                    lines=lines, labels=labels, model_forward_dict=model_forward_out
                 )
             except StopIteration:
                 self.test_epoch_end(epoch_num)
diff --git a/sciwing/infer/bi_lstm_lc_infer_gensect.py b/sciwing/infer/bi_lstm_lc_infer_gensect.py
index efd3f7e..09cea0e 100644
--- a/sciwing/infer/bi_lstm_lc_infer_gensect.py
+++ b/sciwing/infer/bi_lstm_lc_infer_gensect.py
@@ -2,7 +2,7 @@ import os
 import sciwing.constants as constants
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.modules.embedders import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.infer.classification.classification_inference import (
     ClassificationInference,
 )
@@ -42,7 +42,7 @@ def get_bilstm_lc_infer_gensect(dirname: str):
     classifier_encoding_dim = 2 * HIDDEN_DIM if BIDIRECTIONAL else HIDDEN_DIM
 
     embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIM, embedding=embedding)
+    embedder = WordEmbedder(embedding_dim=EMBEDDING_DIM, embedding=embedding)
 
     encoder = LSTM2VecEncoder(
         emb_dim=EMBEDDING_DIM,
diff --git a/sciwing/infer/bi_lstm_lc_infer_parsect.py b/sciwing/infer/bi_lstm_lc_infer_parsect.py
deleted file mode 100644
index e4f3875..0000000
--- a/sciwing/infer/bi_lstm_lc_infer_parsect.py
+++ /dev/null
@@ -1,73 +0,0 @@
-import os
-import sciwing.constants as constants
-from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
-from sciwing.modules.embedders import VanillaEmbedder
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-import json
-import torch.nn as nn
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-def get_bilstm_lc_infer_parsect(dirname: str):
-
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIM = config["EMBEDDING_DIMENSION"]
-    HIDDEN_DIM = config["HIDDEN_DIMENSION"]
-    COMBINE_STRATEGY = config["COMBINE_STRATEGY"]
-    BIDIRECTIONAL = config["BIDIRECTIONAL"]
-    VOCAB_SIZE = config["VOCAB_SIZE"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    classifier_encoding_dim = 2 * HIDDEN_DIM if BIDIRECTIONAL else HIDDEN_DIM
-
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIM, embedding=embedding)
-
-    encoder = LSTM2VecEncoder(
-        emb_dim=EMBEDDING_DIM,
-        embedder=embedder,
-        hidden_dim=HIDDEN_DIM,
-        combine_strategy=COMBINE_STRATEGY,
-        bidirectional=BIDIRECTIONAL,
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=classifier_encoding_dim,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = SectLabelDataset(**test_dataset_args)
-
-    inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return inference
-
-
-if __name__ == "__main__":
-    experiment_dirname = os.path.join(OUTPUT_DIR, "debug_bi_lstm_lc")
-    inference_client = get_bilstm_lc_infer_parsect(experiment_dirname)
diff --git a/sciwing/infer/bilstm_crf_infer.py b/sciwing/infer/bilstm_crf_infer.py
index 98fb144..47618ab 100644
--- a/sciwing/infer/bilstm_crf_infer.py
+++ b/sciwing/infer/bilstm_crf_infer.py
@@ -4,7 +4,7 @@ import json
 from sciwing.datasets.seq_labeling.parscit_dataset import ParscitDataset
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.models.parscit_tagger import ParscitTagger
 from sciwing.infer.seq_label_inference.parscit_inference import ParscitInference
@@ -67,13 +67,13 @@ def get_bilstm_crf_infer(dirname: str):
 
     embedding = test_dataset.word_vocab.load_embedding()
     embedding = nn.Embedding.from_pretrained(embedding)
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
+    embedder = WordEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
 
     char_embedding = test_dataset.char_vocab.load_embedding()
     char_embedding = nn.Embedding.from_pretrained(char_embedding)
 
     if USE_CHAR_ENCODER:
-        char_embedder = VanillaEmbedder(
+        char_embedder = WordEmbedder(
             embedding=char_embedding, embedding_dim=CHAR_EMBEDDING_DIMENSION
         )
         char_encoder = CharLSTMEncoder(
diff --git a/sciwing/infer/bow_bert_emb_lc_parsect_infer.py b/sciwing/infer/bow_bert_emb_lc_parsect_infer.py
deleted file mode 100644
index f26c5a1..0000000
--- a/sciwing/infer/bow_bert_emb_lc_parsect_infer.py
+++ /dev/null
@@ -1,71 +0,0 @@
-import json
-import os
-import sciwing.constants as constants
-from sciwing.modules.embedders.bert_embedder import BertEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-import torch
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-def get_bow_bert_emb_lc_parsect_infer(dirname: str):
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIM = config["EMBEDDING_DIMENSION"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-    BERT_TYPE = config["BERT_TYPE"]
-
-    DEVICE = config["DEVICE"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    embedder = BertEmbedder(
-        emb_dim=EMBEDDING_DIM,
-        dropout_value=0.0,
-        aggregation_type="average",
-        bert_type=BERT_TYPE,
-        device=torch.device(DEVICE),
-    )
-
-    encoder = BOW_Encoder(
-        embedder=embedder, emb_dim=EMBEDDING_DIM, aggregation_type="average"
-    )
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIM,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = SectLabelDataset(**test_dataset_args)
-
-    parsect_inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return parsect_inference
-
-
-if __name__ == "__main__":
-    experiment_dirname = os.path.join(
-        OUTPUT_DIR, "debug_bow_bert_base_cased_emb_lc_10e_1e-2lr"
-    )
-    inference_client = get_bow_bert_emb_lc_parsect_infer(experiment_dirname)
diff --git a/sciwing/infer/bow_elmo_emb_lc_parsect_infer.py b/sciwing/infer/bow_elmo_emb_lc_parsect_infer.py
deleted file mode 100644
index 00fb4a4..0000000
--- a/sciwing/infer/bow_elmo_emb_lc_parsect_infer.py
+++ /dev/null
@@ -1,63 +0,0 @@
-import json
-import os
-import sciwing.constants as constants
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-import pathlib
-
-PATHS = constants.PATHS
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-FILES = constants.FILES
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-def get_elmo_emb_lc_infer_parsect(dirname: str):
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIM = config["EMBEDDING_DIMENSION"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-    LAYER_AGGREGATION = config.get("LAYER_AGGREGATION")
-    WORD_AGGREGATION = config.get("WORD_AGGREGATION")
-
-    embedder = BowElmoEmbedder(
-        emb_dim=EMBEDDING_DIM, layer_aggregation=LAYER_AGGREGATION
-    )
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIM, aggregation_type=WORD_AGGREGATION, embedder=embedder
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIM,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    dataset = SectLabelDataset(**test_dataset_args)
-
-    parsect_inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return parsect_inference
-
-
-if __name__ == "__main__":
-    experiment_dirname = os.path.join(OUTPUT_DIR, "debug_bow_elmo_emb_lc_parsect")
-    inference_client = get_elmo_emb_lc_infer_parsect(experiment_dirname)
diff --git a/sciwing/infer/bow_lc_gensect_infer.py b/sciwing/infer/bow_lc_gensect_infer.py
deleted file mode 100644
index 9ccd8f5..0000000
--- a/sciwing/infer/bow_lc_gensect_infer.py
+++ /dev/null
@@ -1,61 +0,0 @@
-import json
-import os
-import sciwing.constants as constants
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
-import torch.nn as nn
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-GENERIC_SECTION_TRAIN_FILE = FILES["GENERIC_SECTION_TRAIN_FILE"]
-
-
-def get_bow_lc_gensect_infer(dirname: str):
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-    VOCAB_SIZE = config["VOCAB_SIZE"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        dropout_value=0.0,
-        aggregation_type="sum",
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = GenericSectDataset(**test_dataset_args)
-
-    parsect_inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return parsect_inference
diff --git a/sciwing/infer/bow_lc_parsect_infer.py b/sciwing/infer/bow_lc_parsect_infer.py
deleted file mode 100644
index 9177459..0000000
--- a/sciwing/infer/bow_lc_parsect_infer.py
+++ /dev/null
@@ -1,63 +0,0 @@
-import json
-import os
-import sciwing.constants as constants
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-import torch.nn as nn
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-def get_bow_lc_parsect_infer(dirname: str):
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    EMBEDDING_DIMENSION = config["EMBEDDING_DIMENSION"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-    VOCAB_SIZE = config["VOCAB_SIZE"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)
-    embedder = VanillaEmbedder(embedding_dim=EMBEDDING_DIMENSION, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMBEDDING_DIMENSION,
-        embedder=embedder,
-        dropout_value=0.0,
-        aggregation_type="sum",
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMBEDDING_DIMENSION,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = SectLabelDataset(**test_dataset_args)
-
-    dataset.print_stats()
-
-    parsect_inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-
-    return parsect_inference
diff --git a/sciwing/infer/classification/BaseClassificationInference.py b/sciwing/infer/classification/BaseClassificationInference.py
index 783326f..dfebc95 100644
--- a/sciwing/infer/classification/BaseClassificationInference.py
+++ b/sciwing/infer/classification/BaseClassificationInference.py
@@ -1,6 +1,8 @@
 from abc import ABCMeta, abstractmethod
 import torch.nn as nn
-import json
+from sciwing.data.datasets_manager import DatasetsManager
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 import torch
 import wasabi
 from typing import Dict, Any, Optional, Union, List
@@ -16,7 +18,7 @@ class BaseClassificationInference(metaclass=ABCMeta):
         self,
         model: nn.Module,
         model_filepath: str,
-        dataset,
+        datasets_manager: DatasetsManager,
         device: Optional[Union[str, torch.device]] = torch.device("cpu"),
     ):
         """
@@ -28,14 +30,14 @@ class BaseClassificationInference(metaclass=ABCMeta):
         model_filepath : str
             The path where the parameters for the best models are stored. This is usually
             the ``best_model.pt`` while in an experiment directory
-        dataset : Dataset
+        datasets_manager : DatasetsManager
             Any dataset that conforms to the pytorch Dataset specification
         device : Optional[Union[str, torch.device]]
             This is either a string like ``cpu``, ``cuda:0`` or a torch.device object
         """
         self.model = model
         self.model_filepath = model_filepath
-        self.dataset = dataset
+        self.datasets_manager = datasets_manager
 
         self.device = torch.device(device) if isinstance(device, str) else device
         self.msg_printer = wasabi.Printer()
@@ -70,35 +72,15 @@ class BaseClassificationInference(metaclass=ABCMeta):
         pass
 
     @abstractmethod
-    def model_forward_on_iter_dict(self, iter_dict: Dict[str, Any]):
+    def model_forward_on_lines(self, lines: List[Line]):
         """ Perform the model forward pass  given an ``iter_dict``
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            ``iter_dict`` returned by a dataset
-
+        lines : List[Line]
         """
         pass
 
-    @abstractmethod
-    def metric_calc_on_iter_dict(
-        self, iter_dict: Dict[str, Any], model_output_dict: Dict[str, Any]
-    ):
-        """ Calculate the metric given an ``iter_dict`` and an ``model_output_dict``
-        that is obtained by a forward pass of the model
-
-        Parameters
-        ----------
-        iter_dict : Dict[str, Any]
-            ``iter_dict`` returned by a dataset
-
-        model_output_dict : Dict[str, Any]
-            ``model_output_dict`` : output dict that is returned by
-            forwarding the ``iter_dict`` through the model
-
-        """
-
     @abstractmethod
     def model_output_dict_to_prediction_indices_names(
         self, model_output_dict: Dict[str, Any]
@@ -119,31 +101,14 @@ class BaseClassificationInference(metaclass=ABCMeta):
        """
 
     @abstractmethod
-    def iter_dict_to_sentences(self, iter_dict: Dict[str, Any]) -> List[str]:
-        """ Returns human readable sentences given an ``iter_dict``
-
-        Parameters
-        ----------
-        iter_dict : Dict[str, Any]
-            ``iter_dict`` returned by a dataset
-
-        Returns
-        -------
-        List[str]
-            A list of human readable sentences
-
-        """
-
-    @abstractmethod
-    def iter_dict_to_true_indices_names(
-        self, iter_dict: Dict[str, Any]
+    def get_true_label_indices_names(
+        self, labels: List[Label]
     ) -> (List[int], List[str]):
-        """ Given an ``iter_dict``, it returns the indices of the true classes
-        and the corresponding classnames
+        """ Given an list of labels, it returns the indices and the names of the label
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
+        labels : Dict[str, Any]
             ``iter_dict`` returned by a dataset
 
         Returns
diff --git a/sciwing/infer/classification/classification_inference.py b/sciwing/infer/classification/classification_inference.py
index b02e066..53e3811 100644
--- a/sciwing/infer/classification/classification_inference.py
+++ b/sciwing/infer/classification/classification_inference.py
@@ -3,20 +3,18 @@ from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
 from sciwing.infer.classification.BaseClassificationInference import (
     BaseClassificationInference,
 )
-from sciwing.datasets.classification.base_text_classification import (
-    BaseTextClassification,
-)
+from sciwing.data.datasets_manager import DatasetsManager
 from deprecated import deprecated
 from torch.utils.data import DataLoader
 import torch
 import torch.nn as nn
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Dict, List
 import pandas as pd
-from sciwing.utils.tensor_utils import move_to_device
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 from wasabi.util import MESSAGES
 
 FILES = constants.FILES
-
 SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
 
 
@@ -39,21 +37,36 @@ class ClassificationInference(BaseClassificationInference):
     """
 
     def __init__(
-        self, model: nn.Module, model_filepath: str, dataset: BaseTextClassification
+        self,
+        model: nn.Module,
+        model_filepath: str,
+        datasets_manager: DatasetsManager,
+        tokens_namespace: str = "tokens",
+        labels_namespace: str = "label",
+        normalized_probs_namespace: str = "normalized_probs",
     ):
 
         super(ClassificationInference, self).__init__(
-            model=model, model_filepath=model_filepath, dataset=dataset
+            model=model,
+            model_filepath=model_filepath,
+            datasets_manager=datasets_manager,
         )
         self.batch_size = 32
+        self.tokens_namespace = tokens_namespace
+        self.normalized_probs_namespace = normalized_probs_namespace
+        self.label_namespace = labels_namespace
+
+        self.labelname2idx_mapping = self.datasets_manager.get_label_idx_mapping(
+            label_namespace=self.label_namespace
+        )
+        self.idx2labelname_mapping = self.datasets_manager.get_idx_label_mapping(
+            label_namespace=self.label_namespace
+        )
 
-        self.labelname2idx_mapping = self.dataset.get_classname2idx()
-        self.idx2labelname_mapping = {
-            idx: label_name for label_name, idx in self.labelname2idx_mapping.items()
-        }
         self.load_model()
+
         self.metrics_calculator = PrecisionRecallFMeasure(
-            idx2labelname_mapping=self.idx2labelname_mapping
+            datasets_manager=datasets_manager
         )
         self.output_analytics = None
 
@@ -61,61 +74,73 @@ class ClassificationInference(BaseClassificationInference):
         self.output_df = None
 
     def run_inference(self) -> Dict[str, Any]:
-        loader = DataLoader(
-            dataset=self.dataset, batch_size=self.batch_size, shuffle=False
-        )
-        output_analytics = {}
-
-        # contains the predicted class names for all the instances
-        pred_class_names = []
-        true_class_names = []  # contains the true class names for all the instances
-        sentences = []  # batch sentences in english
-        true_labels_indices = []
-        predicted_labels_indices = []
-        all_pred_probs = []
-        self.metrics_calculator.reset()
-
-        for iter_dict in loader:
-            iter_dict = move_to_device(obj=iter_dict, cuda_device=self.device)
-            batch_sentences = self.iter_dict_to_sentences(iter_dict)
-            model_output_dict = self.model_forward_on_iter_dict(iter_dict)
-            normalized_probs = model_output_dict["normalized_probs"]
-            self.metrics_calculator.calc_metric(
-                iter_dict=iter_dict, model_forward_dict=model_output_dict
-            )
-            true_label_ind, true_label_names = self.iter_dict_to_true_indices_names(
-                iter_dict=iter_dict
-            )
-            pred_label_indices, pred_label_names = self.model_output_dict_to_prediction_indices_names(
-                model_output_dict=model_output_dict
-            )
 
-            true_label_ind = torch.LongTensor(true_label_ind)
-            true_labels_indices.append(true_label_ind)
-            true_class_names.extend(true_label_names)
-            predicted_labels_indices.extend(pred_label_indices)
-            pred_class_names.extend(pred_label_names)
-            sentences.extend(batch_sentences)
-            all_pred_probs.append(normalized_probs)
-
-        # contains predicted probs for all the instances
-        all_pred_probs = torch.cat(all_pred_probs, dim=0)
-        true_labels_indices = torch.cat(true_labels_indices, dim=0).squeeze()
-
-        # torch.LongTensor N, 1
-        output_analytics["true_labels_indices"] = true_labels_indices
-        output_analytics["predicted_labels_indices"] = predicted_labels_indices
-        output_analytics["pred_class_names"] = pred_class_names
-        output_analytics["true_class_names"] = true_class_names
-        output_analytics["sentences"] = sentences
-        output_analytics["all_pred_probs"] = all_pred_probs
+        with self.msg_printer.loading(text="Running inference on test data"):
+            loader = DataLoader(
+                dataset=self.datasets_manager.test_dataset,
+                batch_size=self.batch_size,
+                shuffle=False,
+                collate_fn=list,
+            )
+            output_analytics = {}
+
+            # contains the predicted class names for all the instances
+            pred_class_names = []
+            true_class_names = []  # contains the true class names for all the instances
+            sentences = []  # batch sentences in english
+            true_labels_indices = []
+            predicted_labels_indices = []
+            all_pred_probs = []
+            self.metrics_calculator.reset()
+
+            for lines_labels in loader:
+                lines_labels = list(zip(*lines_labels))
+                lines = lines_labels[0]
+                labels = lines_labels[1]
+
+                batch_sentences = [line.text for line in lines]
+                model_output_dict = self.model_forward_on_lines(lines=lines)
+                normalized_probs = model_output_dict[self.normalized_probs_namespace]
+                self.metrics_calculator.calc_metric(
+                    lines=lines, labels=labels, model_forward_dict=model_output_dict
+                )
+                true_label_ind, true_label_names = self.get_true_label_indices_names(
+                    labels=labels
+                )
+                (
+                    pred_label_indices,
+                    pred_label_names,
+                ) = self.model_output_dict_to_prediction_indices_names(
+                    model_output_dict=model_output_dict
+                )
 
+                true_label_ind = torch.LongTensor(true_label_ind)
+                true_labels_indices.append(true_label_ind)
+                true_class_names.extend(true_label_names)
+                predicted_labels_indices.extend(pred_label_indices)
+                pred_class_names.extend(pred_label_names)
+                sentences.extend(batch_sentences)
+                all_pred_probs.append(normalized_probs)
+
+            # contains predicted probs for all the instances
+            all_pred_probs = torch.cat(all_pred_probs, dim=0)
+            true_labels_indices = torch.cat(true_labels_indices, dim=0).squeeze()
+
+            # torch.LongTensor N, 1
+            output_analytics["true_labels_indices"] = true_labels_indices
+            output_analytics["predicted_labels_indices"] = predicted_labels_indices
+            output_analytics["pred_class_names"] = pred_class_names
+            output_analytics["true_class_names"] = true_class_names
+            output_analytics["sentences"] = sentences
+            output_analytics["all_pred_probs"] = all_pred_probs
+
+        self.msg_printer.good(title="Finished running inference")
         return output_analytics
 
-    def model_forward_on_iter_dict(self, iter_dict: Dict[str, Any]):
+    def model_forward_on_lines(self, lines: List[Line]):
         with torch.no_grad():
             model_output_dict = self.model(
-                iter_dict, is_training=False, is_validation=False, is_test=True
+                lines=lines, is_training=False, is_validation=False, is_test=True
             )
         return model_output_dict
 
@@ -180,7 +205,7 @@ class ClassificationInference(BaseClassificationInference):
         prf_table = self.metrics_calculator.report_metrics()
         print(prf_table)
 
-    @deprecated(reason="This method is deprecated. It will be removed in version 0.2")
+    @deprecated(reason="This method is deprecated. It will be removed in version 0.1")
     def generate_report_for_paper(self):
         """ Generates just the fscore to be used in reporting on print
 
@@ -194,13 +219,6 @@ class ClassificationInference(BaseClassificationInference):
         row_names.extend([f"Micro-Fscore", f"Macro-Fscore"])
         return paper_report, row_names
 
-    def metric_calc_on_iter_dict(
-        self, iter_dict: Dict[str, Any], model_output_dict: Dict[str, Any]
-    ):
-        self.metrics_calculator.calc_metric(
-            iter_dict=iter_dict, model_forward_dict=model_output_dict
-        )
-
     def model_output_dict_to_prediction_indices_names(
         self, model_output_dict: Dict[str, Any]
     ) -> (List[int], List[str]):
@@ -230,12 +248,9 @@ class ClassificationInference(BaseClassificationInference):
             Reutrns the class names for all the sentences in the input
 
         """
-        iter_dict = self.dataset.get_iter_dict(lines=lines)
-
-        if len(lines) == 1:
-            iter_dict["tokens"] = iter_dict["tokens"].unsqueeze(0)
+        lines = [self.datasets_manager.make_line(line=line) for line in lines]
 
-        model_output_dict = self.model_forward_on_iter_dict(iter_dict=iter_dict)
+        model_output_dict = self.model_forward_on_lines(lines=lines)
         _, pred_classnames = self.model_output_dict_to_prediction_indices_names(
             model_output_dict=model_output_dict
         )
@@ -258,21 +273,14 @@ class ClassificationInference(BaseClassificationInference):
         """
         return self.infer_batch(lines=[line])[0]
 
-    def iter_dict_to_sentences(self, iter_dict: Dict[str, Any]) -> List[str]:
-        tokens = iter_dict["tokens"]  # N * max_length
-        tokens_list = tokens.tolist()
-        batch_sentences = list(
-            map(self.dataset.word_vocab.get_disp_sentence_from_indices, tokens_list)
-        )
-        return batch_sentences
-
-    def iter_dict_to_true_indices_names(
-        self, iter_dict: Dict[str, Any]
+    def get_true_label_indices_names(
+        self, labels: List[Label]
     ) -> (List[int], List[str]):
-        labels = iter_dict["label"]  # N, 1
-        labels_list = labels.squeeze().tolist()
-        true_label_names = self.dataset.get_class_names_from_indices(labels_list)
-        return labels_list, true_label_names
+        label_names = [label.text for label in labels]
+        label_indices = [
+            self.labelname2idx_mapping[label_name] for label_name in label_names
+        ]
+        return label_indices, label_names
 
     def run_test(self):
         """ Runs inference and reports test metrics
diff --git a/sciwing/infer/elmo_bi_lstm_lc_infer.py b/sciwing/infer/elmo_bi_lstm_lc_infer.py
deleted file mode 100644
index a9d01d8..0000000
--- a/sciwing/infer/elmo_bi_lstm_lc_infer.py
+++ /dev/null
@@ -1,87 +0,0 @@
-import sciwing.constants as constants
-import os
-from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
-from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-from sciwing.infer.classification.classification_inference import (
-    ClassificationInference,
-)
-import json
-import torch
-import torch.nn as nn
-import pathlib
-
-PATHS = constants.PATHS
-FILES = constants.FILES
-OUTPUT_DIR = PATHS["OUTPUT_DIR"]
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-def get_elmo_bilstm_lc_infer(dirname: str):
-
-    exp_dirpath = pathlib.Path(dirname)
-    hyperparam_config_filepath = exp_dirpath.joinpath("config.json")
-    test_dataset_params = exp_dirpath.joinpath("test_dataset_params.json")
-
-    with open(hyperparam_config_filepath, "r") as fp:
-        config = json.load(fp)
-
-    with open(test_dataset_params, "r") as fp:
-        test_dataset_args = json.load(fp)
-
-    DEVICE = config["DEVICE"]
-    EMBEDDING_DIM = config["EMBEDDING_DIMENSION"]
-    VOCAB_SIZE = config["VOCAB_SIZE"]
-    HIDDEN_DIM = config["HIDDEN_DIMENSION"]
-    BIDIRECTIONAL = config["BIDIRECTIONAL"]
-    COMBINE_STRATEGY = config["COMBINE_STRATEGY"]
-    NUM_CLASSES = config["NUM_CLASSES"]
-    MODEL_SAVE_DIR = config["MODEL_SAVE_DIR"]
-
-    model_filepath = os.path.join(MODEL_SAVE_DIR, "best_model.pt")
-
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
-
-    elmo_embedder = BowElmoEmbedder(
-        layer_aggregation="sum",
-        cuda_device_id=-1 if DEVICE == "cpu" else int(DEVICE.split("cuda:")[1]),
-    )
-
-    vanilla_embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIM)
-
-    embedders = ConcatEmbedders([vanilla_embedder, elmo_embedder])
-
-    encoder = LSTM2VecEncoder(
-        emb_dim=EMBEDDING_DIM + 1024,
-        embedder=embedders,
-        hidden_dim=HIDDEN_DIM,
-        bidirectional=BIDIRECTIONAL,
-        combine_strategy=COMBINE_STRATEGY,
-        device=torch.device(DEVICE),
-    )
-
-    encoding_dim = (
-        2 * HIDDEN_DIM if BIDIRECTIONAL and COMBINE_STRATEGY == "concat" else HIDDEN_DIM
-    )
-
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=encoding_dim,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=True,
-    )
-
-    dataset = SectLabelDataset(**test_dataset_args)
-
-    inference = ClassificationInference(
-        model=model, model_filepath=model_filepath, dataset=dataset
-    )
-    return inference
-
-
-if __name__ == "__main__":
-    experiment_dirname = os.path.join(OUTPUT_DIR, "debug_parsect_elmo_bi_lstm_lc")
-    inference_client = get_elmo_bilstm_lc_infer(experiment_dirname)
diff --git a/sciwing/infer/science_ie_infer.py b/sciwing/infer/science_ie_infer.py
index a63b021..c734f00 100644
--- a/sciwing/infer/science_ie_infer.py
+++ b/sciwing/infer/science_ie_infer.py
@@ -3,7 +3,7 @@ import sciwing.constants as constants
 from sciwing.datasets.seq_labeling.science_ie_dataset import ScienceIEDataset
 from sciwing.models.science_ie_tagger import ScienceIETagger
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
 from allennlp.modules.conditional_random_field import allowed_transitions
@@ -106,10 +106,10 @@ def get_science_ie_infer(dirname: str):
         constraint_type="BIOUL", labels=material_idx2classnames
     )
 
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
+    embedder = WordEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIMENSION)
 
     if USE_CHAR_ENCODER:
-        char_embedder = VanillaEmbedder(
+        char_embedder = WordEmbedder(
             embedding=char_embedding, embedding_dim=CHAR_EMBEDDING_DIMENSION
         )
         char_encoder = CharLSTMEncoder(
diff --git a/sciwing/metrics/BaseMetric.py b/sciwing/metrics/BaseMetric.py
index b9ea0dc..8c12f88 100644
--- a/sciwing/metrics/BaseMetric.py
+++ b/sciwing/metrics/BaseMetric.py
@@ -1,5 +1,7 @@
 from abc import ABCMeta, abstractmethod
-from typing import Dict, Any, Union
+from typing import Dict, Any, List
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 
 
 class BaseMetric(metaclass=ABCMeta):
@@ -8,9 +10,9 @@ class BaseMetric(metaclass=ABCMeta):
 
     @abstractmethod
     def calc_metric(
-        self, iter_dict: Dict[str, Any], model_forward_dict: Dict[str, Any]
+        self, lines: List[Line], labels: List[Label], model_forward_dict: Dict[str, Any]
     ) -> None:
-        """ Calculates the metric using ``iter_dict`` returned by any dataset
+        """ Calculates the metric using the lines and labels returned by any dataset
         and ``model_forward_dict`` of a model. This is usually called
         for a batch of inputs and a forward pass. The state of the different
         metrics should be retained by the metric across an epoch before
@@ -19,7 +21,8 @@ class BaseMetric(metaclass=ABCMeta):
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
+        lines : List[Line]
+        labels: List[Label]
         model_forward_dict : Dict[str, Any]
         """
         pass
diff --git a/sciwing/metrics/precision_recall_fmeasure.py b/sciwing/metrics/precision_recall_fmeasure.py
index c142fa6..b7ca07a 100644
--- a/sciwing/metrics/precision_recall_fmeasure.py
+++ b/sciwing/metrics/precision_recall_fmeasure.py
@@ -2,30 +2,42 @@ import torch
 from typing import Dict, Union, Any, Optional, List
 from wasabi import Printer
 from sciwing.utils.common import merge_dictionaries_with_sum
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 import numpy as np
 import pandas as pd
 from sciwing.metrics.BaseMetric import BaseMetric
-from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.datasets_manager import DatasetsManager
 from sciwing.metrics.classification_metrics_utils import ClassificationMetricsUtils
 from sciwing.utils.class_nursery import ClassNursery
 
 
 class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
-    def __init__(self, idx2labelname_mapping: Optional[Dict[int, str]] = None):
+    def __init__(
+        self,
+        datasets_manager: DatasetsManager,
+        label_namespace="label",
+        normalized_probs_namespace="normalized_probs",
+    ):
         """
 
         Parameters
         ----------
-        idx2labelname_mapping : Dict[int, str]
-            Mapping from index to label. If this is not provided
-            then we are going to use the class indices in all the reports
+        datasets_manager : DatasetsManager
+            The dataset manager managing the labels and other information
         """
         super(PrecisionRecallFMeasure, self).__init__()
-        self.idx2labelname_mapping = idx2labelname_mapping
+        self.datasets_manager = datasets_manager
+        self.idx2labelname_mapping = None
         self.msg_printer = Printer()
         self.classification_metrics_utils = ClassificationMetricsUtils(
-            idx2labelname_mapping=idx2labelname_mapping
+            idx2labelname_mapping=self.idx2labelname_mapping
         )
+        self.label_namespace = label_namespace
+        self.normalized_probs_namespace = normalized_probs_namespace
+        self.label_numericalizer = self.datasets_manager.namespace_to_numericalizer[
+            self.label_namespace
+        ]
 
         # setup counters to calculate true positives, false positives,
         # false negatives and true negatives
@@ -82,7 +94,10 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
         # convert labels to 1 dimension
         true_labels_numpy = labels.cpu().numpy().tolist()
 
-        confusion_mtrx, classes = self.classification_metrics_utils.get_confusion_matrix_and_labels(
+        (
+            confusion_mtrx,
+            classes,
+        ) = self.classification_metrics_utils.get_confusion_matrix_and_labels(
             predicted_tag_indices=top_indices_numpy,
             true_tag_indices=true_labels_numpy,
             masked_label_indices=labels_mask,
@@ -111,7 +126,7 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
         )
 
     def calc_metric(
-        self, iter_dict: Dict[str, Any], model_forward_dict: Dict[str, Any]
+        self, lines: List[Line], labels: List[Label], model_forward_dict: Dict[str, Any]
     ) -> None:
         """ Updates the values being tracked for calculating the metric
 
@@ -121,13 +136,11 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            The ``iter_dict`` from the dataset is expected to have
-            ``label`` which are labels for instances. They are usually
-            of the size ``[batch_size]``
-            Optionally there can be a ``label_mask`` of the size ``[batch_size]``
-            The ``label_mask`` is 1 where the label should be masked otherwise
-            if the label is not masked then it is 0
+        lines : List[Line]
+           A list of lines
+        labels: List[Label]
+            A list of labels. This has to be the label used for classification
+            Refer to the documentation of Label for more information
 
         model_forward_dict : Dict[str, Any]
             The dictionary obtained after a forward pass
@@ -135,14 +148,23 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
             that usually is of the size ``[batch_size, num_classes]``
         """
 
-        normalized_probs = model_forward_dict["normalized_probs"]
-        labels = iter_dict["label"]
-        labels_mask = iter_dict.get("label_mask")
-        if labels_mask is None:
-            labels_mask = torch.zeros_like(labels).type(torch.ByteTensor)
+        normalized_probs = model_forward_dict[self.normalized_probs_namespace]
+
+        labels_tensor = []
+        for label in labels:
+            tokens = label.tokens[self.label_namespace]
+            tokens = [tok.text for tok in tokens]
+            numericalized_instance = self.label_numericalizer.numericalize_instance(
+                instance=tokens
+            )
+
+            labels_tensor.extend(numericalized_instance)
+
+        labels_tensor = torch.LongTensor(labels_tensor)
+        labels_tensor = labels_tensor.view(-1, 1)
+        labels_mask = torch.zeros_like(labels_tensor).type(torch.ByteTensor)
 
         normalized_probs = normalized_probs.cpu()
-        labels = labels.cpu()
 
         assert normalized_probs.ndimension() == 2, self.msg_printer.fail(
             "The predicted probs should "
@@ -151,10 +173,10 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
             "{0}".format(normalized_probs.size())
         )
 
-        assert labels.ndimension() == 2, self.msg_printer.fail(
+        assert labels_tensor.ndimension() == 2, self.msg_printer.fail(
             "The labels should have 2 dimension."
             "The labels that you passed have shape "
-            "{0}".format(labels.size())
+            "{0}".format(labels_tensor.size())
         )
 
         # TODO: for now k=1, change it to different number of ks
@@ -164,11 +186,14 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
         top_indices_numpy = top_indices.cpu().numpy().tolist()
 
         # convert labels to 1 dimension
-        true_labels_numpy = labels.cpu().numpy().tolist()
+        true_labels_numpy = labels_tensor.cpu().numpy().tolist()
 
         labels_mask = labels_mask.tolist()
 
-        confusion_mtrx, classes = self.classification_metrics_utils.get_confusion_matrix_and_labels(
+        (
+            confusion_mtrx,
+            classes,
+        ) = self.classification_metrics_utils.get_confusion_matrix_and_labels(
             true_tag_indices=true_labels_numpy,
             predicted_tag_indices=top_indices_numpy,
             masked_label_indices=labels_mask,
@@ -239,7 +264,11 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
                 The micro fscore value considering all different classes
 
         """
-        precision_dict, recall_dict, fscore_dict = self.classification_metrics_utils.get_prf_from_counters(
+        (
+            precision_dict,
+            recall_dict,
+            fscore_dict,
+        ) = self.classification_metrics_utils.get_prf_from_counters(
             tp_counter=self.tp_counter,
             fp_counter=self.fp_counter,
             fn_counter=self.fn_counter,
@@ -250,14 +279,22 @@ class PrecisionRecallFMeasure(BaseMetric, ClassNursery):
         # https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin
 
         # micro scores
-        micro_precision, micro_recall, micro_fscore = self.classification_metrics_utils.get_micro_prf_from_counters(
+        (
+            micro_precision,
+            micro_recall,
+            micro_fscore,
+        ) = self.classification_metrics_utils.get_micro_prf_from_counters(
             tp_counter=self.tp_counter,
             fp_counter=self.fp_counter,
             fn_counter=self.fn_counter,
         )
 
         # macro scores
-        macro_precision, macro_recall, macro_fscore = self.classification_metrics_utils.get_macro_prf_from_prf_dicts(
+        (
+            macro_precision,
+            macro_recall,
+            macro_fscore,
+        ) = self.classification_metrics_utils.get_macro_prf_from_prf_dicts(
             precision_dict=precision_dict,
             recall_dict=recall_dict,
             fscore_dict=fscore_dict,
diff --git a/sciwing/models/simpleclassifier.py b/sciwing/models/simpleclassifier.py
index e5aa80a..a19a364 100644
--- a/sciwing/models/simpleclassifier.py
+++ b/sciwing/models/simpleclassifier.py
@@ -1,9 +1,13 @@
 import torch.nn as nn
 from torch.nn.functional import softmax
 from torch.nn import CrossEntropyLoss
-from typing import Dict, Any
+from typing import List, Any, Dict
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 from wasabi import Printer
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.datasets_manager import DatasetsManager
+import torch
 
 
 class SimpleClassifier(nn.Module, ClassNursery):
@@ -12,14 +16,18 @@ class SimpleClassifier(nn.Module, ClassNursery):
         encoder: nn.Module,
         encoding_dim: int,
         num_classes: int,
-        classification_layer_bias: bool,
+        classification_layer_bias: bool = True,
+        label_namespace: str = "label",
+        datasets_manager: DatasetsManager = None,
+        device: torch.device = torch.device("cpu"),
     ):
         """ SimpleClassifier is a linear classifier head on top of any encoder
 
         Parameters
         ----------
         encoder : nn.Module
-            Any encoder that takes in instances
+            Any encoder that takes in lines and produces a single vector
+            for every line.
         encoding_dim : int
             The encoding dimension
         num_classes : int
@@ -27,6 +35,12 @@ class SimpleClassifier(nn.Module, ClassNursery):
         classification_layer_bias : bool
             Whether to add classification layer bias or no
             This is set to false only for debugging purposes ff
+        label_namespace : str
+            The namespace used for labels in the dataset
+        datasets_manager: DatasetsManager
+            The datasets manager for the model
+        device: torch.device
+            The device on which the model is run
         """
         super(SimpleClassifier, self).__init__()
         self.encoder = encoder
@@ -35,28 +49,37 @@ class SimpleClassifier(nn.Module, ClassNursery):
         print(self.num_classes)
         self.classification_layer_bias = classification_layer_bias
         self.classification_layer = nn.Linear(
-            encoding_dim, num_classes, bias=self.classification_layer_bias
+            self.encoding_dim, num_classes, bias=self.classification_layer_bias
         )
         self._loss = CrossEntropyLoss()
+        self.label_namespace = label_namespace
+        self.datasets_manager = datasets_manager
+        self.label_numericalizer = self.datasets_manager.namespace_to_numericalizer[
+            self.label_namespace
+        ]
+        self.device = device
         self.msg_printer = Printer()
 
     def forward(
         self,
-        iter_dict: Dict[str, Any],
-        is_training: bool,
-        is_validation: bool,
-        is_test: bool,
+        lines: List[Line],
+        labels: List[Label] = None,
+        is_training: bool = False,
+        is_validation: bool = False,
+        is_test: bool = False,
     ) -> Dict[str, Any]:
         """
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
+        lines : List[Line]
             ``iter_dict`` from any dataset that will be passed on to the encoder
+        labels: List[Label]
+            A list of labels for every instance
         is_training : bool
             running forward on training dataset?
         is_validation : bool
-            running forward on training dataset ?
+            running forward on validation dataset?
         is_test : bool
             running forward on test dataset?
 
@@ -77,7 +100,7 @@ class SimpleClassifier(nn.Module, ClassNursery):
 
         """
 
-        encoding = self.encoder(iter_dict=iter_dict)
+        encoding = self.encoder(lines)
 
         # N * C
         # N - batch size
@@ -93,13 +116,21 @@ class SimpleClassifier(nn.Module, ClassNursery):
         output_dict = {"logits": logits, "normalized_probs": normalized_probs}
 
         if is_training or is_validation:
-            labels = iter_dict["label"]
-            labels = labels.squeeze(1)
-            assert labels.ndimension() == 1, self.msg_printer.fail(
+            label_indices = []
+            for label in labels:
+                label_ = label.tokens[self.label_namespace]
+                label_ = [tok.text for tok in label_]
+                label_ = self.label_numericalizer.numericalize_instance(instance=label_)
+                label_indices.append(label_[0])  # taking only the first label here
+
+            labels_tensor = torch.LongTensor(label_indices)
+            labels_tensor = labels_tensor.to(self.device)
+
+            assert labels_tensor.ndimension() == 1, self.msg_printer.fail(
                 "the labels should have 1 dimension "
-                "your input has shape {0}".format(labels.size())
+                "your input has shape {0}".format(labels_tensor.size())
             )
-            loss = self._loss(logits, labels)
+            loss = self._loss(logits, labels_tensor)
             output_dict["loss"] = loss
 
         return output_dict
diff --git a/sciwing/modules/bow_encoder.py b/sciwing/modules/bow_encoder.py
index 471c63c..f3980f8 100644
--- a/sciwing/modules/bow_encoder.py
+++ b/sciwing/modules/bow_encoder.py
@@ -1,24 +1,17 @@
 import torch
 import torch.nn as nn
 from wasabi import Printer
-from typing import Dict, Any
+from typing import List, Any
+from sciwing.data.line import Line
 from sciwing.utils.class_nursery import ClassNursery
 
 
 class BOW_Encoder(nn.Module, ClassNursery):
-    def __init__(
-        self,
-        emb_dim: int = 100,
-        embedder=None,
-        dropout_value: float = 0,
-        aggregation_type="sum",
-    ):
+    def __init__(self, embedder=None, dropout_value: float = 0, aggregation_type="sum"):
         """Bag of Words Encoder
 
         Parameters
         ----------
-        emb_dim : int
-            Embedding dimension of the words
         embedder : nn.Module
             Any embedder that you would want to use
         dropout_value : float
@@ -31,7 +24,7 @@ class BOW_Encoder(nn.Module, ClassNursery):
                     Aggregate word embedding by averaging them
         """
         super(BOW_Encoder, self).__init__()
-        self.emb_dim = emb_dim
+        self.emb_dim = embedder.get_embedding_dimension()
         self.embedder = embedder
         self.dropout_value = dropout_value
         self.aggregation_type = aggregation_type
@@ -42,23 +35,24 @@ class BOW_Encoder(nn.Module, ClassNursery):
 
         self.dropout = nn.Dropout(p=self.dropout_value)
 
-    def forward(self, iter_dict: Dict[str, Any]) -> torch.FloatTensor:
+    def forward(self, lines: List[Line]) -> torch.FloatTensor:
         """
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
+        lines : Dict[str, Any]
             The iter_dict returned by a dataset
 
         Returns
         -------
         torch.FloatTensor
-            The bag of words encoded embedding
+            The bag of words encoded embedding either average or summed
+            The size is [batch_size, embedding_dimension]
 
         """
 
         # N * T * D
-        embeddings = self.embedder(iter_dict)
+        embeddings = self.embedder(lines)
 
         # N * T * D
         embeddings = self.dropout(embeddings)
diff --git a/sciwing/modules/charlstm_encoder.py b/sciwing/modules/charlstm_encoder.py
index 51d51be..f18344f 100644
--- a/sciwing/modules/charlstm_encoder.py
+++ b/sciwing/modules/charlstm_encoder.py
@@ -3,8 +3,10 @@ import torch.nn as nn
 from typing import Dict, Any
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
 from sciwing.utils.class_nursery import ClassNursery
+from deprecated import deprecated
 
 
+@deprecated(reason="We have a new char encoder that is easier to use", version=0.1)
 class CharLSTMEncoder(nn.Module, ClassNursery):
     def __init__(
         self,
diff --git a/sciwing/modules/elmo_lstm_encoder.py b/sciwing/modules/elmo_lstm_encoder.py
index 8bdd3da..0c2b448 100644
--- a/sciwing/modules/elmo_lstm_encoder.py
+++ b/sciwing/modules/elmo_lstm_encoder.py
@@ -3,8 +3,15 @@ from sciwing.modules.embedders.elmo_embedder import ElmoEmbedder
 import torch
 from typing import List
 import wasabi
+from deprecated import deprecated
 
 
+@deprecated(
+    reason="ElmoEmbedder will be deprecated "
+    "Please use concat embedder and lstm 2 vec module to achieve the same thing. "
+    "This will be removed in version 1",
+    version=0.1,
+)
 class ElmoLSTMEncoder(nn.Module):
     def __init__(
         self,
diff --git a/sciwing/modules/embedders/__init__.py b/sciwing/modules/embedders/__init__.py
index cee283c..10c9d13 100644
--- a/sciwing/modules/embedders/__init__.py
+++ b/sciwing/modules/embedders/__init__.py
@@ -1,4 +1,4 @@
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
 from sciwing.modules.embedders.bert_embedder import BertEmbedder
diff --git a/sciwing/modules/embedders/bert_embedder.py b/sciwing/modules/embedders/bert_embedder.py
index 4908ab4..625fb71 100644
--- a/sciwing/modules/embedders/bert_embedder.py
+++ b/sciwing/modules/embedders/bert_embedder.py
@@ -1,12 +1,14 @@
 import torch
 import torch.nn as nn
-from pytorch_pretrained_bert import BertTokenizer, BertModel
-from sciwing.utils.common import pack_to_length
-from typing import Dict, Any, Union
+from sciwing.tokenizers.bert_tokenizer import TokenizerForBert
+from sciwing.numericalizers.transformer_numericalizer import NumericalizerForTransformer
+from typing import List, Union
 import wasabi
 import sciwing.constants as constants
-import os
+from pytorch_pretrained_bert import BertModel
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.line import Line
+import os
 
 PATHS = constants.PATHS
 EMBEDDING_CACHE_DIR = PATHS["EMBEDDING_CACHE_DIR"]
@@ -15,18 +17,16 @@ EMBEDDING_CACHE_DIR = PATHS["EMBEDDING_CACHE_DIR"]
 class BertEmbedder(nn.Module, ClassNursery):
     def __init__(
         self,
-        emb_dim: int = 768,
         dropout_value: float = 0.0,
         aggregation_type: str = "sum",
         bert_type: str = "bert-base-uncased",
+        word_tokens_namespace="tokens",
         device: Union[torch.device, str] = torch.device("cpu"),
     ):
         """ Bert Embedder that embeds the given instance to BERT embeddings
 
         Parameters
         ----------
-        emb_dim : int
-            Embedding dimension
         dropout_value : float
             The amount of dropout to be added after the embedding
         aggregation_type : str
@@ -65,11 +65,14 @@ class BertEmbedder(nn.Module, ClassNursery):
             scibert-sci-uncased
                 12 layer transformer train on scientific documents on ncased scientific vocab
 
+        word_tokens_namespace : str
+            The namespace in the liens where the tokens are stored
+
         device :  Union[torch.device, str]
             The device on which the model is run.
         """
         super(BertEmbedder, self).__init__()
-        self.emb_dim = emb_dim
+
         self.dropout_value = dropout_value
         self.aggregation_type = aggregation_type
         self.bert_type = bert_type
@@ -77,97 +80,100 @@ class BertEmbedder(nn.Module, ClassNursery):
             self.device = torch.device(device)
         else:
             self.device = device
+        self.word_tokens_namespace = word_tokens_namespace
         self.msg_printer = wasabi.Printer()
-        self.allowed_bert_types = [
-            "bert-base-uncased",
-            "bert-large-uncased",
-            "bert-base-cased",
-            "bert-large-cased",
-            "scibert-base-cased",
-            "scibert-sci-cased",
-            "scibert-base-uncased",
-            "scibert-sci-uncased",
-        ]
+        self.embedder_name = bert_type
+
         self.scibert_foldername_mapping = {
             "scibert-base-cased": "scibert_basevocab_cased",
             "scibert-sci-cased": "scibert_scivocab_cased",
             "scibert-base-uncased": "scibert_basevocab_uncased",
             "scibert-sci-uncased": "scibert_scivocab_uncased",
         }
-        self.model_type_or_folder_url = None
-        self.vocab_type_or_filename = None
-
-        assert self.bert_type in self.allowed_bert_types
 
         if "scibert" in self.bert_type:
             foldername = self.scibert_foldername_mapping[self.bert_type]
             self.model_type_or_folder_url = os.path.join(
                 EMBEDDING_CACHE_DIR, foldername, "weights.tar.gz"
             )
-            self.vocab_type_or_filename = os.path.join(
-                EMBEDDING_CACHE_DIR, foldername, "vocab.txt"
-            )
+
         else:
             self.model_type_or_folder_url = self.bert_type
-            self.vocab_type_or_filename = self.bert_type
 
         # load the bert model
         with self.msg_printer.loading(" Loading Bert tokenizer and model. "):
-            self.bert_tokenizer = BertTokenizer.from_pretrained(
-                self.vocab_type_or_filename
+            self.bert_tokenizer = TokenizerForBert(
+                bert_type=self.bert_type, do_basic_tokenize=False
+            )
+            self.bert_numericalizer = NumericalizerForTransformer(
+                tokenizer=self.bert_tokenizer
             )
             self.model = BertModel.from_pretrained(self.model_type_or_folder_url)
             self.model.eval()
             self.model.to(self.device)
 
         self.msg_printer.good(f"Finished Loading {self.bert_type} model and tokenizer")
+        self.embedding_dimension = self.get_embedding_dimension()
 
-    def forward(self, iter_dict: Dict[str, Any]) -> torch.Tensor:
+    def forward(self, lines: List[Line]) -> torch.Tensor:
         """
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            It expects "raw_intance" to be present in the iter dict.
-            "raw_instance" is the instance that is not padded
+        lines : List[Line]
+            A list of lines
 
         Returns
         -------
         torch.Tensor
             The bert embeddings for all the words in the instances
-            The size of the returned embedding is ``[batch_size, num_time_steps, emb_dim]``
+            The size of the returned embedding is ``[batch_size, max_len_word_tokens, emb_dim]``
 
         """
 
         # word_tokenize all the text string in the batch
-        x = iter_dict["raw_instance"]
-        tokenized_text = list(map(self.bert_tokenizer.tokenize, x))
-        lengths = list(map(lambda tokenized: len(tokenized), tokenized_text))
-        max_len = sorted(lengths, reverse=True)[0]
-
+        bert_tokens_lengths = []
+        word_tokens_lengths = []
+        for line in lines:
+            text = line.text
+            word_tokens = line.tokens[self.word_tokens_namespace]
+            word_tokens_lengths.append(len(word_tokens))
+
+            # split every token to subtokens
+            for word_token in word_tokens:
+                word_piece_tokens = self.bert_tokenizer.tokenize(word_token.text)
+                word_token.sub_tokens = word_piece_tokens
+
+            bert_tokenized_text = self.bert_tokenizer.tokenize(text)
+            line.tokenizers[self.embedder_name] = self.bert_tokenizer
+            line.add_tokens(tokens=bert_tokenized_text, namespace=self.embedder_name)
+            bert_tokens_lengths.append(len(bert_tokenized_text))
+
+        max_len_bert = max(bert_tokens_lengths)
+        max_len_words = max(word_tokens_lengths)
         # pad the tokenized text to a maximum length
-        padded_tokenized_text = []
-        for tokens in tokenized_text:
-            padded_tokens = pack_to_length(
-                tokenized_text=tokens,
-                max_length=max_len,
-                pad_token="[PAD]",
+        indexed_tokens = []
+        segment_ids = []
+        for line in lines:
+            bert_tokens = line.tokens[self.embedder_name]
+            tokens_numericalized = self.bert_numericalizer.numericalize_instance(
+                instance=bert_tokens
+            )
+            tokens_numericalized = self.bert_numericalizer.pad_instance(
+                numericalized_text=tokens_numericalized,
+                max_length=max_len_bert + 2,
                 add_start_end_token=True,
-                start_token="[CLS]",
-                end_token="[SEP]",
             )
-            padded_tokenized_text.append(padded_tokens)
+            segment_numbers = [0] * len(tokens_numericalized)
 
-        # convert them to ids based on bert vocab
-        indexed_tokens = list(
-            map(self.bert_tokenizer.convert_tokens_to_ids, padded_tokenized_text)
-        )
-        segment_ids = list(
-            map(lambda tokens_list: [0] * len(tokens_list), indexed_tokens)
-        )
+            tokens_numericalized = torch.LongTensor(tokens_numericalized)
+            segment_numbers = torch.LongTensor(segment_numbers)
 
-        tokens_tensor = torch.tensor(indexed_tokens)
-        segment_tensor = torch.tensor(segment_ids)
+            indexed_tokens.append(tokens_numericalized)
+            segment_ids.append(segment_numbers)
+
+        tokens_tensor = torch.stack(indexed_tokens)
+        segment_tensor = torch.stack(segment_ids)
 
         tokens_tensor = tokens_tensor.to(self.device)
         segment_tensor = segment_tensor.to(self.device)
@@ -180,16 +186,68 @@ class BertEmbedder(nn.Module, ClassNursery):
         elif "large" in self.bert_type:
             assert len(encoded_layers) == 24
 
-        # num_bert_layers, batch_size, sequence_length, bert_hidden_dimension
+        # num_bert_layers, batch_size, max_len_bert + 2, bert_hidden_dimension
         all_layers = torch.stack(encoded_layers, dim=0)
 
+        # batch_size, max_len_bert + 2, bert_hidden_dimension
         if self.aggregation_type == "sum":
-            sum_layers = torch.sum(all_layers, dim=0)
-            return sum_layers
+            encoding = torch.sum(all_layers, dim=0)
 
         elif self.aggregation_type == "average":
-            average_layers = torch.mean(all_layers, dim=0)
-            return average_layers
+            encoding = torch.mean(all_layers, dim=0)
+        else:
+            raise ValueError(f"The aggregation type {self.aggregation_type}")
+
+        # fill up the appropriate embeddings in the tokens of the lines
+        batch_embeddings = []
+        for idx, line in enumerate(lines):
+            word_tokens = line.tokens[self.word_tokens_namespace]  # word tokens
+            bert_tokens_ = line.tokens[self.embedder_name]
+            token_embeddings = encoding[idx]  # max_len_bert + 2, bert_hidden_dimensiofn
+
+            len_word_tokens = len(word_tokens)
+            len_bert_tokens = len(bert_tokens_)
+            padding_length_bert = max_len_bert - len_bert_tokens
+            padding_length_words = max_len_words - len_word_tokens
+
+            # do not want embeddings for padding
+            if padding_length_bert > 0:
+                token_embeddings = token_embeddings[:-padding_length_bert]
+
+            # do not want embeddings for start and end tokens
+            token_embeddings = token_embeddings[1:-1]
+
+            # just have embeddings for the bert tokens now
+            # without padding and start and end tokens
+            assert token_embeddings.size(0) == len_bert_tokens, (
+                f"bert token embeddings size {token_embeddings.size()} and length of bert tokens "
+                f"{len_bert_tokens}"
+            )
 
-    def __call__(self, iter_dict: Dict[str, Any]) -> torch.Tensor:
-        return self.forward(iter_dict)
+            line_embeddings = []
+            for token in word_tokens:
+                idx = 0
+                sub_tokens = token.sub_tokens
+                len_sub_tokens = len(sub_tokens)
+
+                # taking the embedding of only the first token
+                # TODO: Have different strategies for this
+                emb = token_embeddings[idx]
+                line_embeddings.append(emb)
+                token.set_embedding(name=self.embedder_name, value=emb)
+                idx += len_sub_tokens
+
+            for i in range(padding_length_words):
+                zeros = torch.zeros(self.embedding_dimension)
+                zeros = zeros.to(self.device)
+                line_embeddings.append(zeros)
+
+            line_embeddings = torch.stack(line_embeddings)
+            batch_embeddings.append(line_embeddings)
+
+        # batch_size, max_len_words, bert_hidden_dimension
+        batch_embeddings = torch.stack(batch_embeddings)
+        return batch_embeddings
+
+    def get_embedding_dimension(self) -> int:
+        return self.model.config.hidden_size
diff --git a/sciwing/modules/embedders/bow_elmo_embedder.py b/sciwing/modules/embedders/bow_elmo_embedder.py
index cd3dce1..e03ab00 100644
--- a/sciwing/modules/embedders/bow_elmo_embedder.py
+++ b/sciwing/modules/embedders/bow_elmo_embedder.py
@@ -1,27 +1,23 @@
 import torch
 from allennlp.commands.elmo import ElmoEmbedder
 import wasabi
-from typing import List, Iterable, Dict, Any
+from typing import List, Any
 import torch.nn as nn
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.line import Line
 
 
 class BowElmoEmbedder(nn.Module, ClassNursery):
     def __init__(
         self,
-        emb_dim: int = 1024,
-        dropout_value: float = 0.0,
         layer_aggregation: str = "sum",
         cuda_device_id: int = -1,
+        word_tokens_namespace="tokens",
     ):
         """ Bag of words Elmo Embedder which aggregates elmo embedding for every token
 
         Parameters
         ----------
-        emb_dim : int
-            Embedding dimension
-        dropout_value : float
-            Any input dropout to be applied to the embeddings
         layer_aggregation : str
             You can chose one of ``[sum, average, last, first]``
             which decides how to aggregate different layers of ELMO. ELMO produces three
@@ -39,10 +35,14 @@ class BowElmoEmbedder(nn.Module, ClassNursery):
         cuda_device_id : int
             Cuda device id on which representations will be transferred
             -1 indicates cpu
+
+        word_tokens_namespace: int
+            Namespace where all the word tokens are stored
         """
         super(BowElmoEmbedder, self).__init__()
-        self.emb_dim = emb_dim
-        self.dropout_value = dropout_value
+        self.embedding_dimension = self.get_embedding_dimension()
+        self.embedder_name = "elmo"
+        self.word_tokens_namespace = word_tokens_namespace
         self.layer_aggregation_type = layer_aggregation
         self.allowed_layer_aggregation_types = ["sum", "average", "last", "first"]
         self.cuda_device_id = cuda_device_id
@@ -65,53 +65,82 @@ class BowElmoEmbedder(nn.Module, ClassNursery):
             self.elmo = ElmoEmbedder(cuda_device=self.cuda_device_id)
         self.msg_printer.good("Finished Loading Elmo object")
 
-    def forward(self, iter_dict: Dict[str, Any]) -> torch.Tensor:
+    def forward(self, lines: List[Line]) -> torch.Tensor:
         """
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            ``iter_dict`` from any dataset. Expects ``instance`` to be present in the
-            ``iter_dict`` where instance is a list of sentences and the tokens are separated by
-            space
+        lines : List[Line]
+            Just a list of lines
 
         Returns
         -------
         torch.Tensor
             Returns the representation for every token in the instance
-            ``[batch_size, max_len, emb_dim]``. In case of Elmo the ``emb_dim`` is 1024
+            ``[batch_size, max_num_words, emb_dim]``. In case of Elmo the ``emb_dim`` is 1024
 
 
         """
         # [np.array] - A generator of embeddings
         # each array in the list is of the shape (3, #words_in_sentence, 1024)
-        x = iter_dict["instance"]
-        x = x if isinstance(x, list) else [x]
-        x = [instance.split() for instance in x]
-
-        embedded = list(self.elmo.embed_sentences(x))
-
-        # bs, 3, #words_in_sentence, 1024
-        embedded = torch.FloatTensor(embedded)
-
-        embedding_ = None
-        # aggregate of word embeddings
-        if self.layer_aggregation_type == "sum":
-            # bs, #words_in_sentence, 1024
-            embedding_ = torch.sum(embedded, dim=1)
-
-        elif self.layer_aggregation_type == "average":
-            # mean across all layers
-            embedding_ = torch.mean(embedded, dim=1)
-
-        elif self.layer_aggregation_type == "last":
-            # bs, max_len, 1024
-            embedding_ = embedded[:, -1, :, :]
-
-        elif self.layer_aggregation_type == "first":
-            # bs, max_len, 1024
-            embedding_ = embedded[:, 0, :, :]
-
-        embedding_ = embedding_.to(self.device)
 
-        return embedding_
+        batch_tokens = []
+        token_lengths = []
+        for line in lines:
+            line_tokens = line.tokens[self.word_tokens_namespace]
+            line_tokens = [tok.text for tok in line_tokens]
+            batch_tokens.append(line_tokens)
+            token_lengths.append(len(line_tokens))
+
+        max_len = max(token_lengths)
+        embedded = list(self.elmo.embed_sentences(batch_tokens))
+
+        batch_embeddings = []
+
+        for idx, (line, embedding) in enumerate(zip(lines, embedded)):
+            tokens = line.tokens[self.word_tokens_namespace]
+            line_embeddings = []
+            padding_length = max_len - len(tokens)
+            embedding = torch.FloatTensor(embedding)
+            embedding = embedding.to(self.device)
+
+            # 3, #words_in_sentence, 1024
+
+            # aggregate of word embeddings
+            if self.layer_aggregation_type == "sum":
+                # words_in_sentence, 1024
+                embedding = torch.sum(embedding, dim=0)
+
+            elif self.layer_aggregation_type == "average":
+                # mean across all layers
+                embedding = torch.mean(embedding, dim=0)
+
+            elif self.layer_aggregation_type == "last":
+                # words_in_sentence, 1024
+                embedding = embedding[-1, :, :]
+
+            elif self.layer_aggregation_type == "first":
+                # words_in_sentence, 1024
+                embedding = embedding[0, :, :]
+            else:
+                raise ValueError(
+                    f"Layer aggregation can be one of sum, average, last and first"
+                )
+
+            for token, token_emb in zip(tokens, embedding):
+                token.set_embedding(self.embedder_name, token_emb)
+                line_embeddings.append(token_emb)
+
+            # for batching
+            for i in range(padding_length):
+                zeros = torch.zeros(self.embedding_dimension, device=self.device)
+                line_embeddings.append(zeros)
+
+            line_embeddings = torch.stack(line_embeddings)
+            batch_embeddings.append(line_embeddings)
+
+        batch_embeddings = torch.stack(batch_embeddings)
+        return batch_embeddings
+
+    def get_embedding_dimension(self) -> int:
+        return 1024
diff --git a/sciwing/modules/embedders/char_embedder.py b/sciwing/modules/embedders/char_embedder.py
new file mode 100644
index 0000000..429225c
--- /dev/null
+++ b/sciwing/modules/embedders/char_embedder.py
@@ -0,0 +1,154 @@
+import torch.nn as nn
+from typing import List
+from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.datasets_manager import DatasetsManager
+from sciwing.data.line import Line
+import torch
+
+
+class CharEmbedder(nn.Module, ClassNursery):
+    def __init__(
+        self,
+        char_embedding_dimension: int,
+        hidden_dimension: int,
+        datasets_manager: DatasetsManager = None,
+        word_tokens_namespace: str = "tokens",
+        char_tokens_namespace: str = "char_tokens",
+    ):
+        """ This is a character embedder that takes in lines and collates the character
+        embeddings for all the tokens in the lines.
+
+        Parameters
+        ----------
+        char_embedding_dimension : int
+            The dimension of the character embedding
+        word_tokens_namespace : int
+            The name space where the words are saved
+        char_tokens_namespace : str
+            The namespace where the character tokens are saved
+        datasets_manager : DatasetsManager
+            The dataset manager that handles all the datasets
+        hidden_dimension : int
+            The hidden dimension of the LSTM which will be used to get
+            character embeddings
+        """
+        super(CharEmbedder, self).__init__()
+        self.char_embedding_dimension = char_embedding_dimension
+        self.word_tokens_namespace = word_tokens_namespace
+        self.char_tokens_namespace = char_tokens_namespace
+        self.datasets_manager = datasets_manager
+        self.hidden_dimension = hidden_dimension
+
+        self.char_vocab = self.datasets_manager.namespace_to_vocab[
+            self.char_tokens_namespace
+        ]
+        self.word_vocab = self.datasets_manager.namespace_to_vocab[
+            self.word_tokens_namespace
+        ]
+
+        self.char_numericalizer = self.datasets_manager.namespace_to_numericalizer[
+            self.char_tokens_namespace
+        ]
+        self.word_numericalizer = self.datasets_manager.namespace_to_numericalizer[
+            self.word_tokens_namespace
+        ]
+        self.idx2items = self.char_vocab.idx2token
+        self.num_embeddings = len(self.idx2items)
+        self.embedder_name = "char_embedding"
+
+        self.embedding = nn.Embedding(
+            self.num_embeddings, self.char_embedding_dimension
+        )
+        nn.init.xavier_normal_(self.embedding.weight)
+
+        self.char_rnn = nn.LSTM(
+            self.char_embedding_dimension,
+            self.hidden_dimension,
+            bidirectional=True,
+            num_layers=1,
+            batch_first=True,
+        )
+        self.embedding_dimension = self.get_embedding_dimension()
+
+    def forward(self, lines: List[Line]):
+        batch_size = len(lines)
+        token_lengths = []
+        line_lengths = []
+
+        # find the maximum token length
+        for line in lines:
+            word_tokens = line.tokens[self.word_tokens_namespace]
+            len_tokens = [len(token.text) for token in word_tokens]
+            line_lengths.append(len(word_tokens))
+            token_lengths.extend(len_tokens)
+
+        max_token_length = max(token_lengths)
+        max_line_length = max(line_lengths)
+
+        # numericalized version of all the characters in the lines
+        batch_numericalized = []  # batch_size * max_line_length * max_token_length
+        for line in lines:
+            word_tokens = line.tokens[self.word_tokens_namespace]
+            word_tokens = [tok.text for tok in word_tokens]
+            word_tokens_num = self.word_numericalizer.numericalize_instance(
+                instance=word_tokens
+            )
+            word_tokens_num_padded = self.word_numericalizer.pad_instance(
+                numericalized_text=word_tokens_num, max_length=max_line_length
+            )
+            word_tokens_padded = [
+                self.word_vocab.get_token_from_idx(token)
+                for token in word_tokens_num_padded
+            ]
+
+            line_numericalized = []
+            for token in word_tokens_padded:
+                char_tokens = [char for char in token]
+                char_numericalized = self.char_numericalizer.numericalize_instance(
+                    char_tokens
+                )
+                char_numericalized = self.char_numericalizer.pad_instance(
+                    numericalized_text=char_numericalized,
+                    max_length=max_token_length,
+                    add_start_end_token=False,
+                )  # max_num_chars
+                char_numericalized = torch.LongTensor(char_numericalized)
+                line_numericalized.append(char_numericalized)
+            line_numericalized = torch.stack(
+                line_numericalized
+            )  # max_line_length * max_num_chars
+            batch_numericalized.append(line_numericalized)
+
+        batch_numericalized = torch.stack(batch_numericalized)
+        batch_numericalized = batch_numericalized.view(
+            batch_size * max_line_length, max_token_length
+        )
+
+        # get embedding for every character
+        embedded_tokens = self.embedding(batch_numericalized)
+
+        # pass through bilstm
+        output, (h_n, c_n) = self.char_rnn(embedded_tokens)
+
+        # concat forward and backward hidden states
+        forward_hidden = h_n[0, :, :]
+        backward_hidden = h_n[1, :, :]
+        encoding = torch.cat([forward_hidden, backward_hidden], dim=1)
+        encoding = encoding.view(
+            batch_size, max_line_length, -1
+        )  # batch_size, max_line_length, embedding_dimension
+
+        # set the character embeddings in the line tokens
+        for idx, line in enumerate(lines):
+            line_tokens = line.tokens[self.word_tokens_namespace]
+            line_embeddings = encoding[idx]
+
+            # note: line_tokens has no padding tokens
+            # the zip will get the embeddings for non pad tokens here
+            for token, embedding in zip(line_tokens, line_embeddings):
+                token.set_embedding(self.embedder_name, embedding)
+
+        return encoding
+
+    def get_embedding_dimension(self) -> int:
+        return self.hidden_dimension * 2
diff --git a/sciwing/modules/embedders/concat_embedders.py b/sciwing/modules/embedders/concat_embedders.py
index ef79d33..41e2d16 100644
--- a/sciwing/modules/embedders/concat_embedders.py
+++ b/sciwing/modules/embedders/concat_embedders.py
@@ -1,7 +1,8 @@
 import torch
 import torch.nn as nn
-from typing import Dict, Any, List
+from typing import Any, List
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.line import Line
 
 
 class ConcatEmbedders(nn.Module, ClassNursery):
@@ -17,16 +18,15 @@ class ConcatEmbedders(nn.Module, ClassNursery):
         self.embedders = embedders
 
         for idx, embedder in enumerate(self.embedders):
-            self.add_module(f"embedder {idx}", embedder)
+            self.add_module(f"embedder_{embedder.embedder_name}", embedder)
 
-    def forward(self, iter_dict: Dict[str, Any]):
+    def forward(self, lines: List[Line]):
         """
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            The ``iter_dict`` from any dataset. All the ``keys`` that are expected
-            by different embedders are expected to be present in the iterdict
+        lines : List[Line]
+           A list of Lines.
 
         Returns
         -------
@@ -38,8 +38,13 @@ class ConcatEmbedders(nn.Module, ClassNursery):
         """
         embeddings = []
         for embedder in self.embedders:
-            embedding = embedder(iter_dict)
+            embedding = embedder(lines)
             embeddings.append(embedding)
 
         concat_embedding = torch.cat(embeddings, dim=2)
         return concat_embedding
+
+    def get_embedding_dimension(self):
+        dims = [embedder.get_embedding_dimension() for embedder in self.embedders]
+        emb_dim = sum(dims)
+        return emb_dim
diff --git a/sciwing/modules/embedders/elmo_embedder.py b/sciwing/modules/embedders/elmo_embedder.py
index 9654943..6911666 100644
--- a/sciwing/modules/embedders/elmo_embedder.py
+++ b/sciwing/modules/embedders/elmo_embedder.py
@@ -12,10 +12,7 @@ ELMO_OPTIONS_FILE = FILES["ELMO_OPTIONS_FILE"]
 ELMO_WEIGHTS_FILE = FILES["ELMO_WEIGHTS_FILE"]
 
 
-@deprecated(
-    reason="ElmoEmbedder is deprecated in version 0.1 and will be removed "
-    "in version 0.2"
-)
+@deprecated(reason="ElmoEmbedder is deprecated and will be removed " "in version 0.1")
 class ElmoEmbedder(nn.Module):
     def __init__(
         self, dropout_value: float = 0.0, device: torch.device = torch.device("cpu")
diff --git a/sciwing/modules/embedders/vanilla_embedder.py b/sciwing/modules/embedders/vanilla_embedder.py
deleted file mode 100644
index 867d35a..0000000
--- a/sciwing/modules/embedders/vanilla_embedder.py
+++ /dev/null
@@ -1,47 +0,0 @@
-import torch.nn as nn
-from typing import Dict, Any
-from sciwing.utils.class_nursery import ClassNursery
-
-
-class VanillaEmbedder(nn.Module, ClassNursery):
-    def __init__(self, embedding: nn.Embedding, embedding_dim: int):
-        """ Vanilla Embedder embeds the tokens using the embedding passed.
-
-        Parameters
-        ----------
-        embedding : nn.Embedding
-            A pytoch embedding that maps textual units to embeddings
-        embedding_dim : int
-            The embedding dimension
-        """
-        super(VanillaEmbedder, self).__init__()
-        self.embedding = embedding
-        self.embedding_dim = embedding_dim
-
-    def forward(self, iter_dict: Dict[str, Any]):
-        """
-
-        Parameters
-        ----------
-        iter_dict : Dict[str, Any]
-            ``iter_dict`` from a dataset. It expects ``tokens`` to be present as a key of the
-            iter_dict which is usually of the shape ``[batch_size, max_num_timesteps]``
-
-        Returns
-        -------
-        torch.FloatTensor
-            It returns the embedding of the size ``[batch_size, max_num_timesteps, embedding_dimension]``
-
-        """
-        try:
-            tokens = iter_dict["tokens"]  # N * T
-            assert tokens.dim() == 2
-            embedding = self.embedding(tokens)  # N * T * D
-            return embedding
-        except AttributeError:
-            raise ValueError(f"iter_dict passed should have tokens in them")
-        except AssertionError:
-            raise ValueError(
-                f"tokens passed to vanilla embedder must be 2 dimensions. "
-                f"You passed tokens having {tokens.dim()}"
-            )
diff --git a/sciwing/modules/embedders/word_embedder.py b/sciwing/modules/embedders/word_embedder.py
new file mode 100644
index 0000000..7f29e5a
--- /dev/null
+++ b/sciwing/modules/embedders/word_embedder.py
@@ -0,0 +1,78 @@
+import torch.nn as nn
+import torch
+from typing import List
+from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.line import Line
+from sciwing.vocab.embedding_loader import EmbeddingLoader
+
+
+class WordEmbedder(nn.Module, ClassNursery):
+    def __init__(self, embedding_type: str, word_tokens_namespace="tokens"):
+        """ Word Embedder embeds the tokens using the desired embeddings. These are static
+        embeddings.
+
+        Parameters
+        ----------
+        embedding_type : str
+            The type of embedding that you would want
+        """
+        super(WordEmbedder, self).__init__()
+
+        self.embedding_type = embedding_type
+        self.embedding_loader = EmbeddingLoader(embedding_type=self.embedding_type)
+        self.embedder_name = embedding_type
+        self.embedding_dimension = self.get_embedding_dimension()
+        self.word_tokens_namespace = word_tokens_namespace
+
+    def forward(self, lines: List[Line]) -> torch.FloatTensor:
+        """ This will only consider the "tokens" present in the line. The "tokens"
+        namespace
+
+        Parameters
+        ----------
+        lines : List[Line]
+
+
+        Returns
+        -------
+        torch.FloatTensor
+            It returns the embedding of the size ``[batch_size, max_num_timesteps, embedding_dimension]``
+
+        """
+
+        for line in lines:
+            for token in line.tokens[self.word_tokens_namespace]:
+                try:
+                    emb = self.embedding_loader.embeddings[token.text]
+                except:
+                    emb = torch.zeros(self.embedding_dimension)
+                emb = torch.FloatTensor(emb)
+                token.set_embedding(name=self.embedder_name, value=emb)
+
+        # return the [batch_size, longest_sequence, embedding_dimension]
+        # This module store all the information in the tokens and the sentences
+
+        line_lengths = [len(line.tokens[self.word_tokens_namespace]) for line in lines]
+        max_line_length = max(line_lengths)
+
+        batch_embeddings = []
+        for idx, length in enumerate(line_lengths):
+            sentence_embedding = []
+            padding_length = max_line_length - length
+            line = lines[idx]
+            tokens = line.tokens[self.word_tokens_namespace]
+            for token in tokens:
+                token_embedding = token.get_embedding(name=self.embedder_name)
+                sentence_embedding.append(token_embedding)
+            for i in range(padding_length):
+                zeros = torch.zeros(self.embedding_loader.embedding_dimension)
+                sentence_embedding.append(zeros)
+
+            sentence_embedding = torch.stack(sentence_embedding)
+            batch_embeddings.append(sentence_embedding)
+
+        batch_embeddings = torch.stack(batch_embeddings)
+        return batch_embeddings
+
+    def get_embedding_dimension(self) -> int:
+        return self.embedding_loader.embedding_dimension
diff --git a/sciwing/modules/lstm2seqencoder.py b/sciwing/modules/lstm2seqencoder.py
index 4e4f2da..0011861 100644
--- a/sciwing/modules/lstm2seqencoder.py
+++ b/sciwing/modules/lstm2seqencoder.py
@@ -1,14 +1,14 @@
 import torch
 import torch.nn as nn
 import wasabi
-from typing import Dict, Any
+from typing import List
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.data.line import Line
 
 
 class Lstm2SeqEncoder(nn.Module, ClassNursery):
     def __init__(
         self,
-        emb_dim: int,
         embedder: nn.Module,
         dropout_value: float = 0.0,
         hidden_dim: int = 1024,
@@ -22,8 +22,6 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
 
         Parameters
         ----------
-        emb_dim : int
-            Embedding dimension of the tokens
         embedder : nn.Module
             Any embedder can be used for this purpose
         dropout_value : float
@@ -46,8 +44,8 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
         device : torch.device
         """
         super(Lstm2SeqEncoder, self).__init__()
-        self.emb_dim = emb_dim
         self.embedder = embedder
+        self.emb_dim = embedder.get_embedding_dimension()
         self.dropout_value = dropout_value
         self.hidden_dim = hidden_dim
         self.bidirectional = bidirectional
@@ -80,7 +78,7 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
 
     def forward(
         self,
-        iter_dict: Dict[str, Any],
+        lines: List[Line],
         c0: torch.FloatTensor = None,
         h0: torch.FloatTensor = None,
     ) -> torch.Tensor:
@@ -88,8 +86,8 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
 
             Parameters
             ----------
-            iter_dict : Dict[str, Any]
-                Any ``iter_dict`` that is passed from the dataset
+            lines : List[Line]
+                A list of lines
             c0 : torch.FloatTensor
                 The initial state vector for the LSTM
             h0 : torch.FloatTensor
@@ -103,10 +101,9 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
                 [batch_size, 2*hidden_dim] if bidirectional
         """
 
-        # TODO: the batch size should be present in the iter_dict
-        batch_size, seq_length = iter_dict["tokens"].size()
-
-        embeddings = self.embedder(iter_dict=iter_dict)
+        embeddings = self.embedder(lines=lines)
+        batch_size = len(lines)
+        seq_length = embeddings.size(1)
 
         embeddings = self.emb_dropout(embeddings)
 
@@ -126,6 +123,8 @@ class Lstm2SeqEncoder(nn.Module, ClassNursery):
                 encoding = torch.cat([forward_output, backward_output], dim=2)
             elif self.combine_strategy == "sum":
                 encoding = torch.add(forward_output, backward_output)
+            else:
+                raise ValueError("The combine strategy should be one of concat or sum")
         else:
             encoding = output
 
diff --git a/sciwing/modules/lstm2vecencoder.py b/sciwing/modules/lstm2vecencoder.py
index 08ed16e..14fd4ae 100644
--- a/sciwing/modules/lstm2vecencoder.py
+++ b/sciwing/modules/lstm2vecencoder.py
@@ -1,14 +1,14 @@
 import torch
 import torch.nn as nn
 import wasabi
-from typing import Dict, Any
+from typing import Dict, List
+from sciwing.data.line import Line
 from sciwing.utils.class_nursery import ClassNursery
 
 
 class LSTM2VecEncoder(nn.Module, ClassNursery):
     def __init__(
         self,
-        emb_dim: int,
         embedder,
         dropout_value: float = 0.0,
         hidden_dim: int = 1024,
@@ -21,8 +21,6 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
 
         Parameters
         ----------
-        emb_dim : int
-            Embedding dimension of the embedder
         embedder : nn.Module
             Any embedder can be passed
         dropout_value : float
@@ -39,8 +37,8 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
             The device on which the model is run
         """
         super(LSTM2VecEncoder, self).__init__()
-        self.emb_dim = emb_dim
         self.embedder = embedder
+        self.emb_dim = embedder.get_embedding_dimension()
         self.dropout_value = dropout_value
         self.hidden_dimension = hidden_dim
         self.bidirectional = bidirectional
@@ -71,7 +69,7 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
 
     def forward(
         self,
-        iter_dict: Dict[str, Any],
+        lines: List[Line],
         c0: torch.FloatTensor = None,
         h0: torch.FloatTensor = None,
     ) -> torch.FloatTensor:
@@ -79,8 +77,8 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
 
         Parameters
         ----------
-        iter_dict : Dict[str, Any]
-            Any ``iter_dict`` that is passed from the dataset
+        lines: List[Line]
+            A list of lines to be encoder
         c0 : torch.FloatTensor
             The initial state vector for the LSTM
         h0 : torch.FloatTensor
@@ -94,10 +92,8 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
             [batch_size, 2*hidden_dim] if bidirectional
         """
 
-        # TODO: the batch size should be present in the iter_dict
-
-        batch_size = iter_dict["tokens"].size(0)
-        embedded_tokens = self.embedder(iter_dict)
+        batch_size = len(lines)
+        embedded_tokens = self.embedder(lines)
         embedded_tokens = self.emb_dropout(embedded_tokens)
 
         if h0 is None or c0 is None:
@@ -120,6 +116,8 @@ class LSTM2VecEncoder(nn.Module, ClassNursery):
                 encoding = torch.cat([forward_hidden, backward_hidden], dim=1)
             elif self.combine_strategy == "sum":
                 encoding = torch.add(forward_hidden, backward_hidden)
+            else:
+                raise ValueError(f"The combine strategy should be one of concat or sum")
         else:
             encoding = h_n[0, :, :]
 
diff --git a/sciwing/numericalizer/numericalizer.py b/sciwing/numericalizer/numericalizer.py
deleted file mode 100644
index ff9b0fa..0000000
--- a/sciwing/numericalizer/numericalizer.py
+++ /dev/null
@@ -1,66 +0,0 @@
-from typing import Dict, List, Tuple
-from sciwing.vocab.vocab import Vocab
-
-
-class Numericalizer:
-    def __init__(self, vocabulary: Vocab):
-        """ Numericalizer converts tokens that are strings to numbers
-
-        Parameters
-        ----------
-        vocabulary : Vocab
-            A vocabulary object that is built using a set of tokenized strings
-
-        """
-        self.vocabulary = vocabulary
-
-        if not self.vocabulary.vocab:
-            self.vocabulary.build_vocab()
-
-    def numericalize_instance(self, instance: List[str]) -> List[int]:
-        """ Numericalize a single instance
-
-        Parameters
-        ------------
-        instance : List[str]
-            An instance is a list of tokens
-
-
-        Returns
-        --------------
-        List[int]
-            Numericalized instance
-        """
-        numerical_tokens = []
-        len_tokens = len(instance)
-
-        for string in instance:
-            idx = self.vocabulary.get_idx_from_token(string)
-            numerical_tokens.append(idx)
-
-        assert len(numerical_tokens) == len_tokens
-
-        return numerical_tokens
-
-    def numericalize_batch_instances(
-        self, instances: List[List[str]]
-    ) -> List[List[int]]:
-        """ Numericalizes a batch of instances
-
-        Parameters
-        ----------
-        instances : List[List[str]]
-            A list of tokenized sentences
-
-        Returns
-        -------
-        List[List[int]]
-            A list of numericalized instances
-
-        """
-        numerical_tokens_batch = []
-        for instance in instances:
-            tokens = self.numericalize_instance(instance)
-            numerical_tokens_batch.append(tokens)
-
-        return numerical_tokens_batch
diff --git a/sciwing/numericalizers/__init__.py b/sciwing/numericalizers/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/sciwing/numericalizers/base_numericalizer.py b/sciwing/numericalizers/base_numericalizer.py
new file mode 100644
index 0000000..aeda3c6
--- /dev/null
+++ b/sciwing/numericalizers/base_numericalizer.py
@@ -0,0 +1,34 @@
+from sciwing.vocab.vocab import Vocab
+from typing import List
+from abc import ABCMeta, abstractmethod
+
+
+class BaseNumericalizer(metaclass=ABCMeta):
+    def __init__(self, vocabulary: Vocab = None):
+        pass
+
+    @abstractmethod
+    def numericalize_instance(self, instance: List[str]):
+        pass
+
+    @abstractmethod
+    def numericalize_batch_instances(self, instances: List[List[str]]):
+        pass
+
+    @abstractmethod
+    def pad_instance(
+        self,
+        numericalized_text: List[int],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ):
+        pass
+
+    @abstractmethod
+    def pad_batch_instances(
+        self,
+        instances: List[List[int]],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ):
+        pass
diff --git a/sciwing/numericalizers/numericalizer.py b/sciwing/numericalizers/numericalizer.py
new file mode 100644
index 0000000..eccbe6b
--- /dev/null
+++ b/sciwing/numericalizers/numericalizer.py
@@ -0,0 +1,150 @@
+from typing import List
+from sciwing.vocab.vocab import Vocab
+from sciwing.numericalizers.base_numericalizer import BaseNumericalizer
+
+
+class Numericalizer(BaseNumericalizer):
+    def __init__(self, vocabulary: Vocab = None):
+        """ Numericalizer converts tokens that are strings to numbers
+
+        Parameters
+        ----------
+        vocabulary : Vocab
+            A vocabulary object that is built using a set of tokenized strings
+
+        """
+        super().__init__(vocabulary)
+        self.vocabulary = vocabulary
+
+        if vocabulary and not self.vocabulary.vocab:
+            self.vocabulary.build_vocab()
+
+    def numericalize_instance(self, instance: List[str]) -> List[int]:
+        """ Numericalize a single instance
+
+        Parameters
+        ------------
+        instance : List[str]
+            An instance is a list of tokens
+
+
+        Returns
+        --------------
+        List[int]
+            Numericalized instance
+        """
+        numerical_tokens = []
+        len_tokens = len(instance)
+
+        for string in instance:
+            idx = self.vocabulary.get_idx_from_token(string)
+            numerical_tokens.append(idx)
+
+        assert len(numerical_tokens) == len_tokens
+
+        return numerical_tokens
+
+    def numericalize_batch_instances(
+        self, instances: List[List[str]]
+    ) -> List[List[int]]:
+        """ Numericalizes a batch of instances
+
+        Parameters
+        ----------
+        instances : List[List[str]]
+            A list of tokenized sentences
+
+        Returns
+        -------
+        List[List[int]]
+            A list of numericalized instances
+
+        """
+        numerical_tokens_batch = []
+        for instance in instances:
+            tokens = self.numericalize_instance(instance)
+            numerical_tokens_batch.append(tokens)
+
+        return numerical_tokens_batch
+
+    def pad_instance(
+        self,
+        numericalized_text: List[int],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ) -> List[int]:
+        """ Pads the instance according to the vocab object
+
+        Parameters
+        ----------
+        numericalized_text : List[int]
+            Pads a numericalized instance
+        max_length: int
+            The maximum length to pad to
+        add_start_end_token: bool
+            If true, start and end token will be added to
+            the tokenized text
+
+        Returns
+        -------
+        List[int]
+            Padded instance
+
+        """
+        start_token_idx = self.vocabulary.get_idx_from_token(
+            self.vocabulary.start_token
+        )
+        end_token_idx = self.vocabulary.get_idx_from_token(self.vocabulary.end_token)
+        pad_token_idx = self.vocabulary.get_idx_from_token(self.vocabulary.pad_token)
+
+        if not add_start_end_token:
+            numericalized_text = numericalized_text[:max_length]
+        else:
+            max_length = max_length if max_length > 2 else 2
+            numericalized_text = numericalized_text[: max_length - 2]
+            numericalized_text.append(end_token_idx)
+            numericalized_text.insert(0, start_token_idx)
+
+        pad_length = max_length - len(numericalized_text)
+        for i in range(pad_length):
+            numericalized_text.append(pad_token_idx)
+
+        assert len(numericalized_text) == max_length
+
+        return numericalized_text
+
+    def pad_batch_instances(
+        self,
+        instances: List[List[int]],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ) -> List[List[int]]:
+        """ Pads a batch of instances according to the vocab object
+
+        Parameters
+        ----------
+        instances : List[List[int]]
+        max_length : int
+        add_start_end_token : int
+
+        Returns
+        -------
+        List[List[int]]
+        """
+        padded_instances = []
+        for instance in instances:
+            padded_instance = self.pad_instance(
+                numericalized_text=instance,
+                max_length=max_length,
+                add_start_end_token=add_start_end_token,
+            )
+            padded_instances.append(padded_instance)
+        return padded_instances
+
+    @property
+    def vocabulary(self):
+        return self._vocabulary
+
+    @vocabulary.setter
+    def vocabulary(self, value):
+        self._vocabulary = value
diff --git a/sciwing/numericalizers/transformer_numericalizer.py b/sciwing/numericalizers/transformer_numericalizer.py
new file mode 100644
index 0000000..1d27cb0
--- /dev/null
+++ b/sciwing/numericalizers/transformer_numericalizer.py
@@ -0,0 +1,101 @@
+from typing import List, Union
+from sciwing.vocab.vocab import Vocab
+from sciwing.tokenizers.bert_tokenizer import TokenizerForBert
+from sciwing.numericalizers.base_numericalizer import BaseNumericalizer
+from sciwing.data.token import Token
+
+
+class NumericalizerForTransformer(BaseNumericalizer):
+    def __init__(self, vocab: Vocab = None, tokenizer: TokenizerForBert = None):
+        super(NumericalizerForTransformer, self).__init__()
+        self.vocab = vocab
+        self.tokenizer = tokenizer
+
+    def numericalize_instance(
+        self, instance: Union[List[str], List[Token]]
+    ) -> List[int]:
+        if isinstance(instance[0], Token):
+            instance = [tok.text for tok in instance]
+        tokens = self.tokenizer.tokenizer.convert_tokens_to_ids(instance)
+        return tokens
+
+    def numericalize_batch_instances(self, instances: List[List[str]]) -> List[int]:
+        numerical_tokens_batch = []
+
+        for instance in instances:
+            tokens = self.numericalize_instance(instance)
+            numerical_tokens_batch.append(tokens)
+
+        return numerical_tokens_batch
+
+    def pad_instance(
+        self,
+        numericalized_text: List[int],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ) -> List[int]:
+        """ Pads the instance according to the vocab object
+
+            Parameters
+            ----------
+            numericalized_text : List[int]
+                Pads a numericalized instance
+            max_length: int
+                The maximum length to pad to
+            add_start_end_token: bool
+                If true, start and end token will be added to
+                the tokenized text
+
+            Returns
+            -------
+            List[int]
+                Padded instance
+
+                """
+        start_token_idx = self.tokenizer.tokenizer.vocab["[CLS]"]
+        end_token_idx = self.tokenizer.tokenizer.vocab["[SEP]"]
+        pad_token_idx = self.tokenizer.tokenizer.vocab["[PAD]"]
+
+        if not add_start_end_token:
+            numericalized_text = numericalized_text[:max_length]
+        else:
+            max_length = max_length if max_length > 2 else 2
+            numericalized_text = numericalized_text[: max_length - 2]
+            numericalized_text.append(end_token_idx)
+            numericalized_text.insert(0, start_token_idx)
+
+        pad_length = max_length - len(numericalized_text)
+        for i in range(pad_length):
+            numericalized_text.append(pad_token_idx)
+
+        assert len(numericalized_text) == max_length
+
+        return numericalized_text
+
+    def pad_batch_instances(
+        self,
+        instances: List[List[int]],
+        max_length: int,
+        add_start_end_token: bool = True,
+    ):
+        """ Pads a batch of instances according to the vocab object
+
+                Parameters
+                ----------
+                instances : List[List[int]]
+                max_length : int
+                add_start_end_token : int
+
+                Returns
+                -------
+                List[List[int]]
+                """
+        padded_instances = []
+        for instance in instances:
+            padded_instance = self.pad_instance(
+                numericalized_text=instance,
+                max_length=max_length,
+                add_start_end_token=add_start_end_token,
+            )
+            padded_instances.append(padded_instance)
+        return padded_instances
diff --git a/sciwing/tokenizers/bert_tokenizer.py b/sciwing/tokenizers/bert_tokenizer.py
index bdfaefc..014405a 100644
--- a/sciwing/tokenizers/bert_tokenizer.py
+++ b/sciwing/tokenizers/bert_tokenizer.py
@@ -11,9 +11,10 @@ EMBEDDING_CACHE_DIR = PATHS["EMBEDDING_CACHE_DIR"]
 
 
 class TokenizerForBert(BaseTokenizer):
-    def __init__(self, bert_type: str):
+    def __init__(self, bert_type: str, do_basic_tokenize=True):
         super(TokenizerForBert, self).__init__()
         self.bert_type = bert_type
+        self.do_basic_tokenize = do_basic_tokenize
         self.msg_printer = wasabi.Printer()
         self.allowed_bert_types = [
             "bert-base-uncased",
@@ -45,7 +46,9 @@ class TokenizerForBert(BaseTokenizer):
             self.vocab_type_or_filename = self.bert_type
 
         with self.msg_printer.loading("Loading Bert model"):
-            self.tokenizer = BertTokenizer.from_pretrained(self.vocab_type_or_filename)
+            self.tokenizer = BertTokenizer.from_pretrained(
+                self.vocab_type_or_filename, do_basic_tokenize=do_basic_tokenize
+            )
 
     def tokenize(self, text: str) -> List[str]:
         return self.tokenizer.tokenize(text)
@@ -57,6 +60,3 @@ class TokenizerForBert(BaseTokenizer):
 
         self.msg_printer.good(f"Finished tokenizing text")
         return tokenized
-
-    def convert_tokens_to_ids(self, tokens: List[str]):
-        return self.tokenizer.convert_tokens_to_ids(tokens)
diff --git a/sciwing/tokenizers/character_tokenizer.py b/sciwing/tokenizers/character_tokenizer.py
index 1322723..1b832ab 100644
--- a/sciwing/tokenizers/character_tokenizer.py
+++ b/sciwing/tokenizers/character_tokenizer.py
@@ -1,5 +1,4 @@
 from typing import List
-
 from sciwing.tokenizers.BaseTokenizer import BaseTokenizer
 
 
diff --git a/sciwing/utils/common.py b/sciwing/utils/common.py
index 8d93750..7f12e78 100644
--- a/sciwing/utils/common.py
+++ b/sciwing/utils/common.py
@@ -1,4 +1,4 @@
-from typing import Dict, List, Any, Iterable, Iterator, Tuple
+from typing import Dict, List, Any, Iterable, Iterator
 
 import math
 import requests
@@ -15,6 +15,7 @@ import importlib
 from tqdm import tqdm
 import tarfile
 import psutil
+from sklearn.model_selection import StratifiedShuffleSplit
 
 PATHS = constants.PATHS
 FILES = constants.FILES
@@ -53,12 +54,11 @@ def convert_sectlabel_to_json(filename: str) -> Dict:
         for line in tqdm(fp, desc="Converting SectLabel File to JSON"):
             line = line.replace("\n", "")
 
-            # if the line is empty then the next line is the beginning of the
+            # if the line is empty then the next line is the beginning of the new file
             if not line:
                 file_count += 1
                 continue
 
-            # new file
             fields = line.split()
             line_content = fields[0]  # first column contains the content text
             line_content = line_content.replace(
@@ -79,6 +79,176 @@ def convert_sectlabel_to_json(filename: str) -> Dict:
     return output_json
 
 
+def convert_generic_sect_to_json(filename: str) -> Dict[str, Any]:
+    """ Converts the Generic sect data file into more readable json format
+
+        Parameters
+        ----------
+        filename : str
+            The sectlabel file name available at WING-NUS website
+
+        Returns
+        -------
+        Dict[str, Any]
+            text
+                The text of the line
+            label
+                The label of the file
+            file_no
+                A unique file number
+            line_count
+                A line count within the file
+
+    """
+    file_no = 1
+    line_no = 1
+    json_dict = {"generic_sect": []}
+    with open(filename) as fp:
+        for line in fp:
+            if bool(line.strip()):
+                match_obj = re.search("currHeader=(.*)", line.strip())
+                header_label = match_obj.groups()[0]
+                header, label = header_label.split(" ")
+                header = " ".join(header.split("-"))
+                line_no += 1
+
+                json_dict["generic_sect"].append(
+                    {
+                        "header": header,
+                        "label": label,
+                        "file_no": file_no,
+                        "line_no": line_no,
+                    }
+                )
+            else:
+                file_no += 1
+
+    return json_dict
+
+
+def convert_sectlabel_to_sciwing_clf_format(filename: str, out_dir: str):
+    """ Writes the file in the format required for sciwing text classification dataset
+    
+    Parameters
+    ----------
+    filename : str
+        The path of the sectlabel original format file.
+    out_dir : str
+        The path where the new files will be written
+
+    Returns
+    -------
+
+    """
+    texts = []
+    labels = []
+    with open(filename) as fp:
+        for line in tqdm(
+            fp, desc="Converting original sect label to Sciwing Classification format"
+        ):
+            line = line.replace("\n", "")
+
+            if not line:
+                continue
+
+            fields = line.split()
+            line_content = fields[0]
+            line_content = line_content.replace("|||", " ").strip()
+            label = fields[-1]
+            texts.append(line_content)
+            labels.append(label)
+
+    out_dir = pathlib.Path(out_dir)
+    train_filename = out_dir.joinpath("sectLabel.train")
+    dev_filename = out_dir.joinpath("sectLabel.dev")
+    test_filename = out_dir.joinpath("sectLabel.test")
+
+    # TODO: This wot be good for testing sectlabel.
+    #  You have to split the lines according to the files they come from.
+    #  If the lines are randomly split, then you will lose all the information such as the context of a line
+
+    (
+        (train_lines, train_labels),
+        (dev_lines, dev_labels),
+        (test_lines, test_labels),
+    ) = get_train_dev_test_stratified_split(lines=texts, labels=labels)
+
+    with open(train_filename, "w") as fp:
+        for text, label in zip(train_lines, train_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+    with open(dev_filename, "w") as fp:
+        for text, label in zip(dev_lines, dev_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+    with open(test_filename, "w") as fp:
+        for text, label in zip(test_lines, test_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+
+def convert_generic_sect_to_sciwing_clf_format(filename: str, out_dir: str):
+    """ Converts the generic sect original file to the sciwing classification format
+
+    Parameters
+    ----------
+    filename : str
+        The path of the file where the original generic section classification file is stored
+    out_dir : str
+        The output path where the train, dev and test files are written
+
+    Returns
+    -------
+    None
+
+    """
+    lines = []
+    labels = []
+    with open(filename) as fp:
+        for line in fp:
+            if bool(line.strip()):
+                match_obj = re.search("currHeader=(.*)", line.strip())
+                header_label = match_obj.groups()[0]
+                header, label = header_label.split(" ")
+                header = " ".join(header.split("-"))
+                lines.append(header)
+                labels.append(label)
+
+    out_dir = pathlib.Path(out_dir)
+    train_filename = out_dir.joinpath("genericSect.train")
+    dev_filename = out_dir.joinpath("genericSect.dev")
+    test_filename = out_dir.joinpath("genericSect.test")
+
+    (
+        (train_lines, train_labels),
+        (dev_lines, dev_labels),
+        (test_lines, test_labels),
+    ) = get_train_dev_test_stratified_split(lines=lines, labels=labels)
+
+    with open(train_filename, "w") as fp:
+        for text, label in zip(train_lines, train_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+    with open(dev_filename, "w") as fp:
+        for text, label in zip(dev_lines, dev_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+    with open(test_filename, "w") as fp:
+        for text, label in zip(test_lines, test_labels):
+            line = text + "###" + label
+            fp.write(line)
+            fp.write("\n")
+
+
 def merge_dictionaries_with_sum(a: Dict, b: Dict) -> Dict:
     # refer to https://stackoverflow.com/questions/11011756/is-there-any-pythonic-way-to-combine-two-dicts-adding-values-for-keys-that-appe?rq=1
     return dict(
@@ -213,55 +383,8 @@ def extract_tar(filename: str, destination_dir: str, mode="r"):
         msg_printer.fail("Couldnot extract {filename} to {destination}")
 
 
-def convert_generic_sect_to_json(filename: str) -> Dict[str, Any]:
-    """ Converts the Generic sect data file into more readable json format
-
-        Parameters
-        ----------
-        filename : str
-            The sectlabel file name available at WING-NUS website
-
-        Returns
-        -------
-        Dict[str, Any]
-            text
-                The text of the line
-            label
-                The label of the file
-            file_no
-                A unique file number
-            line_count
-                A line count within the file
-
-    """
-    file_no = 1
-    line_no = 1
-    json_dict = {"generic_sect": []}
-    with open(filename) as fp:
-        for line in fp:
-            if bool(line.strip()):
-                match_obj = re.search("currHeader=(.*)", line.strip())
-                header_label = match_obj.groups()[0]
-                header, label = header_label.split(" ")
-                header = " ".join(header.split("-"))
-                line_no += 1
-
-                json_dict["generic_sect"].append(
-                    {
-                        "header": header,
-                        "label": label,
-                        "file_no": file_no,
-                        "line_no": line_no,
-                    }
-                )
-            else:
-                file_no += 1
-
-    return json_dict
-
-
 def convert_parscit_to_conll(
-    parscit_train_filepath: pathlib.Path
+    parscit_train_filepath: pathlib.Path,
 ) -> List[Dict[str, Any]]:
     """ Convert the parscit data available at
     "https://github.com/knmnyn/ParsCit/blob/master/crfpp/traindata/parsCit.train.data"
@@ -305,6 +428,63 @@ def convert_parscit_to_conll(
     return output_list
 
 
+def convert_parscit_to_sciwing_seqlabel_format(
+    parscit_train_filepath: pathlib.Path, output_dir: str
+):
+    """ Convert the parscit data availabel at
+    "https://github.com/knmnyn/ParsCit/blob/master/crfpp/traindata/parsCit.train.data"
+    to the format required for sciwing seqential labelling
+
+    Parameters
+    ----------
+    parscit_train_filepath : pathlib.Path
+        The local path where the files are stored
+
+    output_dir: str
+        The output dir where the train dev and test file will be written
+
+    Returns
+    -------
+
+    """
+    conll_lines = convert_parscit_to_conll(pathlib.Path(parscit_train_filepath))
+    instances = []
+    for line in conll_lines:
+        word_tags = line["word_tags"]
+        line_ = []
+        for word_tag in word_tags:
+            word_tag_ = word_tag.split(" ")
+            word = word_tag_[0]
+            tag = word_tag_[-1]
+            word_tag_ = "###".join([word, tag])
+            line_.append(word_tag_)
+        instances.append(" ".join(line_))
+
+    # shuffle and split train dev and test
+    kf = KFold(n_splits=2, shuffle=True, random_state=1729)
+    len_citations = len(instances)
+    splits = kf.split(np.arange(len_citations))
+    splits = list(splits)
+    train_indices, test_indices = splits[0]
+
+    train_instances = [instances[train_idx] for train_idx in train_indices]
+    test_instances = [instances[test_idx] for test_idx in test_indices]
+
+    output_dir = pathlib.Path(output_dir)
+    train_filepath = output_dir.joinpath("parscit.train")
+    dev_filepath = output_dir.joinpath("parscit.dev")
+    test_filepath = output_dir.joinpath("parscit.test")
+
+    with open(train_filepath, "w") as fp:
+        fp.write("\n".join(train_instances))
+
+    with open(dev_filepath, "w") as fp:
+        fp.write("\n".join(test_instances))
+
+    with open(test_filepath, "w") as fp:
+        fp.write("\n".join(test_instances))
+
+
 def write_nfold_parscit_train_test(
     parscit_train_filepath: pathlib.Path,
     output_train_filepath: pathlib.Path,
@@ -463,3 +643,86 @@ def get_system_mem_in_gb():
     memory_size = psutil.virtual_memory().total
     memory_size = memory_size * 1e-9
     return memory_size
+
+
+def get_train_dev_test_stratified_split(
+    lines: List[str],
+    labels: List[str],
+    train_split: float = 0.8,
+    dev_split: float = 0.1,
+    test_split: float = 0.1,
+    random_state: int = 1729,
+) -> ((List[str], List[str]), (List[str], List[str]), (List[str], List[str])):
+    """ Slits the lines and labels into train, dev and test splits using stratified and
+    random shuffle
+
+    Parameters
+    ----------
+    lines: List[str]
+        A list of lines
+    labels: List[str]
+        A list of labels
+    train_split : float
+        The proportion of lines to be used for training
+    dev_split : float
+        The proportion of lines to be used for validation
+    test_split : float
+        The proportion of lines to be used for testing
+    random_state : int
+        The seed to be used for randomization. Good for reproducing the same splits
+        Passing None will cause the random number generator to be RandomState used by np.random
+
+    Returns
+    -------
+
+    """
+    len_lines = len(lines)
+    len_labels = len(labels)
+
+    assert len_lines == len_labels
+    train_test_splitter = StratifiedShuffleSplit(
+        n_splits=1,
+        test_size=dev_split + test_split,
+        train_size=train_split,
+        random_state=random_state,
+    )
+
+    splits = list(train_test_splitter.split(lines, labels))
+    train_indices, test_valid_indices = splits[0]
+
+    train_lines = [lines[idx] for idx in train_indices]
+    train_labels = [labels[idx] for idx in train_indices]
+
+    test_valid_lines = [lines[idx] for idx in test_valid_indices]
+    test_valid_labels = [labels[idx] for idx in test_valid_indices]
+
+    validation_size = dev_split / (test_split + dev_split)
+    validation_test_splitter = StratifiedShuffleSplit(
+        n_splits=1,
+        test_size=validation_size,
+        train_size=1 - validation_size,
+        random_state=random_state,
+    )
+
+    len_test_valid_lines = len(test_valid_lines)
+    len_test_valid_labels = len(test_valid_labels)
+
+    assert len_test_valid_labels == len_test_valid_lines
+
+    test_valid_splits = list(
+        validation_test_splitter.split(test_valid_lines, test_valid_labels)
+    )
+
+    test_indices, validation_indices = test_valid_splits[0]
+
+    test_lines = [test_valid_lines[idx] for idx in test_indices]
+    test_labels = [test_valid_labels[idx] for idx in test_indices]
+
+    validation_lines = [test_valid_lines[idx] for idx in validation_indices]
+    validation_labels = [test_valid_labels[idx] for idx in validation_indices]
+
+    return (
+        (train_lines, train_labels),
+        (validation_lines, validation_labels),
+        (test_lines, test_labels),
+    )
diff --git a/sciwing/utils/sciwing_toml_runner.py b/sciwing/utils/sciwing_toml_runner.py
index 4b6ff3b..75bc8a1 100644
--- a/sciwing/utils/sciwing_toml_runner.py
+++ b/sciwing/utils/sciwing_toml_runner.py
@@ -13,6 +13,11 @@ from typing import Dict
 import networkx as nx
 import copy
 import wasabi
+import pathlib
+import sciwing.constants as constants
+
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
 
 
 class SciWingTOMLRunner:
@@ -21,11 +26,11 @@ class SciWingTOMLRunner:
         self.infer = infer
         self.msg_printer = wasabi.Printer()
         self.doc = self._parse_toml_file()
+        self.data_dir = pathlib.Path(DATA_DIR)
 
         self.experiment_name = None
         self.experiment_dir = None
-        # Dict {'train': Dataset, 'valid': Dataset, 'test': Dataset}
-        self.all_datasets = None
+        self.datasets_manager = None
         self.model_section = None
         self.dataset_section = None
         self.engine_section = None
@@ -70,70 +75,72 @@ class SciWingTOMLRunner:
                 )
 
         # get the dataset section from toml
-        self.dataset_section = self.doc.get(f"dataset")
-        if self.dataset_section is None:
-            raise TOMLConfigurationError(
-                f"{self.toml_filename} does not have a datasets section. Please "
-                f"Provide a dataset section in your toml file"
-            )
-        else:
-            self.all_datasets = self.parse_dataset_section()
-            pass
+        self.datasets_manager = self.parse_dataset_section()
 
         # get the model section from toml
-        self.model_section = self.doc.get("model")
-        if self.model_section is None:
-            raise TOMLConfigurationError(
-                f"{self.toml_filename} does not have model section."
-                f"Please provide a model section to construct the model"
-            )
-        else:
-            self.model = self.parse_model_section()
+        self.model = self.parse_model_section()
 
         # get the engine section from toml
-        self.engine_section = self.doc.get("engine")
-        if self.engine_section is None:
-            raise TOMLConfigurationError(
-                f"{self.toml_filename} does not have an engine section"
-            )
-        else:
-            self.engine = self.parse_engine_section()
+        self.engine = self.parse_engine_section()
 
     def parse_dataset_section(self):
         """ Parse the dataset section of the toml file and instantiate the dataset
 
         Returns
         -------
-        Dict[str, Any]
-            The keys are ``[train, valid, test]`` with values being the
-            instantiations of the dataset mentioned in the toml filename
+        DatasetManager
+            The dataset manager for the experiment
         """
         dataset_section = self.doc.get("dataset")
-        all_datasets = {}
+        if dataset_section is None:
+            raise TOMLConfigurationError(
+                f"{self.toml_filename} does not have a datasets section. Please "
+                f"Provide a dataset section in your toml file"
+            )
 
+        self.dataset_section = dataset_section
         dataset_classname = dataset_section.get("class")
         if dataset_classname is None:
             raise TOMLConfigurationError(
                 f"Dataset section needs to have a name and class section"
             )
-        args = dataset_section.get("args")
-        for dataset_type in ["train", "valid", "test"]:
-            try:
-                dataset_cls = create_class(
-                    classname=dataset_classname,
-                    module_name=ClassNursery.class_nursery[dataset_classname],
-                )
-                args["dataset_type"] = dataset_type
-                args["filename"] = dataset_section[f"{dataset_type}_filename"]
-                dataset = dataset_cls(**args)
-                all_datasets[dataset_type] = dataset
-            except ModuleNotFoundError:
-                print(
-                    f"Module {ClassNursery.class_nursery[dataset_classname]} is not found"
-                )
-            except AttributeError:
-                print(f"Class {dataset_classname} is not found ")
-        return all_datasets
+
+        train_filename = dataset_section.get("train_filename")
+        dev_filename = dataset_section.get("dev_filename")
+        test_filename = dataset_section.get("test_filename")
+        train_filename = self.data_dir.joinpath(train_filename)
+        dev_filename = self.data_dir.joinpath(dev_filename)
+        test_filename = self.data_dir.joinpath(test_filename)
+        train_filename = str(train_filename)
+        dev_filename = str(dev_filename)
+        test_filename = str(test_filename)
+
+        tokenizers = dataset_section.get("tokenizers")
+        namespace_vocab_options = dataset_section.get("namespace_vocab_options")
+        namespace_numericalizer_map = dataset_section.get("namespace_numericalizer_map")
+
+        args = {
+            "train_filename": train_filename,
+            "dev_filename": dev_filename,
+            "test_filename": test_filename,
+            "tokenizers": tokenizers,
+            "namespace_vocab_options": namespace_vocab_options,
+            "namespace_numericalizer_map": namespace_numericalizer_map,
+        }
+        try:
+            dataset_cls = create_class(
+                classname=dataset_classname,
+                module_name=ClassNursery.class_nursery[dataset_classname],
+            )
+            dataset_manager = dataset_cls(**args)
+            self.msg_printer.good(title=f"Finished Creating {dataset_classname}")
+            return dataset_manager
+        except ModuleNotFoundError:
+            print(
+                f"Module {ClassNursery.class_nursery[dataset_classname]} is not found"
+            )
+        except AttributeError:
+            print(f"Class {dataset_classname} is not found ")
 
     def _form_dag(self, section_name: str, section: Dict, parent: str):
         """ Forms a DAG of the model section for execution
@@ -209,59 +216,39 @@ class SciWingTOMLRunner:
         root_nodename = topo_order[-1]
 
         for node_id in topo_order:
-
             node_data = self.model_dag.nodes[node_id]
             tag = node_data.get("tag", None)
             classname = node_data.pop("class", None)
             if classname is None:
                 raise TOMLConfigurationError(
-                    f"class is missing for one of the components of your model"
+                    f"Class {classname} is missing for one of the components of your model."
+                    f"Have you added the class into the ClassNursery?"
                 )
             class_args = copy.deepcopy(self.model_dag.nodes[node_id])
+
+            # models accept a datasets manager by default
+            # TODO: make this true for all the modules and models including embedders?
+            if tag == "model":
+                class_args["datasets_manager"] = self.datasets_manager
+
+            # instantiated class holds the object that is instantiated for
+            # the node
             class_args.pop("instantiated_class", None)
             current_node_tag = class_args.pop("tag", None)
+
             # leaf node
-            # we always assume that vanilla embedder is used at the lower level
+            # we always assume that embedders are  used at the lower level
             # This is a reasonable assumption to make
             if not list(self.model_dag.successors(node_id)):
-                if node_data.get("embed") == "word_vocab":
-                    embedding = self.all_datasets["train"].word_vocab.load_embedding()
-                    embedding_dim = self.all_datasets[
-                        "train"
-                    ].word_vocab.embedding_dimension
-                    freeze = node_data.get("freeze", False)
-                    embedding = nn.Embedding.from_pretrained(embedding, freeze=freeze)
-                    embedder = VanillaEmbedder(
-                        embedding_dim=embedding_dim, embedding=embedding
-                    )
-                    self.model_dag.nodes[node_id]["instantiated_class"] = {
-                        "key": tag,
-                        "object": embedder,
-                    }
-                elif node_data.get("embed") == "char_vocab":
-                    embedding = self.all_datasets["train"].char_vocab.load_embedding()
-                    embedding_dim = self.all_datasets[
-                        "train"
-                    ].char_vocab.embedding_dimension
-                    freeze = node_data.get("freeze", False)
-                    embedding = nn.Embedding.from_pretrained(embedding, freeze=freeze)
-                    embedder = VanillaEmbedder(
-                        embedding_dim=embedding_dim, embedding=embedding
-                    )
-                    self.model_dag.nodes[node_id]["instantiated_class"] = {
-                        "key": tag,
-                        "object": embedder,
-                    }
-                else:
-                    cls_obj = create_class(
-                        classname=classname,
-                        module_name=ClassNursery.class_nursery[classname],
-                    )
-                    cls_obj = cls_obj(**class_args)
-                    self.model_dag.nodes[node_id]["instantiated_class"] = {
-                        "key": tag,
-                        "object": cls_obj,
-                    }
+                cls_obj = create_class(
+                    classname=classname,
+                    module_name=ClassNursery.class_nursery[classname],
+                )
+                cls_obj = cls_obj(**class_args)
+                self.model_dag.nodes[node_id]["instantiated_class"] = {
+                    "key": tag,
+                    "object": cls_obj,
+                }
 
             # must have children that would have been instantiated
             else:
@@ -272,16 +259,19 @@ class SciWingTOMLRunner:
                 )
                 num_different_tags = len(different_tags)
 
+                # They are a lst of embedders
+                # They all have the same section name
+                # but different classes
                 if num_successors > 1 and num_different_tags == 1:
                     # pass through concat embedders
-                    embedders = []
+                    embedders_ = []
                     unique_tag = different_tags.pop()
                     for successor in successors:
                         node_data = self.model_dag.nodes[successor]
                         embedder = node_data["instantiated_class"]["object"]
-                        embedders.append(embedder)
+                        embedders_.append(embedder)
 
-                    embedder = ConcatEmbedders(embedders=embedders)
+                    embedder = ConcatEmbedders(embedders=embedders_)
 
                     # instantiate the current node here
                     class_args[unique_tag] = embedder
@@ -326,8 +316,14 @@ class SciWingTOMLRunner:
             nn.Module
                 A torch module representing the model
             """
+        model_section = self.doc.get("model")
+        if model_section is None:
+            raise TOMLConfigurationError(
+                f"{self.toml_filename} does not have model section."
+                f"Please provide a model section to construct the model"
+            )
+        self.model_section = model_section
         with self.msg_printer.loading("Loading Model from file"):
-            model_section = self.doc.get("model")
             self._form_dag(section_name="model", section=model_section, parent=None)
             # it has to be a DAG (no cycles please!)
             assert nx.dag.is_directed_acyclic_graph(self.model_dag)
@@ -347,6 +343,11 @@ class SciWingTOMLRunner:
 
         """
         engine_section = self.doc.get("engine")
+        if engine_section is None:
+            raise TOMLConfigurationError(
+                f"{self.toml_filename} does not have an engine section"
+            )
+        self.engine_section = engine_section
         engine_args = {}
         for key, value in engine_section.items():
             if not isinstance(value, dict):
@@ -375,7 +376,9 @@ class SciWingTOMLRunner:
 
         metric_section = engine_section.get("metric")
         metric_classname = metric_section.get("class")
-        metric_args = {}
+
+        # parse any other arguments for metric
+        metric_args = {"datasets_manager": self.datasets_manager}
         for key, value in metric_section.items():
             if key == "class":
                 pass
@@ -386,15 +389,13 @@ class SciWingTOMLRunner:
             module_name=ClassNursery.class_nursery[metric_classname],
             classname=metric_classname,
         )
-        metric = metric_cls(**metric_args)
-        engine_args["metric"] = metric
-
-        train_dataset = self.all_datasets["train"]
-        valid_dataset = self.all_datasets["valid"]
-        test_dataset = self.all_datasets["test"]
-        engine_args["train_dataset"] = train_dataset
-        engine_args["validation_dataset"] = valid_dataset
-        engine_args["test_dataset"] = test_dataset
+        train_metric = metric_cls(**metric_args)
+        dev_metric = metric_cls(**metric_args)
+        test_metric = metric_cls(**metric_args)
+        engine_args["train_metric"] = train_metric
+        engine_args["validation_metric"] = dev_metric
+        engine_args["test_metric"] = test_metric
+        engine_args["datasets_manager"] = self.datasets_manager
         engine_args["model"] = self.model
         engine_args["experiment_name"] = self.experiment_name
         engine_args["experiment_hyperparams"] = self.doc
@@ -408,13 +409,3 @@ class SciWingTOMLRunner:
     def run(self):
         self.parse()
         self.engine.run()
-
-
-if __name__ == "__main__":
-    import sciwing.constants as constants
-
-    PATHS = constants.PATHS
-    CONFIGS_DIR = PATHS["CONFIGS_DIR"]
-    bow_random_parsect_toml = pathlib.Path(CONFIGS_DIR, "bow_random_gensect.toml")
-    toml_parser = SciWingTOMLRunner(toml_filename=bow_random_parsect_toml)
-    toml_parser.run()
diff --git a/sciwing/vocab/embedding_loader.py b/sciwing/vocab/embedding_loader.py
index 5e3b2b5..900771f 100644
--- a/sciwing/vocab/embedding_loader.py
+++ b/sciwing/vocab/embedding_loader.py
@@ -5,6 +5,7 @@ import numpy as np
 from tqdm import tqdm
 from wasabi import Printer
 import gensim
+from sciwing.vocab.vocab import Vocab
 
 
 PATHS = constants.PATHS
@@ -18,49 +19,38 @@ class EmbeddingLoader:
 
     """
 
-    def __init__(
-        self,
-        token2idx: Dict,
-        embedding_type: Union[str, None] = None,
-        embedding_dimension: Union[str, None] = None,
-    ):
+    def __init__(self, embedding_type: Union[str] = "glove_6B_50"):
         """
 
-        :param token2idx: type: Dict
-        The mapping between token2idx
-        :param embedding_type: type: Union[str, None]
+        Parameters
+        ----------
+        embedding_type : str
+            The type of embedding that needs to be loaded
         """
-        self.token2idx_mapping = token2idx
-        self.embedding_type = "random" if embedding_type is None else embedding_type
-        self.embedding_dimension = embedding_dimension
+        self.embedding_dimension = None
+        self.embedding_type = "glove_" if embedding_type is None else embedding_type
 
         self.allowed_embedding_types = [
             "glove_6B_50",
             "glove_6B_100",
             "glove_6B_200",
             "glove_6B_300",
-            "random",
             "parscit",
         ]
 
         assert (
             self.embedding_type in self.allowed_embedding_types
-        ), "You can use one of {0} for embedding type".format(
-            self.allowed_embedding_types
-        )
-
+        ), f"You can use one of {self.allowed_embedding_types} for embedding type"
         self.embedding_filename = self.get_preloaded_filename()
         self.vocab_embedding = {}  # stores the embedding for all words in vocab
         self.msg_printer = Printer()
-
-        if "random" in self.embedding_type:
-            self.vocab_embedding = self.load_random_embedding()
+        self._embeddings: Dict[str, np.array] = {}
 
         if "glove" in self.embedding_type:
-            self.vocab_embedding = self.load_glove_embedding()
+            self._embeddings = self.load_glove_embedding()
 
         if "parscit" in self.embedding_type:
-            self.vocab_embedding = self.load_parscit_embedding()
+            self._embeddings = self.load_parscit_embedding()
 
     def get_preloaded_filename(self):
         filename = None
@@ -87,13 +77,10 @@ class EmbeddingLoader:
         Loads the word embedding for words in the vocabulary
         If the word in the vocabulary doesnot have an embedding
         then it is loaded with zeros
-        TODO: Load only once in the project and store it in json file
-            - Read from json file at once
-            - This might be memory expensive and save a little bit of time
-        :return:
         """
         embedding_dim = int(self.embedding_type.split("_")[-1])
-        glove_embeddings = {}
+        self.embedding_dimension = embedding_dim
+        glove_embeddings: Dict[str, np.array] = {}
         with self.msg_printer.loading("Loading GLOVE embeddings"):
             with open(self.embedding_filename, "r") as fp:
                 for line in tqdm(
@@ -105,44 +92,20 @@ class EmbeddingLoader:
                     embedding = np.array([float(value) for value in values[1:]])
                     glove_embeddings[word] = embedding
 
-            tokens = self.token2idx_mapping.keys()
-
-            vocab_embeddings = {}
-
-            for token in tokens:
-                try:
-                    emb = glove_embeddings[token]
-                except KeyError:
-                    emb = np.zeros(embedding_dim)
-
-                vocab_embeddings[token] = emb
-
-        self.msg_printer.good(f"Loaded Glove embeddings - {self.embedding_type}")
-        return vocab_embeddings
+        return glove_embeddings
 
-    def load_random_embedding(self) -> Dict[str, np.array]:
-        tokens = self.token2idx_mapping.keys()
-
-        vocab_embeddings = {}
+    def load_parscit_embedding(self) -> Dict[str, np.array]:
+        pretrained = gensim.models.KeyedVectors.load(self.embedding_filename, mmap="r")
+        self.embedding_dimension = 500
+        return pretrained
 
-        for token in tokens:
-            emb = np.random.normal(loc=-0.1, scale=0.1, size=self.embedding_dimension)
-            vocab_embeddings[token] = emb
+    def get_embeddings_for_vocab(self, vocab: Vocab):
+        pass
 
-        self.msg_printer.good("Finished loading Random word Embedding")
-        return vocab_embeddings
+    @property
+    def embeddings(self):
+        return self._embeddings
 
-    def load_parscit_embedding(self) -> Dict[str, np.array]:
-        pretrained = gensim.models.KeyedVectors.load(self.embedding_filename, mmap="r")
-        tokens = self.token2idx_mapping.keys()
-        vocab_embeddings = {}
-
-        for token in tokens:
-            try:
-                emb = pretrained[token]
-            except:
-                emb = pretrained["<UNK>"]
-            vocab_embeddings[token] = emb
-
-        self.msg_printer.good("Finished Loading Parscit Embeddings")
-        return vocab_embeddings
+    @embeddings.setter
+    def embeddings(self, value):
+        self._embeddings = value
diff --git a/sciwing/vocab/vocab.py b/sciwing/vocab/vocab.py
index 341e24a..5a751b4 100644
--- a/sciwing/vocab/vocab.py
+++ b/sciwing/vocab/vocab.py
@@ -1,4 +1,4 @@
-from typing import List, Dict, Tuple, Any, Optional
+from typing import List, Dict, Tuple, Optional
 from collections import Counter
 from operator import itemgetter
 import json
@@ -6,10 +6,6 @@ import os
 from wasabi import Printer
 import wasabi
 from copy import deepcopy
-from sciwing.vocab.embedding_loader import EmbeddingLoader
-from sciwing.vocab.char_emb_loader import CharEmbLoader
-import torch
-from typing import Union
 
 
 class Vocab:
@@ -24,40 +20,51 @@ class Vocab:
         end_token: str = "<EOS>",
         special_token_freq: float = 1e10,
         store_location: str = None,
-        embedding_type: Union[str, None] = None,
-        embedding_dimension: Union[int, None] = None,
+        max_instance_length: int = 100,
+        include_special_vocab: bool = True,
     ):
         """
 
-        :param instances: type: List[List[str]]
-         Pass in the list of tokenized instances from which vocab is built
-        :param max_num_tokens: type: int
-        The top `max_num_words` frequent words will be considered for
-        vocabulary and the rest of them will be mapped to `unk_token`
-        :param min_count: type: int
-        All words that do not have min count will be mapped to `unk_token`
-        :param unk_token: str
-        This token will be used for unknown words
-        :param pad_token: type: str
-        This token will be used for <PAD> words
-        :param start_token: type: str
-        This token will be used for start of sentence indicator
-        :param end_token: type: str
-        This token will be used for end of sentence indicator
-        :param special_token_freq: type: float
-        special tokens should have high frequency.
-        The higher the frequency, the more common they are
-        :param store_location: type: str
-        The users can provide a store location optionally.
-        The vocab will be stored in the location
-        If the file exists then, the vocab will be restored from the file, rather than building it.
-        :param embedding_type: type: str
-        The embedding type is the type of pre-trained embedding that will be loaded
-        for all the words in the vocab optionally. You can refer to `WordEmbLoder`
-        for all the available embedding types
-        :param embedding_dimension: type: int
-        Embedding dimension of the embedding type
+        Parameters
+        ----------
+        instances : Optional[List[List[str]]]
+            A list of tokenized instances
+        max_num_tokens : int
+            The maximum number of tokens to be used in the vocab
+            All the other tokens above this number will be replaced
+            by UNK.
+            If this is not passed then the maximum possible number
+            will be used
+        min_count : int
+            All words that do not have min count will be mapped to `unk_token`
+        unk_token : str
+            This token will be used for unknown words
+        pad_token : str
+            This token will be used for <PAD> words
+        start_token : str
+            This token will be used for start of line indicator
+        end_token : str
+            This token will be used for end of sentence indicator
+        special_token_freq : float
+            special tokens should have high frequency.
+        store_location : str
+            The users can provide a store location optionally.
+            The vocab will be stored in the location
+            If the file exists then, the vocab will be restored from the file, rather than building it.
+        max_instance_length : int
+            Every vocab is related to a namespace. Every instance
+            in that namespace will be clipped or padded to this
+            length
+        include_special_vocab : bool
+            Boolean value to indicate whether special vocab should be included or no
+            If this is false, you will have to set add_start_end_token to False
+            and you cannot pad your instances. This is mostly set for labels -
+            such as for classification that require no padding. For such
+            cases please make sure that min_count is always 1 and max_num_tokens
+            is always None. Otherwise some of the labels will be missed and it
+            might result in error
         """
+
         self.instances = instances
         self.max_num_tokens = max_num_tokens
         self.min_count = min_count
@@ -71,18 +78,31 @@ class Vocab:
         self.idx2token = None
         self.token2idx = None
         self.store_location = store_location
-        self.embedding_type = embedding_type
-        self.embedding_dimension = embedding_dimension
+        self.max_instance_length = max_instance_length
+        self.include_special_vocab = include_special_vocab
 
         self.msg_printer = Printer()
 
         # store the special tokens
-        self.special_vocab = {
-            self.unk_token: (self.special_token_freq + 3, 0),
-            self.pad_token: (self.special_token_freq + 2, 1),
-            self.start_token: (self.special_token_freq + 1, 2),
-            self.end_token: (self.special_token_freq, 3),
-        }
+        if self.include_special_vocab:
+            self.special_vocab = {
+                self.unk_token: (self.special_token_freq + 3, 0),
+                self.pad_token: (self.special_token_freq + 2, 1),
+                self.start_token: (self.special_token_freq + 1, 2),
+                self.end_token: (self.special_token_freq, 3),
+            }
+        else:
+            if self.min_count != 1:
+                self.msg_printer.warn(
+                    "Warning: You are building vocab without special vocab. "
+                    "Please make sure that min_count is 1"
+                )
+            if self.max_num_tokens is not None:
+                self.msg_printer.warn(
+                    "You are building vocab without special vocab. Please make "
+                    "sure that max_num_tokens is None"
+                )
+            self.special_vocab = {}
 
     def map_tokens_to_freq_idx(self) -> Dict[str, Tuple[int, int]]:
         """
@@ -142,9 +162,15 @@ class Vocab:
         Clip the vocab based on the maximum number of words
         We return `max_num_words + len(self.special_vocab)` words effectively
         The rest of them will be mapped to `self.unk_token`
-        :param vocab: type: Dict[str, Tuple[int, int]]
-        :return: vocab: type: Dict[str, Tuple[int, int]]
-        The new vocab
+        Parameters
+        ----------
+        vocab : Dict[str, Tuple[int, int]]
+            The mapping from token to idx and frequency
+        Returns
+        -------
+        Dict[str, Tuple[int, int]]
+            The new vocab
+
         """
         for key, (freq, idx) in vocab.items():
             if idx >= len(self.special_vocab) + self.max_num_tokens:
@@ -202,9 +228,14 @@ class Vocab:
         else:
             self.msg_printer.info("BUILDING VOCAB")
             vocab = self.map_tokens_to_freq_idx()
-            self.orig_vocab = deepcopy(
-                vocab
-            )  # dictionary are passed by reference. Be careful
+
+            # dictionary are passed by reference. Be careful
+            self.orig_vocab = deepcopy(vocab)
+
+            # set max num of tokens to maximum possible if it is not set
+            if self.max_num_tokens is None:
+                self.max_num_tokens = len(self.orig_vocab.keys())
+
             vocab = self.clip_on_mincount(vocab)
             vocab = self.clip_on_max_num(vocab)
             self.vocab = vocab
@@ -272,8 +303,6 @@ class Vocab:
             "start_token": self.start_token,
             "end_token": self.end_token,
             "special_token_freq": self.special_token_freq,
-            "embedding_type": self.embedding_type,
-            "embedding_dimension": self.embedding_dimension,
             "special_vocab": self.special_vocab,
         }
         vocab_state["vocab"] = self.vocab
@@ -308,8 +337,6 @@ class Vocab:
                 end_token = vocab_options["end_token"]
                 special_token_freq = vocab_options["special_token_freq"]
                 store_location = filename
-                embedding_type = vocab_options["embedding_type"]
-                embedding_dimension = vocab_options["embedding_dimension"]
                 vocab = cls(
                     max_num_tokens=max_num_tokens,
                     min_count=min_count,
@@ -320,8 +347,6 @@ class Vocab:
                     instances=None,
                     special_token_freq=special_token_freq,
                     store_location=store_location,
-                    embedding_type=embedding_type,
-                    embedding_dimension=embedding_dimension,
                 )
 
                 # instead of building the vocab, set the vocab from vocab_dict
@@ -347,21 +372,18 @@ class Vocab:
         if not self.idx2token:
             self.idx2token = self.get_idx2token_mapping()
 
-        try:
-            if idx == self.special_vocab[self.unk_token][1]:
-                return self.unk_token
-            else:
-                token = self.idx2token[idx]
-                return token
-        except KeyError:
-            vocab_len = self.get_vocab_len()
+        vocab_len = self.get_vocab_len()
+
+        if idx > vocab_len - 1:
             message = (
-                "You tried to access idx {0} of the vocab "
-                "The length of the vocab is {1}. Please Provide "
-                "Number between {2}".format(idx, vocab_len, vocab_len - 1)
+                f"You tried to access idx {idx} of the vocab The length of the vocab is "
+                f"{vocab_len}. Please Provide Number between 0 and {vocab_len - 1}"
             )
             raise ValueError(message)
 
+        token = self.idx2token.get(idx)
+        return token
+
     def get_idx_from_token(self, token: str) -> int:
         if not self.vocab:
             raise ValueError("Please build the vocab first")
@@ -372,7 +394,7 @@ class Vocab:
         try:
             return self.token2idx[token]
         except KeyError:
-            return self.token2idx[self.unk_token]
+            return self.token2idx.get(self.unk_token, None)
 
     def get_topn_frequent_words(self, n: int = 5) -> List[Tuple[str, int]]:
         idx2token = self.idx2token
@@ -401,29 +423,6 @@ class Vocab:
         self.msg_printer.divider("VOCAB STATS")
         print(table_string)
 
-    def load_embedding(self) -> torch.FloatTensor:
-        if not self.vocab:
-            raise ValueError("Please build the vocab first")
-
-        embedding_loader = EmbeddingLoader(
-            token2idx=self.token2idx,
-            embedding_type=self.embedding_type,
-            embedding_dimension=self.embedding_dimension,
-        )
-
-        indices = [key for key in self.idx2token.keys()]
-        indices = sorted(indices)
-
-        embeddings = []
-        for idx in indices:
-            token = self.idx2token[idx]
-            # numpy array appends to the embeddings array
-            embedding = embedding_loader.vocab_embedding[token]
-            embeddings.append(embedding)
-
-        embeddings = torch.FloatTensor(embeddings)
-        return embeddings
-
     def set_vocab(self, vocab: Dict[str, Tuple[int, int]]):
         self.vocab = vocab
 
@@ -449,10 +448,13 @@ class Vocab:
         str
             A string representing the index
         """
-        pad_token_index = self.get_idx_from_token(self.pad_token)
-        start_token_index = self.get_idx_from_token(self.start_token)
-        end_token_index = self.get_idx_from_token(self.end_token)
-        special_indices = [pad_token_index, start_token_index, end_token_index]
+        if self.special_vocab:
+            pad_token_index = self.get_idx_from_token(self.pad_token)
+            start_token_index = self.get_idx_from_token(self.start_token)
+            end_token_index = self.get_idx_from_token(self.end_token)
+            special_indices = [pad_token_index, start_token_index, end_token_index]
+        else:
+            special_indices = []
 
         token = [
             self.get_token_from_idx(idx)
@@ -461,3 +463,19 @@ class Vocab:
         ]
         sentence = " ".join(token)
         return sentence
+
+    @property
+    def token2idx(self):
+        return self._token2idx
+
+    @token2idx.setter
+    def token2idx(self, value):
+        self._token2idx = value
+
+    @property
+    def idx2token(self):
+        return self._idx2token
+
+    @idx2token.setter
+    def idx2token(self, value):
+        self._idx2token = value
diff --git a/tests/data/test_dataset_manager.py b/tests/data/test_dataset_manager.py
new file mode 100644
index 0000000..21b652c
--- /dev/null
+++ b/tests/data/test_dataset_manager.py
@@ -0,0 +1,64 @@
+import pytest
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.utils.class_nursery import ClassNursery
+
+
+@pytest.fixture(scope="session")
+def clf_dataset_manager(tmpdir_factory, request):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
+
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
+
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("dev_line1###label1\ndev_line2###label2")
+
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+    )
+
+    return clf_dataset_manager
+
+
+class TestDatasetManager:
+    def test_namespaces(self, clf_dataset_manager):
+        namespaces = clf_dataset_manager.namespaces
+        assert set(namespaces) == {"tokens", "char_tokens", "label"}
+
+    def test_namespace_to_vocab(self, clf_dataset_manager):
+        namespace_to_vocab = clf_dataset_manager.namespace_to_vocab
+        assert namespace_to_vocab["tokens"].get_vocab_len() == 2 + 4
+        # there is no special vocab here
+        assert namespace_to_vocab["label"].get_vocab_len() == 2
+
+    def test_namespace_to_numericalizers(self, clf_dataset_manager):
+        namespace_to_numericalizer = clf_dataset_manager.namespace_to_numericalizer
+        assert set(namespace_to_numericalizer.keys()) == {
+            "tokens",
+            "char_tokens",
+            "label",
+        }
+
+    def test_label_namespace(self, clf_dataset_manager):
+        label_namespaces = clf_dataset_manager.label_namespaces
+        assert label_namespaces == ["label"]
+
+    def test_num_labels(self, clf_dataset_manager):
+        num_labels = clf_dataset_manager.num_labels["label"]
+        assert num_labels == 2
+
+    def test_print_stats(self, clf_dataset_manager):
+        try:
+            clf_dataset_manager.print_stats()
+        except:
+            pytest.fail(f"Print Stats fail to work in datasets manager")
+
+    def test_texclassification_dataset_manager_in_nursery(self, clf_dataset_manager):
+        assert (
+            ClassNursery.class_nursery["TextClassificationDatasetManager"] is not None
+        )
diff --git a/tests/data/test_label.py b/tests/data/test_label.py
new file mode 100644
index 0000000..e2105ce
--- /dev/null
+++ b/tests/data/test_label.py
@@ -0,0 +1,10 @@
+import pytest
+from sciwing.data.label import Label
+
+
+class TestLabel:
+    def test_label_str_getter(self):
+        label = Label(text="Introduction", namespace="label")
+        tokens = label.tokens["label"]
+        token_text = tokens[0].text
+        assert token_text == "Introduction"
diff --git a/tests/data/test_line.py b/tests/data/test_line.py
new file mode 100644
index 0000000..576e518
--- /dev/null
+++ b/tests/data/test_line.py
@@ -0,0 +1,39 @@
+import pytest
+from sciwing.data.line import Line
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
+
+
+class TestLine:
+    def test_line_word_tokenizers(self):
+        text = "This is a single line"
+        line = Line(text=text, tokenizers={"tokens": WordTokenizer()})
+        tokens = line.tokens
+        assert [token.text for token in tokens["tokens"]] == [
+            "This",
+            "is",
+            "a",
+            "single",
+            "line",
+        ]
+
+    def test_line_char_tokenizer(self):
+        text = "Word"
+        line = Line(
+            text=text,
+            tokenizers={"tokens": WordTokenizer(), "chars": CharacterTokenizer()},
+        )
+        tokens = line.tokens
+        word_tokens = tokens["tokens"]
+        char_tokens = tokens["chars"]
+
+        word_tokens = [tok.text for tok in word_tokens]
+        char_tokens = [tok.text for tok in char_tokens]
+
+        assert word_tokens == ["Word"]
+        assert char_tokens == ["W", "o", "r", "d"]
+
+    def test_line_namespaces(self):
+        text = "Single line"
+        line = Line(text=text, tokenizers={"tokens": WordTokenizer()})
+        assert line.namespaces == ["tokens"]
diff --git a/tests/data/test_seq_label.py b/tests/data/test_seq_label.py
new file mode 100644
index 0000000..0b31eba
--- /dev/null
+++ b/tests/data/test_seq_label.py
@@ -0,0 +1,18 @@
+from sciwing.data.seq_label import SeqLabel
+import pytest
+
+
+@pytest.fixture
+def setup_seq_labels():
+    labels = ["B-PER", "L-PER"]
+    label = SeqLabel(labels=labels)
+    return label
+
+
+class TestSeqLabel:
+    def test_labels_set(self, setup_seq_labels):
+        label = setup_seq_labels
+        tokens = label.tokens["seq_label"]
+        for token in tokens:
+            assert len(token.text) > 1
+            assert isinstance(token.text, str)
diff --git a/tests/data/test_token.py b/tests/data/test_token.py
new file mode 100644
index 0000000..431451a
--- /dev/null
+++ b/tests/data/test_token.py
@@ -0,0 +1,24 @@
+import pytest
+from sciwing.data.token import Token
+import numpy as np
+
+
+class TestToken:
+    def test_token_initialization(self):
+        token = Token("token")
+        assert token.text == "token"
+
+    def test_token_len(self):
+        token = Token("token")
+        assert token.len == 5
+
+    @pytest.mark.parametrize(
+        "embedding_type, embedding",
+        [("glove", np.random.rand(100)), ("bert", np.random.rand(1000))],
+    )
+    def test_token_set_embedding(self, embedding_type, embedding):
+        token = Token("token")
+        try:
+            token.set_embedding(name=embedding_type, value=embedding)
+        except:
+            pytest.fail("setting the embedding failed")
diff --git a/tests/datasets/classification/test_generic_sect_dataset.py b/tests/datasets/classification/test_generic_sect_dataset.py
index 13b1e36..c0a5ce1 100644
--- a/tests/datasets/classification/test_generic_sect_dataset.py
+++ b/tests/datasets/classification/test_generic_sect_dataset.py
@@ -1,111 +1,57 @@
-from sciwing.datasets.classification.generic_sect_dataset import GenericSectDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
 import pytest
 import sciwing.constants as constants
-from torch.utils.data import DataLoader
-from sciwing.utils.class_nursery import ClassNursery
+import pathlib
 
 FILES = constants.FILES
+PATHS = constants.PATHS
+DATA_DIR = PATHS["DATA_DIR"]
 GENERIC_SECTION_TRAIN_FILE = FILES["GENERIC_SECTION_TRAIN_FILE"]
 
 
-@pytest.fixture(scope="session", params=["train", "valid", "test"])
-def setup_generic_sect_dataset(tmpdir_factory, request):
-    vocab_store_location = tmpdir_factory.mktemp("tempdir").join("vocab.json")
-    DEBUG = True
-    MAX_NUM_WORDS = 100
-    MAX_LENGTH = 5
-    DEBUG_DATASET_PROPORTION = 0.1
-    EMBEDDING_TYPE = "random"
-    EMBEDDING_DIMENSION = 300
-    dataset_type = request.param
+@pytest.fixture(scope="module")
+def setup_generic_sect_dataset_manager():
+    data_dir = pathlib.Path(DATA_DIR)
+    sect_label_train_file = data_dir.joinpath("genericSect.train")
+    sect_label_dev_file = data_dir.joinpath("genericSect.dev")
+    sect_label_test_file = data_dir.joinpath("genericSect.test")
 
-    dataset = GenericSectDataset(
-        filename=GENERIC_SECTION_TRAIN_FILE,
-        dataset_type=dataset_type,
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=EMBEDDING_TYPE,
-        word_embedding_dimension=EMBEDDING_DIMENSION,
+    dataset_manager = TextClassificationDatasetManager(
+        train_filename=sect_label_train_file,
+        dev_filename=sect_label_dev_file,
+        test_filename=sect_label_test_file,
     )
 
-    return (
-        dataset,
-        {
-            "MAX_NUM_WORDS": MAX_NUM_WORDS,
-            "MAX_LENGTH": MAX_LENGTH,
-            "DEBUG": DEBUG,
-            "DEBUG_DATASET_PROPORTION": DEBUG_DATASET_PROPORTION,
-            "EMBEDDING_TYPE": EMBEDDING_TYPE,
-            "EMBEDDING_DIMENSION": EMBEDDING_DIMENSION,
-        },
-    )
+    return dataset_manager
 
 
 class TestGenericSectDataset:
-    def test_train_validation_test_split_numbers(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        headers = ["a"] * 100
-        labels = [dataset.idx2classname[0]] * 100
-        (train_headers, train_labels), (valid_headers, valid_labels), (
-            test_headers,
-            test_labels,
-        ) = dataset.get_train_valid_test_stratified_split(
-            headers, labels, dataset.classname2idx
-        )
-
-        assert len(train_headers) == 80
-        assert len(train_labels) == 80
-        assert len(valid_headers) == 10
-        assert len(valid_labels) == 10
-        assert len(test_headers) == 10
-        assert len(test_labels) == 10
-
-    def test_num_labels(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        classname2idx = dataset.classname2idx
-        num_labels = len(list(classname2idx.keys()))
+    def test_num_labels(self, setup_generic_sect_dataset_manager):
+        dataset_manager = setup_generic_sect_dataset_manager
+        train_dataset = dataset_manager.train_dataset
+        lines, labels = train_dataset.get_lines_labels()
+        labels = [label.text for label in labels]
+        num_labels = len(set(labels))
         assert num_labels == 12
 
-    def test_no_train_header_empty(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        headers, labels = dataset.get_lines_labels(GENERIC_SECTION_TRAIN_FILE)
-        assert all([bool(header.strip()) for header in headers])
-
-    def test_no_train_label_empty(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        headers, labels = dataset.get_lines_labels(filename=GENERIC_SECTION_TRAIN_FILE)
-        assert all([bool(label.strip()) for label in labels])
-
-    def test_embedding_has_values(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        embedding_tensors = dataset.word_vocab.load_embedding()
-        assert embedding_tensors.size(0) > 0
-        assert embedding_tensors.size(1) == options["EMBEDDING_DIMENSION"]
-
-    def test_train_loader(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        loader = DataLoader(dataset=dataset, batch_size=3)
-        instances_dict = next(iter(loader))
-        assert len(instances_dict["instance"]) == 3
-        assert instances_dict["tokens"].size(0) == 3
-        assert instances_dict["tokens"].size(1) == options["MAX_LENGTH"]
-        assert instances_dict["label"].size(0) == 3
-
-    def test_iter_dict_should_have_tokens_labels(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        iter_dict = dataset[0]
-        assert "label" in iter_dict.keys()
-        assert "tokens" in iter_dict.keys()
-
-    def test_get_stats_works(self, setup_generic_sect_dataset):
-        dataset, options = setup_generic_sect_dataset
-        try:
-            dataset.print_stats()
-        except:
-            pytest.fail("print_stats() of GenericSect Dataset fails")
-
-    def test_generic_sect_in_nursery(self):
-        assert ClassNursery.class_nursery.get("GenericSectDataset") is not None
+    def test_no_line_empty(self, setup_generic_sect_dataset_manager):
+        dataset_manager = setup_generic_sect_dataset_manager
+        for dataset in [
+            dataset_manager.train_dataset,
+            dataset_manager.dev_dataset,
+            dataset_manager.test_dataset,
+        ]:
+            lines, labels = dataset.get_lines_labels()
+            assert all([bool(line.text.strip()) for line in lines])
+
+    def test_no_train_label_empty(self, setup_generic_sect_dataset_manager):
+        dataset_manager = setup_generic_sect_dataset_manager
+        for dataset in [
+            dataset_manager.train_dataset,
+            dataset_manager.dev_dataset,
+            dataset_manager.test_dataset,
+        ]:
+            lines, labels = dataset.get_lines_labels()
+            assert all([bool(label.text.strip()) for label in labels])
diff --git a/tests/datasets/classification/test_sectlabel_dataset.py b/tests/datasets/classification/test_sectlabel_dataset.py
index 513029c..7c2d0c3 100644
--- a/tests/datasets/classification/test_sectlabel_dataset.py
+++ b/tests/datasets/classification/test_sectlabel_dataset.py
@@ -1,139 +1,55 @@
 import sciwing.constants as constants
 import pytest
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-import torch
-from torch.utils.data import DataLoader
-from sciwing.utils.class_nursery import ClassNursery
-
-FILES = constants.FILES
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-
-
-@pytest.fixture(scope="session", params=["vanilla", "spacy"])
-def setup_parsect_train_dataset(tmpdir_factory, request):
-    MAX_NUM_WORDS = 1000
-    MAX_LENGTH = 10
-    vocab_store_location = tmpdir_factory.mktemp("tempdir").join("vocab.json")
-    DEBUG = True
-    tokenization_type = request.param
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        train_size=0.8,
-        test_size=0.2,
-        validation_size=0.5,
-        word_tokenization_type=tokenization_type,
-    )
-
-    return (
-        train_dataset,
-        {
-            "MAX_NUM_WORDS": MAX_NUM_WORDS,
-            "MAX_LENGTH": MAX_LENGTH,
-            "vocab_store_location": vocab_store_location,
-        },
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import pathlib
+
+FILES = constants.PATHS
+DATA_DIR = FILES["DATA_DIR"]
+
+
+@pytest.fixture(scope="module")
+def setup_sectlabel_dataset_manager():
+    data_dir = pathlib.Path(DATA_DIR)
+    sect_label_train_file = data_dir.joinpath("sectLabel.train")
+    sect_label_dev_file = data_dir.joinpath("sectLabel.dev")
+    sect_label_test_file = data_dir.joinpath("sectLabel.test")
+
+    dataset_manager = TextClassificationDatasetManager(
+        train_filename=sect_label_train_file,
+        dev_filename=sect_label_dev_file,
+        test_filename=sect_label_test_file,
     )
 
-
-class TestParsectDataset:
-    def test_label_mapping_len(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        label_mapping = train_dataset.get_classname2idx()
-        assert len(label_mapping) == 23
-
-    def test_no_line_empty(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        lines, labels = train_dataset.get_lines_labels(filename=SECT_LABEL_FILE)
-        assert all([bool(line.strip()) for line in lines])
-
-    def test_no_label_empty(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        lines, labels = train_dataset.get_lines_labels(filename=SECT_LABEL_FILE)
-        assert all([bool(label.strip()) for label in labels])
-
-    def test_tokens_max_length(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        lines, labels = train_dataset.get_lines_labels(filename=SECT_LABEL_FILE)
-        num_lines = len(lines)
-        for idx in range(num_lines):
-            assert len(train_dataset[idx]["tokens"]) == dataset_options["MAX_LENGTH"]
-
-    def test_get_class_names_from_indices(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        instance_dict = next(iter(train_dataset))
-        labels = instance_dict["label"]
-        labels_list = labels.tolist()
-        true_classnames = train_dataset.get_class_names_from_indices(labels_list)
-        assert len(true_classnames) == len(labels_list)
-
-    def test_get_disp_sentence_from_indices(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=False)
-        instances_dict = next(iter(loader))
-        tokens = instances_dict["tokens"]
-        tokens_list = tokens.tolist()
-        train_sentence = train_dataset.word_vocab.get_disp_sentence_from_indices(
-            tokens_list[0]
-        )
-        assert all([True for sentence in train_sentence if type(sentence) == str])
-
-    def test_preloaded_embedding_has_values(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        preloaded_emb = train_dataset.word_vocab.load_embedding()
-        assert type(preloaded_emb) == torch.Tensor
-
-    def test_dataset_returns_instances_when_required(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        first_instance = train_dataset[0]["instance"].split()
-        assert all([type(word) == str for word in first_instance])
-
-    def test_loader_returns_list_of_instances(self, setup_parsect_train_dataset):
-        train_dataset, dataset_options = setup_parsect_train_dataset
-        loader = DataLoader(dataset=train_dataset, batch_size=3, shuffle=False)
-        instances_dict = next(iter(loader))
-
-        assert len(instances_dict["instance"]) == 3
-        assert type(instances_dict["instance"][0]) == str
-
-    def test_stratified_split(self, setup_parsect_train_dataset):
-        dataset, options = setup_parsect_train_dataset
-        lines = ["a"] * 100
-        labels = ["title"] * 100
-
-        (
-            (train_lines, train_labels),
-            (validation_lines, validation_labels),
-            (test_lines, test_labels),
-        ) = dataset.get_train_valid_test_stratified_split(
-            lines, labels, dataset.classname2idx
-        )
-
-        assert len(train_lines) == 80
-        assert len(train_labels) == 80
-        assert len(validation_lines) == 10
-        assert len(validation_labels) == 10
-        assert len(test_lines) == 10
-        assert len(test_labels) == 10
-
-    def test_get_lines_labels_stratified(self, setup_parsect_train_dataset):
-        dataset, options = setup_parsect_train_dataset
-        dataset.debug_dataset_proportion = 1
-
-        lines, labels = dataset.get_lines_labels(filename=SECT_LABEL_FILE)
-        assert all([bool(line.strip()) for line in lines])
-        assert all([bool(label.strip()) for label in labels])
-
-    def test_print_stats_works(self, setup_parsect_train_dataset):
-        dataset, options = setup_parsect_train_dataset
-        try:
-            dataset.print_stats()
-        except:
-            pytest.fail("Test print stats works")
-
-    def test_parsect_in_sciwing_class_nursery(self):
-        assert ClassNursery.class_nursery.get("SectLabelDataset") is not None
+    return dataset_manager
+
+
+class TestSectLabelDataset:
+    def test_label_mapping_len(self, setup_sectlabel_dataset_manager):
+        dataset_manager = setup_sectlabel_dataset_manager
+        train_dataset = dataset_manager.train_dataset
+        lines, labels = train_dataset.get_lines_labels()
+        labels = [label.text for label in labels]
+        labels = list(set(labels))
+        assert len(labels) == 23
+
+    def test_no_line_empty(self, setup_sectlabel_dataset_manager):
+        dataset_manager = setup_sectlabel_dataset_manager
+        for dataset in [
+            dataset_manager.train_dataset,
+            dataset_manager.dev_dataset,
+            dataset_manager.test_dataset,
+        ]:
+            lines, labels = dataset.get_lines_labels()
+            assert all([bool(line.text.strip()) for line in lines])
+
+    def test_no_label_empty(self, setup_sectlabel_dataset_manager):
+        dataset_manager = setup_sectlabel_dataset_manager
+        for dataset in [
+            dataset_manager.train_dataset,
+            dataset_manager.dev_dataset,
+            dataset_manager.test_dataset,
+        ]:
+            lines, labels = dataset.get_lines_labels()
+            assert all([bool(label.text.strip()) for label in labels])
diff --git a/tests/datasets/classification/test_text_classification_dataset.py b/tests/datasets/classification/test_text_classification_dataset.py
new file mode 100644
index 0000000..9cf4b34
--- /dev/null
+++ b/tests/datasets/classification/test_text_classification_dataset.py
@@ -0,0 +1,42 @@
+import pytest
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDataset,
+)
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+
+
+@pytest.fixture(scope="session")
+def test_file(tmpdir_factory):
+    p = tmpdir_factory.mktemp("data").join("test.txt")
+    p.write("line1###label1\nline2###label2")
+    return p
+
+
+class TestTextClassificationDataset:
+    def test_get_lines_labels(self, test_file):
+        classification_dataset = TextClassificationDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        lines = classification_dataset.lines
+        assert len(lines) == 2
+
+    def test_len_dataset(self, test_file):
+        classification_dataset = TextClassificationDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        assert len(classification_dataset) == 2
+
+    def test_get_item(self, test_file):
+        classification_dataset = TextClassificationDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        num_instances = len(classification_dataset)
+        tokens = ["line1", "line2"]
+        line_tokens = []
+        for idx in range(num_instances):
+            line, label = classification_dataset[idx]
+            line_tokens.extend(line.tokens["tokens"])
+
+        line_tokens = list(map(lambda token: token.text, line_tokens))
+
+        assert set(tokens) == set(line_tokens)
diff --git a/tests/datasets/seq_labeling/test_parscit_dataset.py b/tests/datasets/seq_labeling/test_parscit_dataset.py
index 9d2ca05..bfbdd9c 100644
--- a/tests/datasets/seq_labeling/test_parscit_dataset.py
+++ b/tests/datasets/seq_labeling/test_parscit_dataset.py
@@ -1,11 +1,9 @@
 import sciwing.constants as constants
 import pytest
-from sciwing.utils.common import write_nfold_parscit_train_test
-from sciwing.datasets.seq_labeling.parscit_dataset import ParscitDataset
-from torch.utils.data import DataLoader
-from sciwing.utils.class_nursery import ClassNursery
-
 import pathlib
+from sciwing.datasets.seq_labeling.seq_labelling_dataset import (
+    SeqLabellingDatasetManager,
+)
 
 FILES = constants.FILES
 PATHS = constants.PATHS
@@ -13,240 +11,28 @@ PARSCIT_TRAIN_FILE = FILES["PARSCIT_TRAIN_FILE"]
 DATA_DIR = PATHS["DATA_DIR"]
 
 
-@pytest.fixture()
-def setup_parscit_train_dataset(tmpdir):
-    parscit_train_filepath = pathlib.Path(PARSCIT_TRAIN_FILE)
-    train_file = pathlib.Path(DATA_DIR, "parscit_train_conll.txt")
-    test_file = pathlib.Path(DATA_DIR, "parscit_test_conll.txt")
-    is_write_success = next(
-        write_nfold_parscit_train_test(
-            parscit_train_filepath,
-            output_train_filepath=train_file,
-            output_test_filepath=test_file,
-        )
-    )
-    vocab_store_location = tmpdir.mkdir("tempdir").join("vocab.json")
-    char_vocab_store_location = tmpdir.mkdir("tempdir_char").join("char_vocab.json")
-    DEBUG = True
-    MAX_NUM_WORDS = 10000
-    MAX_LENGTH = 20
-    MAX_CHAR_LENGTH = 25
-    EMBEDDING_DIM = 100
-    CHAR_EMBEDDING_DIM = 25
-    train_dataset = None
-    test_dataset = None
-
-    if is_write_success:
-        train_dataset = ParscitDataset(
-            filename=str(train_file),
-            dataset_type=" train",
-            max_num_words=MAX_NUM_WORDS,
-            max_instance_length=MAX_LENGTH,
-            max_char_length=MAX_CHAR_LENGTH,
-            word_vocab_store_location=vocab_store_location,
-            debug=DEBUG,
-            word_embedding_type="random",
-            word_embedding_dimension=EMBEDDING_DIM,
-            word_add_start_end_token=False,
-            char_vocab_store_location=char_vocab_store_location,
-            char_embedding_dimension=CHAR_EMBEDDING_DIM,
-        )
-        test_dataset = ParscitDataset(
-            filename=str(test_file),
-            dataset_type="train",
-            max_num_words=MAX_NUM_WORDS,
-            max_instance_length=MAX_LENGTH,
-            max_char_length=MAX_CHAR_LENGTH,
-            word_vocab_store_location=vocab_store_location,
-            debug=DEBUG,
-            word_embedding_type="random",
-            word_embedding_dimension=EMBEDDING_DIM,
-            word_add_start_end_token=False,
-            char_vocab_store_location=char_vocab_store_location,
-            char_embedding_dimension=CHAR_EMBEDDING_DIM,
-        )
-
-    options = {
-        "MAX_NUM_WORDS": MAX_NUM_WORDS,
-        "MAX_LENGTH": MAX_LENGTH,
-        "MAX_CHAR_LENGTH": MAX_CHAR_LENGTH,
-        "EMBEDDING_DIM": EMBEDDING_DIM,
-        "TRAIN_FILE": str(train_file),
-        "TEST_FILE": str(test_file),
-    }
-
-    return train_dataset, test_dataset, options
-
+@pytest.fixture
+def setup_parscit_dataset_manager():
+    data_dir = pathlib.Path(DATA_DIR)
+    parscit_train_file = data_dir.joinpath("parscit.train")
+    parscit_dev_file = data_dir.joinpath("parscit.dev")
+    parscit_test_file = data_dir.joinpath("parscit.test")
 
-@pytest.fixture()
-def setup_parscit_train_dataset_maxlen_2(tmpdir):
-    parscit_train_filepath = pathlib.Path(PARSCIT_TRAIN_FILE)
-    train_file = pathlib.Path(DATA_DIR, "parscit_train_conll.txt")
-    test_file = pathlib.Path(DATA_DIR, "parscit_test_conll.txt")
-    is_write_success = next(
-        write_nfold_parscit_train_test(
-            parscit_train_filepath,
-            output_train_filepath=train_file,
-            output_test_filepath=test_file,
-        )
+    dataset_manager = SeqLabellingDatasetManager(
+        train_filename=str(parscit_train_file),
+        dev_filename=str(parscit_dev_file),
+        test_filename=str(parscit_test_file),
     )
-    vocab_store_location = tmpdir.mkdir("tempdir").join("vocab.json")
-    char_vocab_store_location = tmpdir.mkdir("tempdir_char").join("char_vocab.json")
-    DEBUG = True
-    MAX_NUM_WORDS = 10000
-    MAX_CHAR_LENGTH = 25
-    MAX_LENGTH = 2
-    EMBEDDING_DIM = 100
-    CHAR_EMBEDDING_DIM = 25
-    train_dataset = None
-    test_dataset = None
-
-    if is_write_success:
-        train_dataset = ParscitDataset(
-            filename=str(train_file),
-            dataset_type="train",
-            max_num_words=MAX_NUM_WORDS,
-            max_instance_length=MAX_LENGTH,
-            max_char_length=MAX_CHAR_LENGTH,
-            word_vocab_store_location=vocab_store_location,
-            debug=DEBUG,
-            word_embedding_type="random",
-            word_embedding_dimension=EMBEDDING_DIM,
-            word_add_start_end_token=True,
-            char_vocab_store_location=char_vocab_store_location,
-            char_embedding_dimension=CHAR_EMBEDDING_DIM,
-        )
-        test_dataset = ParscitDataset(
-            filename=str(test_file),
-            dataset_type="train",
-            max_num_words=MAX_NUM_WORDS,
-            max_instance_length=MAX_LENGTH,
-            max_char_length=MAX_CHAR_LENGTH,
-            word_vocab_store_location=vocab_store_location,
-            debug=DEBUG,
-            word_embedding_type="random",
-            word_embedding_dimension=EMBEDDING_DIM,
-            word_add_start_end_token=True,
-            char_vocab_store_location=char_vocab_store_location,
-            char_embedding_dimension=CHAR_EMBEDDING_DIM,
-        )
-
-    options = {
-        "MAX_NUM_WORDS": MAX_NUM_WORDS,
-        "MAX_LENGTH": MAX_LENGTH,
-        "MAX_CHAR_LENGTH": MAX_CHAR_LENGTH,
-        "EMBEDDING_DIM": EMBEDDING_DIM,
-        "TRAIN_FILE": str(train_file),
-        "TEST_FILE": str(test_file),
-    }
-
-    return train_dataset, test_dataset, options
+    return dataset_manager
 
 
 class TestParscitDataset:
-    def test_num_classes(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        num_classes = train_dataset.get_num_classes()
-        assert num_classes == 16
-
-    def test_lines_labels_not_empty(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        train_lines, train_labels = train_dataset.get_lines_labels(
-            options["TRAIN_FILE"]
-        )
-        assert all([bool(line.strip()) for line in train_lines])
-        assert all([bool(label.strip()) for label in train_labels])
-
-        test_lines, test_labels = test_dataset.get_lines_labels(options["TEST_FILE"])
-        assert all([bool(line.strip()) for line in test_lines])
-        assert all([bool(label.strip()) for label in test_labels])
-
-    def test_lines_labels_are_equal_length(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        train_lines, train_labels = train_dataset.get_lines_labels(
-            options["TRAIN_FILE"]
-        )
-        len_lines_labels = zip(
-            (len(line.split()) for line in train_lines),
-            (len(label.split()) for label in train_labels),
-        )
-        assert all([len_line == len_label for len_line, len_label in len_lines_labels])
-
-        test_lines, test_labels = test_dataset.get_lines_labels(options["TEST_FILE"])
-        len_lines_labels = zip(
-            (len(line.split()) for line in test_lines),
-            (len(label.split()) for label in test_labels),
-        )
-        assert all([len_line == len_label for len_line, len_label in len_lines_labels])
-
-    def test_get_stats_works(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        try:
-            train_dataset.print_stats()
-        except:
-            pytest.fail(
-                f"Get stats for Parscit {train_dataset.dataset_type} does not work"
-            )
-
-        try:
-            test_dataset.print_stats()
-        except:
-            pytest.fail(
-                f"Get stats for Parscit {train_dataset.dataset_type} does not work"
-            )
-
-    def test_tokens_max_length(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        lines, labels = train_dataset.get_lines_labels(options["TRAIN_FILE"])
-        num_lines = len(lines)
-        for idx in range(num_lines):
-            assert len(train_dataset[idx]["tokens"]) == options["MAX_LENGTH"]
-
-    def test_char_tokens_max_length(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        lines, labels = train_dataset.get_lines_labels(options["TRAIN_FILE"])
-        num_lines = len(lines)
-        for idx in range(num_lines):
-            char_tokens = train_dataset[idx]["char_tokens"]
-            assert char_tokens.size() == (
-                options["MAX_LENGTH"],
-                options["MAX_CHAR_LENGTH"],
-            )
-
-    def test_instance_dict_with_loader(self, setup_parscit_train_dataset):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=False)
-        instances_dict = next(iter(loader))
-        assert len(instances_dict["tokens"]) == 2
-
-        test_loader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=False)
-        instances_dict = next(iter(test_loader))
-        assert len(instances_dict["tokens"]) == 2
-
-    def test_instance_dict_have_correct_padded_lengths(
-        self, setup_parscit_train_dataset
-    ):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset
-        loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=False)
-        instances_dict = next(iter(loader))
-
-        assert instances_dict["tokens"].size() == (2, options["MAX_LENGTH"])
-        assert instances_dict["label"].size() == (2, options["MAX_LENGTH"])
-        assert instances_dict["char_tokens"].size() == (
-            2,
-            options["MAX_LENGTH"],
-            options["MAX_CHAR_LENGTH"],
-        )
-
-    def test_labels_maxlen_2(self, setup_parscit_train_dataset_maxlen_2):
-        train_dataset, test_dataset, options = setup_parscit_train_dataset_maxlen_2
-        instances_dict = train_dataset[0]
-        label = instances_dict["label"].tolist()
-        tokens = instances_dict["tokens"].tolist()
-        label = [train_dataset.idx2classname[lbl] for lbl in label]
-        tokens = [train_dataset.word_vocab.idx2token[token_idx] for token_idx in tokens]
-        assert label == ["starting", "ending"]
-        assert tokens == ["<SOS>", "<EOS>"]
-
-    def test_parscit_in_nursery(self):
-        assert ClassNursery.class_nursery.get("ParscitDataset") is not None
+    def test_num_classes(self, setup_parscit_dataset_manager):
+        dataset_manager = setup_parscit_dataset_manager
+        train_dataset = dataset_manager.train_dataset
+        lines, labels = train_dataset.get_lines_labels()
+        train_labels = []
+        for label in labels:
+            train_labels.extend(label.labels)
+        num_classes = len(set(train_labels))
+        assert num_classes == 14
diff --git a/tests/datasets/seq_labeling/test_seq_labelling_dataset.py b/tests/datasets/seq_labeling/test_seq_labelling_dataset.py
new file mode 100644
index 0000000..529447f
--- /dev/null
+++ b/tests/datasets/seq_labeling/test_seq_labelling_dataset.py
@@ -0,0 +1,40 @@
+import pytest
+from sciwing.datasets.seq_labeling.seq_labelling_dataset import SeqLabellingDataset
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+
+
+@pytest.fixture(scope="session")
+def test_file(tmpdir_factory):
+    p = tmpdir_factory.mktemp("data").join("test.txt")
+    p.write(
+        "word11###label1 word21###label2\nword12###label1 word22###label2 word32###label3"
+    )
+    return p
+
+
+class TestSeqLabellingDataset:
+    def test_get_lines_labels(self, test_file):
+        dataset = SeqLabellingDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        lines, labels = dataset.get_lines_labels()
+        assert len(lines) == 2
+
+    def test_len(self, test_file):
+        dataset = SeqLabellingDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        assert len(dataset) == 2
+
+    def test_get_item(self, test_file):
+        dataset = SeqLabellingDataset(
+            filename=str(test_file), tokenizers={"tokens": WordTokenizer()}
+        )
+        num_instances = len(dataset)
+
+        for idx in range(num_instances):
+            line, label = dataset[idx]
+            word_tokens = line.tokens["tokens"]
+            label_tokens = label.tokens["seq_label"]
+            print(f"label tokens {label.tokens}")
+            assert len(word_tokens) == len(label_tokens)
diff --git a/tests/datasets/test_sprinkle_dataset.py b/tests/datasets/test_sprinkle_dataset.py
deleted file mode 100644
index 3d452aa..0000000
--- a/tests/datasets/test_sprinkle_dataset.py
+++ /dev/null
@@ -1,200 +0,0 @@
-import pytest
-import sciwing.constants as constants
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
-from sciwing.datasets.seq_labeling.parscit_dataset import ParscitDataset
-import pathlib
-from sciwing.utils.common import write_nfold_parscit_train_test
-
-
-FILES = constants.FILES
-PATHS = constants.PATHS
-SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
-PARSCIT_TRAIN_FILE = FILES["PARSCIT_TRAIN_FILE"]
-DATA_DIR = PATHS["DATA_DIR"]
-
-
-@pytest.fixture(scope="session")
-def parsect_dataset(tmpdir_factory):
-    MAX_NUM_WORDS = 1000
-    MAX_LENGTH = 10
-    vocab_store_location = tmpdir_factory.mktemp("tempdir").join("vocab.json")
-    DEBUG = True
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        train_size=0.8,
-        test_size=0.2,
-        validation_size=0.5,
-    )
-
-    return (
-        train_dataset,
-        {
-            "MAX_NUM_WORDS": MAX_NUM_WORDS,
-            "MAX_LENGTH": MAX_LENGTH,
-            "vocab_store_location": vocab_store_location,
-        },
-    )
-
-
-@pytest.fixture(scope="session")
-def parscit_dataset(tmpdir_factory):
-    parscit_train_filepath = pathlib.Path(PARSCIT_TRAIN_FILE)
-    train_file = pathlib.Path(DATA_DIR, "parscit_train_conll.txt")
-    test_file = pathlib.Path(DATA_DIR, "parscit_test_conll.txt")
-    is_write_success = next(
-        write_nfold_parscit_train_test(
-            parscit_train_filepath,
-            output_train_filepath=train_file,
-            output_test_filepath=test_file,
-        )
-    )
-    MAX_NUM_WORDS = 1000
-    MAX_CHAR_LENGTH = 10
-    MAX_INSTANCE_LENGTH = 10
-    WORD_VOCAB_STORE_LOCATION = tmpdir_factory.mktemp("tempdir").join("vocab.json")
-    CHAR_VOCAB_STORE_LOCATION = tmpdir_factory.mktemp("tempdir_char").join(
-        "char_vocab.json"
-    )
-    CAPITALIZATION_VOCAB_STORE_LOCATION = None
-    CAPITALIZATION_EMBEDDING_DIMENSION = None
-    DEBUG = True
-    DEBUG_DATASET_PROPORTION = 0.1
-    WORD_EMBEDDING_TYPE = "random"
-    WORD_EMBEDDING_DIMENSION = 100
-    CHAR_EMBEDDING_DIMENSION = 10
-
-    dataset = ParscitDataset(
-        filename=str(train_file),
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_INSTANCE_LENGTH,
-        max_char_length=MAX_CHAR_LENGTH,
-        word_vocab_store_location=WORD_VOCAB_STORE_LOCATION,
-        char_vocab_store_location=CHAR_VOCAB_STORE_LOCATION,
-        captialization_vocab_store_location=CAPITALIZATION_VOCAB_STORE_LOCATION,
-        capitalization_emb_dim=CAPITALIZATION_EMBEDDING_DIMENSION,
-        debug=DEBUG,
-        debug_dataset_proportion=DEBUG_DATASET_PROPORTION,
-        word_embedding_type=WORD_EMBEDDING_TYPE,
-        word_embedding_dimension=WORD_EMBEDDING_DIMENSION,
-        char_embedding_dimension=CHAR_EMBEDDING_DIMENSION,
-        word_start_token="<SOS>",
-        word_end_token="<EOS>",
-        word_pad_token="<PAD>",
-        word_unk_token="<UNK>",
-        word_add_start_end_token=False,
-    )
-
-    options = {
-        "MAX_NUM_WORDS": MAX_NUM_WORDS,
-        "MAX_INSTANCE_LENGTH": MAX_INSTANCE_LENGTH,
-        "MAX_CHAR_LENGTH": MAX_CHAR_LENGTH,
-        "WORD_VOCAB_STORE_LOCATION": WORD_VOCAB_STORE_LOCATION,
-        "CHAR_VOCAB_STORE_LOCATION": CHAR_VOCAB_STORE_LOCATION,
-        "CAPITALIZATION_VOCAB_STORE_LOCATION": CAPITALIZATION_VOCAB_STORE_LOCATION,
-        "CAPITALIZATION_EMBEDDING_DIMENSION": CAPITALIZATION_EMBEDDING_DIMENSION,
-        "DEBUG": DEBUG,
-        "DEBUG_DATASET_PROPORTION": DEBUG_DATASET_PROPORTION,
-        "WORD_EMBEDDING_TYPE": WORD_EMBEDDING_TYPE,
-        "WORD_EMBEDDING_DIMENSION": WORD_EMBEDDING_DIMENSION,
-        "CHAR_EMBEDDING_DIMENSION": CHAR_EMBEDDING_DIMENSION,
-    }
-
-    return dataset, options
-
-
-class TestSprinkleOnSectLabelDataset:
-    @pytest.mark.parametrize(
-        "attribute",
-        [
-            "filename",
-            "dataset_type",
-            "max_num_words",
-            "max_instance_length",
-            "word_vocab_store_location",
-            "debug",
-            "debug_dataset_proportion",
-            "word_embedding_type",
-            "word_embedding_dimension",
-            "word_start_token",
-            "word_end_token",
-            "word_pad_token",
-            "word_unk_token",
-            "train_size",
-            "test_size",
-            "validation_size",
-            "word_tokenizer",
-            "word_tokenization_type",
-            "word_vocab",
-            "num_instances",
-            "label_stats_table",
-        ],
-    )
-    def test_decorated_instance_has_attribute(self, attribute, parsect_dataset):
-        dataset, options = parsect_dataset
-        assert hasattr(dataset, attribute)
-
-    @pytest.mark.parametrize(
-        "attribute, exp_value",
-        [
-            ("filename", SECT_LABEL_FILE),
-            ("dataset_type", "train"),
-            ("max_num_words", 1000),
-            ("max_instance_length", 10),
-            ("train_size", 0.8),
-            ("test_size", 0.2),
-            ("validation_size", 0.5),
-        ],
-    )
-    def test_decorated_instance_has_correct_values_for(
-        self, parsect_dataset, attribute, exp_value
-    ):
-        dataset, options = parsect_dataset
-        value = getattr(dataset, attribute)
-        assert value == exp_value
-
-
-class TestSprinkleOnParscitDataset:
-    @pytest.mark.parametrize(
-        "attribute",
-        [
-            "filename",
-            "dataset_type",
-            "max_num_words",
-            "max_instance_length",
-            "word_vocab_store_location",
-            "debug",
-            "debug_dataset_proportion",
-            "word_embedding_type",
-            "word_embedding_dimension",
-            "word_start_token",
-            "word_end_token",
-            "word_pad_token",
-            "word_unk_token",
-            "train_size",
-            "test_size",
-            "validation_size",
-            "word_tokenizer",
-            "word_tokenization_type",
-            "word_vocab",
-            "num_instances",
-            "label_stats_table",
-            "max_num_chars",
-            "char_vocab_store_location",
-            "char_embedding_type",
-            "char_embedding_dimension",
-            "char_unk_token",
-            "char_pad_token",
-            "char_start_token",
-            "char_end_token",
-        ],
-    )
-    def test_decorated_instance_has_attribute(self, attribute, parscit_dataset):
-        dataset, options = parscit_dataset
-        assert hasattr(dataset, attribute) and getattr(dataset, attribute) is not None
diff --git a/tests/engine/test_engine.py b/tests/engine/test_engine.py
index 2b1d4c9..72f27c1 100644
--- a/tests/engine/test_engine.py
+++ b/tests/engine/test_engine.py
@@ -1,13 +1,14 @@
 from sciwing.engine.engine import Engine
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.modules.bow_encoder import BOW_Encoder
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.datasets.classification.sectlabel_dataset import SectLabelDataset
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.data.line import Line
+from sciwing.data.label import Label
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-from torch.nn import Embedding
-import torch.optim as optim
 import torch
-import numpy as np
 import os
 from sciwing.utils.class_nursery import ClassNursery
 
@@ -18,151 +19,91 @@ FILES = constants.FILES
 SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
 
 
-@pytest.fixture(scope="session", params=["loss", "micro_fscore", "macro_fscore"])
-def setup_engine_test_with_simple_classifier(request, tmpdir_factory):
-    MAX_NUM_WORDS = 1000
-    MAX_LENGTH = 50
-    vocab_store_location = tmpdir_factory.mktemp("tempdir").join("vocab.json")
-    DEBUG = True
-    BATCH_SIZE = 1
-    NUM_TOKENS = 3
-    EMB_DIM = 300
-
-    train_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="train",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        word_embedding_type="random",
-        word_embedding_dimension=EMB_DIM,
-    )
+@pytest.fixture(scope="session")
+def clf_datasets_manager(tmpdir_factory):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
 
-    validation_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="valid",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        word_embedding_type="random",
-        word_embedding_dimension=EMB_DIM,
-    )
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
 
-    test_dataset = SectLabelDataset(
-        filename=SECT_LABEL_FILE,
-        dataset_type="test",
-        max_num_words=MAX_NUM_WORDS,
-        max_instance_length=MAX_LENGTH,
-        word_vocab_store_location=vocab_store_location,
-        debug=DEBUG,
-        word_embedding_type="random",
-        word_embedding_dimension=EMB_DIM,
-    )
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("test_line1###label1\ntest_line2###label2")
 
-    VOCAB_SIZE = MAX_NUM_WORDS + len(train_dataset.word_vocab.special_vocab)
-    NUM_CLASSES = train_dataset.get_num_classes()
-    NUM_EPOCHS = 1
-    embedding = Embedding.from_pretrained(torch.zeros([VOCAB_SIZE, EMB_DIM]))
-    labels = torch.LongTensor([1])
-    metric = PrecisionRecallFMeasure(idx2labelname_mapping=train_dataset.idx2classname)
-    embedder = VanillaEmbedder(embedding_dim=EMB_DIM, embedding=embedding)
-    encoder = BOW_Encoder(
-        emb_dim=EMB_DIM, embedder=embedder, dropout_value=0, aggregation_type="sum"
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+        batch_size=1,
     )
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, NUM_TOKENS))
-    tokens = torch.LongTensor(tokens)
-    model = SimpleClassifier(
-        encoder=encoder,
-        encoding_dim=EMB_DIM,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=False,
+
+    return clf_dataset_manager
+
+
+@pytest.fixture(scope="session", params=["loss", "micro_fscore", "macro_fscore"])
+def setup_engine_test_with_simple_classifier(
+    request, clf_datasets_manager, tmpdir_factory
+):
+    track_for_best = request.param
+    sample_proportion = 0.5
+    datasets_manager = clf_datasets_manager
+    word_embedder = WordEmbedder(embedding_type="glove_6B_50")
+    bow_encoder = BOW_Encoder(embedder=word_embedder)
+    classifier = SimpleClassifier(
+        encoder=bow_encoder,
+        encoding_dim=word_embedder.get_embedding_dimension(),
+        num_classes=2,
+        classification_layer_bias=True,
+        datasets_manager=datasets_manager,
     )
+    train_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+    validation_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+
+    optimizer = torch.optim.Adam(params=classifier.parameters())
+    batch_size = 1
+    save_dir = tmpdir_factory.mktemp("experiment_1")
+    num_epochs = 1
+    save_every = 1
+    log_train_metrics_every = 10
 
-    optimizer = optim.SGD(model.parameters(), lr=0.01)
     engine = Engine(
-        model,
-        train_dataset,
-        validation_dataset,
-        test_dataset,
+        model=classifier,
+        datasets_manager=datasets_manager,
         optimizer=optimizer,
-        batch_size=BATCH_SIZE,
-        save_dir=tmpdir_factory.mktemp("model_save"),
-        num_epochs=NUM_EPOCHS,
-        save_every=1,
-        log_train_metrics_every=10,
-        metric=metric,
-        track_for_best=request.param,
+        batch_size=batch_size,
+        save_dir=save_dir,
+        num_epochs=num_epochs,
+        save_every=save_every,
+        log_train_metrics_every=log_train_metrics_every,
+        train_metric=train_metric,
+        validation_metric=validation_metric,
+        test_metric=test_metric,
+        track_for_best=track_for_best,
+        sample_proportion=sample_proportion,
     )
 
-    options = {
-        "MAX_NUM_WORDS": MAX_NUM_WORDS,
-        "MAX_LENGTH": MAX_LENGTH,
-        "BATCH_SIZE": BATCH_SIZE,
-        "NUM_TOKENS": NUM_TOKENS,
-        "EMB_DIM": EMB_DIM,
-        "VOCAB_SIZE": VOCAB_SIZE,
-        "NUM_CLASSES": NUM_CLASSES,
-        "NUM_EPOCHS": NUM_EPOCHS,
-    }
-
-    return engine, tokens, labels, options
+    return engine
 
 
 class TestEngine:
-    def test_train_loader_gets_equal_length_tokens(
-        self, setup_engine_test_with_simple_classifier
-    ):
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
+    def test_train_loader(self, setup_engine_test_with_simple_classifier):
+        engine = setup_engine_test_with_simple_classifier
         train_dataset = engine.get_train_dataset()
         train_loader = engine.get_loader(train_dataset)
 
-        len_tokens = []
-        for iter_dict in train_loader:
-            tokens = iter_dict["tokens"]
-            len_tokens.append(tokens.size()[1])
-
-        # check all lengths are same
-        assert len(set(len_tokens)) == 1
-
-    def test_validation_loader_gets_equal_length_tokens(
-        self, setup_engine_test_with_simple_classifier
-    ):
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
-        validation_dataset = engine.get_validation_dataset()
-        validation_loader = engine.get_loader(validation_dataset)
-
-        len_tokens = []
-
-        for iter_dict in validation_loader:
-            tokens = iter_dict["tokens"]
-            len_tokens.append(tokens.size()[1])
-
-        assert len(set(len_tokens)) == 1
-
-    def test_loader_gets_equal_length_tokens(
-        self, setup_engine_test_with_simple_classifier
-    ):
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
-        test_dataset = engine.get_test_dataset()
-        test_loader = engine.get_loader(test_dataset)
-
-        len_tokens = []
-
-        for iter_dict in test_loader:
-            tokens = iter_dict["tokens"]
-            len_tokens.append(tokens.size()[1])
-
-        assert len(set(len_tokens)) == 1
+        for lines_labels in train_loader:
+            for line, label in lines_labels:
+                assert isinstance(line, Line)
+                assert isinstance(label, Label)
 
     def test_one_train_epoch(self, setup_engine_test_with_simple_classifier):
         # check whether you can run train_epoch without throwing an error
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
+        engine = setup_engine_test_with_simple_classifier
         engine.train_epoch(0)
 
     def test_save_model(self, setup_engine_test_with_simple_classifier):
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
+        engine = setup_engine_test_with_simple_classifier
         engine.train_epoch_end(0)
 
         # test for the file model_epoch_1.pt
@@ -173,7 +114,7 @@ class TestEngine:
         """
         Just tests runs without any errors
         """
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
+        engine = setup_engine_test_with_simple_classifier
         try:
             engine.run()
         except:
@@ -183,7 +124,7 @@ class TestEngine:
         """
         Test whether engine loads the model without any error.
         """
-        engine, tokens, labels, options = setup_engine_test_with_simple_classifier
+        engine = setup_engine_test_with_simple_classifier
         try:
             engine.train_epoch_end(0)
             engine.load_model_from_file(
diff --git a/tests/infer/classification/test_classification_inference.py b/tests/infer/classification/test_classification_inference.py
index a37cb09..3f057cc 100644
--- a/tests/infer/classification/test_classification_inference.py
+++ b/tests/infer/classification/test_classification_inference.py
@@ -1,19 +1,18 @@
 import pytest
 import sciwing.constants as constants
 import pathlib
-from sciwing.infer.bow_lc_parsect_infer import get_bow_lc_parsect_infer
-from sciwing.infer.bow_lc_gensect_infer import get_bow_lc_gensect_infer
-from sciwing.infer.bow_elmo_emb_lc_parsect_infer import get_elmo_emb_lc_infer_parsect
-from sciwing.infer.bow_elmo_emb_lc_gensect_infer import get_elmo_emb_lc_infer_gensect
-from sciwing.infer.bow_bert_emb_lc_parsect_infer import (
-    get_bow_bert_emb_lc_parsect_infer,
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
 )
-from sciwing.infer.bow_bert_emb_lc_gensect_infer import (
-    get_bow_bert_emb_lc_gensect_infer,
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.engine.engine import Engine
+from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
+import torch
+from sciwing.infer.classification.classification_inference import (
+    ClassificationInference,
 )
-from sciwing.infer.bi_lstm_lc_infer_parsect import get_bilstm_lc_infer_parsect
-from sciwing.infer.bi_lstm_lc_infer_gensect import get_bilstm_lc_infer_gensect
-from sciwing.infer.elmo_bi_lstm_lc_infer import get_elmo_bilstm_lc_infer
 
 FILES = constants.FILES
 SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
@@ -21,81 +20,119 @@ PATHS = constants.PATHS
 
 OUTPUT_DIR = PATHS["OUTPUT_DIR"]
 
-directory_infer_client_mapping = {
-    "parsect_bow_random_emb_lc_debug": get_bow_lc_parsect_infer,
-    "gensect_bow_random_emb_lc_debug": get_bow_lc_gensect_infer,
-    "parsect_bow_elmo_emb_lc_debug": get_elmo_emb_lc_infer_parsect,
-    "debug_gensect_bow_elmo_emb_lc": get_elmo_emb_lc_infer_gensect,
-    "parsect_bow_bert_debug": get_bow_bert_emb_lc_parsect_infer,
-    "gensect_bow_bert_debug": get_bow_bert_emb_lc_gensect_infer,
-    "parsect_bi_lstm_lc_debug": get_bilstm_lc_infer_parsect,
-    "debug_gensect_bi_lstm_lc": get_bilstm_lc_infer_gensect,
-    "debug_parsect_elmo_bi_lstm_lc": get_elmo_bilstm_lc_infer,
-}
 
-directory_infer_client_mapping = directory_infer_client_mapping.items()
+@pytest.fixture(scope="session")
+def clf_datasets_manager(tmpdir_factory):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
 
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
 
-@pytest.fixture(params=directory_infer_client_mapping)
-def setup_inference(tmpdir_factory, request):
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("test_line1###label1\ntest_line2###label2")
 
-    dirname = request.param[0]
-    infer_func = request.param[1]
-    debug_experiment_path = pathlib.Path(OUTPUT_DIR, dirname)
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+        batch_size=1,
+    )
 
-    inference_client = infer_func(str(debug_experiment_path))
-    return inference_client
+    return clf_dataset_manager
+
+
+@pytest.fixture(scope="session", params=["loss", "micro_fscore", "macro_fscore"])
+def setup_sectlabel_bow_glove_infer(request, clf_datasets_manager, tmpdir_factory):
+    track_for_best = request.param
+    sample_proportion = 0.5
+    datasets_manager = clf_datasets_manager
+    word_embedder = WordEmbedder(embedding_type="glove_6B_50")
+    bow_encoder = BOW_Encoder(embedder=word_embedder)
+    classifier = SimpleClassifier(
+        encoder=bow_encoder,
+        encoding_dim=word_embedder.get_embedding_dimension(),
+        num_classes=2,
+        classification_layer_bias=True,
+        datasets_manager=datasets_manager,
+    )
+    train_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+    validation_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+    test_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
+
+    optimizer = torch.optim.Adam(params=classifier.parameters())
+    batch_size = 1
+    save_dir = tmpdir_factory.mktemp("experiment_1")
+    num_epochs = 1
+    save_every = 1
+    log_train_metrics_every = 10
+
+    engine = Engine(
+        model=classifier,
+        datasets_manager=datasets_manager,
+        optimizer=optimizer,
+        batch_size=batch_size,
+        save_dir=save_dir,
+        num_epochs=num_epochs,
+        save_every=save_every,
+        log_train_metrics_every=log_train_metrics_every,
+        train_metric=train_metric,
+        validation_metric=validation_metric,
+        test_metric=test_metric,
+        track_for_best=track_for_best,
+        sample_proportion=sample_proportion,
+    )
+
+    engine.run()
+    model_filepath = pathlib.Path(save_dir).joinpath("best_model.pt")
+    infer = ClassificationInference(
+        model=classifier,
+        model_filepath=str(model_filepath),
+        datasets_manager=datasets_manager,
+    )
+    return infer
 
 
-@pytest.mark.skipif(
-    not all(
-        [
-            pathlib.Path(OUTPUT_DIR, dirname).exists()
-            for dirname, _ in directory_infer_client_mapping
-        ]
-    ),
-    reason="debug models do not exist in the output dir",
-)
 class TestClassificationInference:
-    def test_run_inference_works(self, setup_inference):
-        inference_client = setup_inference
+    def test_run_inference_works(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         try:
             inference_client.run_inference()
         except:
             pytest.fail("Run inference for classification dataset fails")
 
-    def test_run_test_works(self, setup_inference):
-        inference_client = setup_inference
+    def test_run_test_works(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         try:
             inference_client.run_test()
         except:
             pytest.fail("Run test doest not work")
 
-    def test_on_user_input_works(self, setup_inference):
-        inference_client = setup_inference
+    def test_on_user_input_works(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         try:
             inference_client.on_user_input(line="test input")
         except:
             pytest.fail("On user input fails")
 
-    def test_print_metrics_works(self, setup_inference):
-        inference_client = setup_inference
+    def test_print_metrics_works(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         inference_client.run_test()
         try:
             inference_client.report_metrics()
         except:
             pytest.fail("Print metrics failed")
 
-    def test_print_confusion_metrics_works(self, setup_inference):
-        inference_client = setup_inference
+    def test_print_confusion_metrics_works(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         inference_client.run_test()
         try:
             inference_client.print_confusion_matrix()
         except:
             pytest.fail("Print confusion matrix fails")
 
-    def test_get_misclassified_sentences(self, setup_inference):
-        inference_client = setup_inference
+    def test_get_misclassified_sentences(self, setup_sectlabel_bow_glove_infer):
+        inference_client = setup_sectlabel_bow_glove_infer
         inference_client.run_test()
         try:
             inference_client.get_misclassified_sentences(
diff --git a/tests/integration/test_pipeline_till_numericalization.py b/tests/integration/test_pipeline_till_numericalization.py
index da939b3..136c92a 100644
--- a/tests/integration/test_pipeline_till_numericalization.py
+++ b/tests/integration/test_pipeline_till_numericalization.py
@@ -8,7 +8,7 @@ from sciwing.utils.common import convert_sectlabel_to_json
 from sciwing.tokenizers.word_tokenizer import WordTokenizer
 from sciwing.preprocessing.instance_preprocessing import InstancePreprocessing
 from sciwing.vocab.vocab import Vocab
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 
 FILES = constants.FILES
 SECT_LABEL_FILE = FILES["SECT_LABEL_FILE"]
diff --git a/tests/metrics/test_precision_recall_fmeasure.py b/tests/metrics/test_precision_recall_fmeasure.py
index 1766d04..ce4010a 100644
--- a/tests/metrics/test_precision_recall_fmeasure.py
+++ b/tests/metrics/test_precision_recall_fmeasure.py
@@ -2,13 +2,41 @@ import pytest
 import torch
 from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
 from sciwing.utils.class_nursery import ClassNursery
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.modules.bow_encoder import BOW_Encoder
+from sciwing.models.simpleclassifier import SimpleClassifier
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+
+
+@pytest.fixture(scope="session")
+def clf_dataset_manager(tmpdir_factory):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
+
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
+
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("test_line1###label1\ntest_line2###label2")
+
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+        batch_size=1,
+    )
+
+    return clf_dataset_manager
 
 
 @pytest.fixture
-def setup_data_basecase():
+def setup_data_basecase(clf_dataset_manager):
+    dataset_manager = clf_dataset_manager
+    prf_metric = PrecisionRecallFMeasure(dataset_manager)
     predicted_probs = torch.FloatTensor([[0.1, 0.9], [0.7, 0.3]])
     labels = torch.LongTensor([1, 0]).view(-1, 1)
-    idx2labelname_mapping = {0: "good class", 1: "bad class"}
 
     expected_precision = {0: 1.0, 1: 1.0}
     expected_recall = {0: 1.0, 1: 1.0}
@@ -23,11 +51,11 @@ def setup_data_basecase():
     expected_micro_recall = 1.0
     expected_micro_fscore = 1.0
 
-    accuracy = PrecisionRecallFMeasure(idx2labelname_mapping=idx2labelname_mapping)
     return (
         predicted_probs,
         labels,
-        accuracy,
+        prf_metric,
+        dataset_manager,
         {
             "expected_precision": expected_precision,
             "expected_recall": expected_recall,
@@ -46,53 +74,10 @@ def setup_data_basecase():
 
 
 @pytest.fixture
-def setup_data_one_true_class_missing():
-    """
-    The batch of instances during training might not have all
-    true classes. What happens in that case??
-    The test case here captures the situation
-    :return:
-    """
-    predicted_probs = torch.FloatTensor([[0.8, 0.1, 0.2], [0.2, 0.5, 0.3]])
-    idx2labelname_mapping = {0: "good class", 1: "bad class", 2: "average_class"}
-    labels = torch.LongTensor([0, 2]).view(-1, 1)
-
-    expected_precision = {0: 1.0, 1: 0.0, 2: 0.0}
-    expected_recall = {0: 1.0, 1: 0.0, 2: 0.0}
-    expected_fscore = {0: 1.0, 1: 0.0, 2: 0.0}
-
-    accuracy = PrecisionRecallFMeasure(idx2labelname_mapping=idx2labelname_mapping)
-
-    return (
-        predicted_probs,
-        labels,
-        accuracy,
-        {
-            "expected_precision": expected_precision,
-            "expected_recall": expected_recall,
-            "expected_fscore": expected_fscore,
-        },
-    )
-
-
-@pytest.fixture
-def setup_data_to_test_length():
-    predicted_probs = torch.FloatTensor([[0.1, 0.8, 0.2], [0.2, 0.3, 0.5]])
-    labels = torch.LongTensor([0, 2]).view(-1, 1)
-    idx2labelname_mapping = {0: "good class", 1: "bad class", 2: "average_class"}
-
-    accuracy = PrecisionRecallFMeasure(idx2labelname_mapping=idx2labelname_mapping)
-
-    expected_length = 3
-
-    return predicted_probs, labels, accuracy, expected_length
-
-
-@pytest.fixture
-def setup_data_for_all_zeros():
+def setup_data_for_all_zeros(clf_dataset_manager):
     predicted_probs = torch.FloatTensor([[0.9, 0.1], [0.3, 0.7]])
+    datasets_manager = clf_dataset_manager
     labels = torch.LongTensor([1, 0]).view(-1, 1)
-    idx2labelname_mapping = {0: "good class", 1: "bad class"}
 
     expected_precision = {0: 0.0, 1: 0.0}
     expected_recall = {0: 0.0, 1: 0.0}
@@ -107,11 +92,12 @@ def setup_data_for_all_zeros():
     expected_micro_recall = 0.0
     expected_micro_fscore = 0.0
 
-    accuracy = PrecisionRecallFMeasure(idx2labelname_mapping=idx2labelname_mapping)
+    prf_metric = PrecisionRecallFMeasure(datasets_manager=datasets_manager)
     return (
         predicted_probs,
         labels,
-        accuracy,
+        prf_metric,
+        datasets_manager,
         {
             "expected_precision": expected_precision,
             "expected_recall": expected_recall,
@@ -131,23 +117,25 @@ def setup_data_for_all_zeros():
 
 class TestAccuracy:
     def test_print_confusion_matrix_works(self, setup_data_basecase):
-        predicted_probs, labels, accuracy, expected = setup_data_basecase
+        predicted_probs, labels, metric, dataset_manager, expected = setup_data_basecase
         labels_mask = torch.zeros_like(predicted_probs).type(torch.ByteTensor)
-        accuracy.print_confusion_metrics(
-            predicted_probs=predicted_probs, labels=labels, labels_mask=labels_mask
-        )
+        try:
+            metric.print_confusion_metrics(
+                predicted_probs=predicted_probs, labels=labels, labels_mask=labels_mask
+            )
+        except:
+            pytest.fail("Precision Recall and FMeasure print_confusion_metrics fails")
 
     def test_accuracy_basecase(self, setup_data_basecase):
-        predicted_probs, labels, accuracy, expected = setup_data_basecase
+        predicted_probs, _, metric, dataset_manager, expected = setup_data_basecase
         expected_precision = expected["expected_precision"]
         expected_recall = expected["expected_recall"]
         expected_fmeasure = expected["expected_fscore"]
 
-        label_mask = torch.zeros_like(labels).type(torch.ByteTensor)
-        iter_dict = {"label": labels, "label_mask": label_mask}
+        lines, labels = dataset_manager.train_dataset.get_lines_labels()
         forward_dict = {"normalized_probs": predicted_probs}
-        accuracy.calc_metric(iter_dict=iter_dict, model_forward_dict=forward_dict)
-        accuracy_metrics = accuracy.get_metric()
+        metric.calc_metric(lines=lines, labels=labels, model_forward_dict=forward_dict)
+        accuracy_metrics = metric.get_metric()
 
         precision = accuracy_metrics["precision"]
         recall = accuracy_metrics["recall"]
@@ -162,33 +150,8 @@ class TestAccuracy:
         for class_label, fscore_value in fscore.items():
             assert fscore_value == expected_fmeasure[class_label]
 
-    def test_accuracy_one_true_class_missing(self, setup_data_one_true_class_missing):
-        predicted_probs, labels, accuracy, expected = setup_data_one_true_class_missing
-        expected_precision = expected["expected_precision"]
-        expected_recall = expected["expected_recall"]
-        expected_fscore = expected["expected_fscore"]
-
-        label_mask = torch.zeros_like(predicted_probs).type(torch.ByteTensor)
-        iter_dict = {"label": labels, "label_mask": label_mask}
-        forward_dict = {"normalized_probs": predicted_probs}
-        accuracy.calc_metric(iter_dict=iter_dict, model_forward_dict=forward_dict)
-        accuracy_metrics = accuracy.get_metric()
-
-        precision = accuracy_metrics["precision"]
-        recall = accuracy_metrics["recall"]
-        fscore = accuracy_metrics["fscore"]
-
-        for class_label, precision_value in precision.items():
-            assert precision_value == expected_precision[class_label]
-
-        for class_label, recall_value in recall.items():
-            assert recall_value == expected_recall[class_label]
-
-        for class_label, fscore_value in fscore.items():
-            assert fscore_value == expected_fscore[class_label]
-
     def test_macro_scores_basecase(self, setup_data_basecase):
-        predicted_probs, labels, accuracy, expected = setup_data_basecase
+        predicted_probs, _, metric, dataset_manager, expected = setup_data_basecase
         expected_macro_precision = expected["expected_macro_precision"]
         expected_macro_recall = expected["expected_macro_recall"]
         expected_macro_fscore = expected["expected_macro_fscore"]
@@ -199,11 +162,10 @@ class TestAccuracy:
         expected_micro_recall = expected["expected_micro_recall"]
         expected_micro_fscore = expected["expected_micro_fscore"]
 
-        label_mask = torch.zeros_like(predicted_probs).type(torch.ByteTensor)
-        iter_dict = {"label": labels, "label_mask": label_mask}
+        lines, labels = dataset_manager.train_dataset.get_lines_labels()
         forward_dict = {"normalized_probs": predicted_probs}
-        accuracy.calc_metric(iter_dict=iter_dict, model_forward_dict=forward_dict)
-        metrics = accuracy.get_metric()
+        metric.calc_metric(lines=lines, labels=labels, model_forward_dict=forward_dict)
+        metrics = metric.get_metric()
 
         macro_precision = metrics["macro_precision"]
         macro_recall = metrics["macro_recall"]
@@ -226,16 +188,21 @@ class TestAccuracy:
         assert micro_fscore == expected_micro_fscore
 
     def test_accuracy_all_zeros(self, setup_data_for_all_zeros):
-        predicted_probs, labels, accuracy, expected = setup_data_for_all_zeros
+        (
+            predicted_probs,
+            labels,
+            metric,
+            dataset_manager,
+            expected,
+        ) = setup_data_for_all_zeros
         expected_precision = expected["expected_precision"]
         expected_recall = expected["expected_recall"]
         expected_fmeasure = expected["expected_fscore"]
 
-        label_mask = torch.zeros_like(predicted_probs).type(torch.ByteTensor)
-        iter_dict = {"label": labels, "label_mask": label_mask}
+        lines, labels = dataset_manager.train_dataset.get_lines_labels()
         forward_dict = {"normalized_probs": predicted_probs}
-        accuracy.calc_metric(iter_dict=iter_dict, model_forward_dict=forward_dict)
-        accuracy_metrics = accuracy.get_metric()
+        metric.calc_metric(lines=lines, labels=labels, model_forward_dict=forward_dict)
+        accuracy_metrics = metric.get_metric()
 
         precision = accuracy_metrics["precision"]
         recall = accuracy_metrics["recall"]
@@ -251,7 +218,13 @@ class TestAccuracy:
             assert fscore_value == expected_fmeasure[class_]
 
     def test_macro_scores_all_zeros(self, setup_data_for_all_zeros):
-        predicted_probs, labels, accuracy, expected = setup_data_for_all_zeros
+        (
+            predicted_probs,
+            labels,
+            metric,
+            datasets_manager,
+            expected,
+        ) = setup_data_for_all_zeros
         expected_macro_precision = expected["expected_macro_precision"]
         expected_macro_recall = expected["expected_macro_recall"]
         expected_macro_fscore = expected["expected_macro_fscore"]
@@ -262,11 +235,10 @@ class TestAccuracy:
         expected_num_fps = expected["expected_num_fps"]
         expected_num_fns = expected["expected_num_fns"]
 
-        label_mask = torch.zeros_like(predicted_probs).type(torch.ByteTensor)
-        iter_dict = {"label": labels, "label_mask": label_mask}
+        lines, labels = datasets_manager.train_dataset.get_lines_labels()
         forward_dict = {"normalized_probs": predicted_probs}
-        accuracy.calc_metric(iter_dict=iter_dict, model_forward_dict=forward_dict)
-        accuracy_metrics = accuracy.get_metric()
+        metric.calc_metric(lines=lines, labels=labels, model_forward_dict=forward_dict)
+        accuracy_metrics = metric.get_metric()
 
         macro_precision = accuracy_metrics["macro_precision"]
         macro_recall = accuracy_metrics["macro_recall"]
diff --git a/tests/models/test_parscit_tagger.py b/tests/models/test_parscit_tagger.py
index 0fcd766..e7f9045 100644
--- a/tests/models/test_parscit_tagger.py
+++ b/tests/models/test_parscit_tagger.py
@@ -1,7 +1,7 @@
 import pytest
 from sciwing.models.parscit_tagger import ParscitTagger
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 import itertools
@@ -43,9 +43,9 @@ def setup_parscit_tagger(request):
     labels = torch.LongTensor(labels)
     char_tokens = torch.LongTensor(char_tokens)
 
-    embedder = VanillaEmbedder(embedding=EMBEDDING, embedding_dim=EMBEDDING_DIM)
+    embedder = WordEmbedder(embedding=EMBEDDING, embedding_dim=EMBEDDING_DIM)
     if HAVE_CHARACTER_ENCODER:
-        char_embedder = VanillaEmbedder(
+        char_embedder = WordEmbedder(
             embedding=CHARACTER_EMBEDDING, embedding_dim=CHARACTER_EMBEDDING_DIM
         )
         char_encoder = CharLSTMEncoder(
diff --git a/tests/models/test_science_ie_tagger.py b/tests/models/test_science_ie_tagger.py
index c2986d5..07d632b 100644
--- a/tests/models/test_science_ie_tagger.py
+++ b/tests/models/test_science_ie_tagger.py
@@ -2,7 +2,7 @@ import pytest
 from sciwing.models.science_ie_tagger import ScienceIETagger
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
 from sciwing.modules.charlstm_encoder import CharLSTMEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
+from sciwing.modules.embedders.vanilla_embedder import WordEmbedder
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
 from allennlp.modules.conditional_random_field import allowed_transitions
 from sciwing.datasets.seq_labeling.science_ie_dataset import ScienceIEDataset
@@ -83,10 +83,10 @@ def setup_science_ie_tagger(request):
         constraint_type="BIOUL", labels=material_idx2classnames
     )
 
-    embedder = VanillaEmbedder(embedding=EMBEDDING, embedding_dim=EMBEDDING_DIM)
+    embedder = WordEmbedder(embedding=EMBEDDING, embedding_dim=EMBEDDING_DIM)
 
     if HAVE_CHARACTER_ENCODER:
-        char_embedder = VanillaEmbedder(
+        char_embedder = WordEmbedder(
             embedding=CHARACTER_EMBEDDING, embedding_dim=CHARACTER_EMBEDDING_DIM
         )
         char_encoder = CharLSTMEncoder(
diff --git a/tests/models/test_simple_classifier.py b/tests/models/test_simple_classifier.py
index 27dcf1a..dbc1da7 100644
--- a/tests/models/test_simple_classifier.py
+++ b/tests/models/test_simple_classifier.py
@@ -1,96 +1,122 @@
 import pytest
-
-import torch
 from sciwing.modules.bow_encoder import BOW_Encoder
 from sciwing.models.simpleclassifier import SimpleClassifier
-from sciwing.metrics.precision_recall_fmeasure import PrecisionRecallFMeasure
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from torch.nn import Embedding
-import numpy as np
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+from sciwing.modules.embedders.word_embedder import WordEmbedder
 from sciwing.utils.class_nursery import ClassNursery
 
 
-@pytest.fixture
-def setup_simple_classifier():
-    BATCH_SIZE = 1
-    NUM_TOKENS = 3
-    EMB_DIM = 300
-    VOCAB_SIZE = 10
-    NUM_CLASSES = 3
-    embedding = Embedding.from_pretrained(torch.zeros([VOCAB_SIZE, EMB_DIM]))
-    embedder = VanillaEmbedder(embedding_dim=EMB_DIM, embedding=embedding)
-    labels = torch.LongTensor([[1]])
-    encoder = BOW_Encoder(
-        emb_dim=EMB_DIM, embedder=embedder, dropout_value=0, aggregation_type="sum"
+@pytest.fixture(scope="session")
+def clf_dataset_manager(tmpdir_factory):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
+
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
+
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("test_line1###label1\ntest_line2###label2")
+
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+        batch_size=1,
     )
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, NUM_TOKENS))
-    tokens = torch.LongTensor(tokens)
-    simple_classifier = SimpleClassifier(
+
+    return clf_dataset_manager
+
+
+@pytest.fixture
+def setup_simple_classifier(clf_dataset_manager):
+    datasets_manager = clf_dataset_manager
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
+    encoder = BOW_Encoder(embedder=embedder)
+    classifier = SimpleClassifier(
         encoder=encoder,
-        encoding_dim=EMB_DIM,
-        num_classes=NUM_CLASSES,
-        classification_layer_bias=False,
+        encoding_dim=50,
+        num_classes=2,
+        datasets_manager=datasets_manager,
+        classification_layer_bias=True,
     )
-    iter_dict = {"tokens": tokens, "label": labels}
-    return iter_dict, simple_classifier, BATCH_SIZE, NUM_CLASSES
+    train_dataset = datasets_manager.train_dataset
+    lines, labels = train_dataset.get_lines_labels()
+
+    return classifier, lines, labels
 
 
 class TestSimpleClassifier:
-    def test_classifier_produces_0_logits_for_0_embedding(
-        self, setup_simple_classifier
-    ):
-        iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
-        output = simple_classifier(
-            iter_dict, is_training=True, is_validation=False, is_test=False
+    def test_classifier_logits_shape(self, setup_simple_classifier):
+        classifier, lines, labels = setup_simple_classifier
+        output = classifier(
+            lines=lines,
+            labels=labels,
+            is_training=True,
+            is_validation=False,
+            is_test=False,
         )
         logits = output["logits"]
-        expected_logits = torch.zeros([batch_size, num_classes])
-        assert torch.allclose(logits, expected_logits)
-
-    def test_classifier_produces_equal_probs_for_0_embedding(
-        self, setup_simple_classifier
-    ):
-        iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
-        output = simple_classifier(
-            iter_dict, is_training=True, is_validation=False, is_test=False
-        )
-        probs = output["normalized_probs"]
-        expected_probs = torch.ones([batch_size, num_classes]) / num_classes
-        assert torch.allclose(probs, expected_probs)
-
-    def test_classifier_produces_correct_initial_loss_for_0_embedding(
-        self, setup_simple_classifier
-    ):
-        iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
-        output = simple_classifier(
-            iter_dict, is_training=True, is_validation=False, is_test=False
-        )
-        loss = output["loss"].item()
-        correct_loss = -np.log(1 / num_classes)
-        assert torch.allclose(torch.Tensor([loss]), torch.Tensor([correct_loss]))
+        assert logits.size(0) == 2
+        assert logits.size(1) == 2
 
-    def test_classifier_produces_correct_precision(self, setup_simple_classifier):
-        iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
-        output = simple_classifier(
-            iter_dict, is_training=True, is_validation=False, is_test=False
-        )
-        idx2labelname_mapping = {0: "good class", 1: "bad class", 2: "average_class"}
-        metrics_calc = PrecisionRecallFMeasure(
-            idx2labelname_mapping=idx2labelname_mapping
+    def test_classifier_normalized_probs_shape(self, setup_simple_classifier):
+        classifier, lines, labels = setup_simple_classifier
+        output = classifier(
+            lines=lines,
+            labels=labels,
+            is_training=True,
+            is_validation=False,
+            is_test=False,
         )
+        normalized_probs = output["normalized_probs"]
+        assert normalized_probs.size(0) == 2 and normalized_probs.size(1) == 2
 
-        metrics_calc.calc_metric(iter_dict=iter_dict, model_forward_dict=output)
-        metrics = metrics_calc.get_metric()
-        precision = metrics["precision"]
-
-        # NOTE: topk returns the last value in the dimension incase
-        # all the values are equal.
-        expected_precision = {1: 0, 2: 0}
-
-        assert len(precision) == 2
+    # def test_classifier_produces_equal_probs_for_0_embedding(
+    #     self, setup_simple_classifier
+    # ):
+    #     iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
+    #     output = simple_classifier(
+    #         iter_dict, is_training=True, is_validation=False, is_test=False
+    #     )
+    #     probs = output["normalized_probs"]
+    #     expected_probs = torch.ones([batch_size, num_classes]) / num_classes
+    #     assert torch.allclose(probs, expected_probs)
+    #
+    # def test_classifier_produces_correct_initial_loss_for_0_embedding(
+    #     self, setup_simple_classifier
+    # ):
+    #     iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
+    #     output = simple_classifier(
+    #         iter_dict, is_training=True, is_validation=False, is_test=False
+    #     )
+    #     loss = output["loss"].item()
+    #     correct_loss = -np.log(1 / num_classes)
+    #     assert torch.allclose(torch.Tensor([loss]), torch.Tensor([correct_loss]))
 
-        for class_label, precision_value in precision.items():
-            assert precision_value == expected_precision[class_label]
+    # def test_classifier_produces_correct_precision(self, setup_simple_classifier):
+    #     iter_dict, simple_classifier, batch_size, num_classes = setup_simple_classifier
+    #     output = simple_classifier(
+    #         iter_dict, is_training=True, is_validation=False, is_test=False
+    #     )
+    #     idx2labelname_mapping = {0: "good class", 1: "bad class", 2: "average_class"}
+    #     metrics_calc = PrecisionRecallFMeasure(
+    #         idx2labelname_mapping=idx2labelname_mapping
+    #     )
+    #
+    #     metrics_calc.calc_metric(iter_dict=iter_dict, model_forward_dict=output)
+    #     metrics = metrics_calc.get_metric()
+    #     precision = metrics["precision"]
+    #
+    #     # NOTE: topk returns the last value in the dimension incase
+    #     # all the values are equal.
+    #     expected_precision = {1: 0, 2: 0}
+    #
+    #     assert len(precision) == 2
+    #
+    #     for class_label, precision_value in precision.items():
+    #         assert precision_value == expected_precision[class_label]
 
     def test_simple_classifier_in_class_nursery(self):
         assert ClassNursery.class_nursery.get("SimpleClassifier") is not None
diff --git a/tests/modules/embedders/test_bert_embedder.py b/tests/modules/embedders/test_bert_embedder.py
index 4867d9d..124817f 100644
--- a/tests/modules/embedders/test_bert_embedder.py
+++ b/tests/modules/embedders/test_bert_embedder.py
@@ -2,84 +2,76 @@ import pytest
 from sciwing.modules.embedders.bert_embedder import BertEmbedder
 import itertools
 from sciwing.utils.common import get_system_mem_in_gb
+from sciwing.data.line import Line
+import torch
 
-bert_base_types = [
+bert_types = [
     "bert-base-uncased",
     "bert-base-cased",
     "scibert-base-cased",
     "scibert-sci-cased",
     "scibert-base-uncased",
     "scibert-sci-uncased",
+    "bert-large-uncased",
+    "bert-large-cased",
 ]
 
-bert_large_types = ["bert-large-uncased", "bert-large-cased"]
-
 aggregation_types = ["sum", "average"]
 
-bert_base_type_agg_type = list(itertools.product(bert_base_types, aggregation_types))
-bert_large_type_agg_type = list(itertools.product(bert_large_types, aggregation_types))
+bert_type_aggregation = list(itertools.product(bert_types, aggregation_types))
+
 
 system_memory = get_system_mem_in_gb()
 system_memory = int(system_memory)
 
 
-@pytest.fixture(scope="module", params=bert_base_type_agg_type)
+@pytest.fixture(params=bert_type_aggregation)
 def setup_bert_embedder(request):
-    emb_dim = 768
     dropout_value = 0.0
+    bert_type, aggregation_type = request.param
 
-    bow_bert_encoder = BertEmbedder(
-        emb_dim=emb_dim,
+    bert_embedder = BertEmbedder(
         dropout_value=dropout_value,
-        aggregation_type=request.param[1],
-        bert_type=request.param[0],
+        aggregation_type=aggregation_type,
+        bert_type=bert_type,
     )
     strings = [
         "Lets start by talking politics",
         "there are radical ways to test your code",
     ]
-    iter_dict = {"raw_instance": strings}
 
-    return bow_bert_encoder, iter_dict
+    lines = []
+    for string in strings:
+        line = Line(text=string)
+        lines.append(line)
 
-
-@pytest.fixture(scope="module", params=bert_large_type_agg_type)
-def setup_bert_embedder_large(request):
-    emb_dim = 1024
-    dropout_value = 0.0
-
-    bow_bert_encoder = BertEmbedder(
-        emb_dim=emb_dim,
-        dropout_value=dropout_value,
-        aggregation_type=request.param[1],
-        bert_type=request.param[0],
-    )
-    strings = [
-        "Lets start by talking politics",
-        "there are radical ways to test your code",
-    ]
-    iter_dict = {"raw_instance": strings}
-    return bow_bert_encoder, iter_dict
+    return bert_embedder, lines
 
 
 @pytest.mark.skipif(
-    system_memory < 20, reason="System memory too small to run BERT Embedder"
+    system_memory < 10, reason="System memory too small to run testing for BertEmbedder"
 )
 class TestBertEmbedder:
-    def test_bert_embedder_base_type(self, setup_bert_embedder):
+    def test_embedder_dimensions(self, setup_bert_embedder):
         """
             The bow bert encoder should return a single instance
             that is the sum of the word embeddings of the instance
         """
-        bert_embedder, iter_dict = setup_bert_embedder
-        encoding = bert_embedder(iter_dict)
-        assert encoding.size() == (2, 8, 768)
+        bert_embedder, lines = setup_bert_embedder
+        encoding = bert_embedder(lines)
+        lens = [len(line.tokens["tokens"]) for line in lines]
+        max_word_len = max(lens)
+        assert encoding.size(0) == 2
+        assert encoding.size(2) == bert_embedder.get_embedding_dimension()
+        assert encoding.size(1) == max_word_len
 
-    def test_bert_embedder_encoder_large_type(self, setup_bert_embedder_large):
-        """
-            The bow bert encoder should return a single instance
-            that is the sum of the word embeddings of the instance
-        """
-        bert_embedder, iter_dict = setup_bert_embedder_large
-        encoding = bert_embedder(iter_dict)
-        assert encoding.size() == (2, 8, 1024)
+    def test_bert_embedder_tokens(self, setup_bert_embedder):
+        bert_embedder, lines = setup_bert_embedder
+        _ = bert_embedder(lines)
+        emb_dim = bert_embedder.get_embedding_dimension()
+        emb_name = bert_embedder.embedder_name
+        for line in lines:
+            tokens = line.tokens[bert_embedder.word_tokens_namespace]
+            for token in tokens:
+                assert isinstance(token.get_embedding(emb_name), torch.FloatTensor)
+                assert token.get_embedding(emb_name).size(0) == emb_dim
diff --git a/tests/modules/embedders/test_bow_elmo_embedder.py b/tests/modules/embedders/test_bow_elmo_embedder.py
index 3362963..ec95d56 100644
--- a/tests/modules/embedders/test_bow_elmo_embedder.py
+++ b/tests/modules/embedders/test_bow_elmo_embedder.py
@@ -1,7 +1,8 @@
 import pytest
 from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
-from sciwing.utils.common import pack_to_length
+from sciwing.data.line import Line
 from sciwing.utils.common import get_system_mem_in_gb
+import torch
 
 mem_gb = get_system_mem_in_gb()
 mem_gb = int(mem_gb)
@@ -11,18 +12,30 @@ mem_gb = int(mem_gb)
 @pytest.fixture(params=["sum", "average", "first", "last"])
 def setup_bow_elmo_encoder(request):
     layer_aggregation = request.param
-    instances = ["I like to eat carrot", "I like to go out on long drives in a car"]
-    padded_instances = []
-    for instance in instances:
-        padded_inst = pack_to_length(tokenized_text=instance.split(), max_length=10)
-        padded_instances.append(" ".join(padded_inst))
-    iter_dict = {"instance": padded_instances}
+    strings = ["I like to eat carrot", "I like to go out on long drives in a car"]
+
+    lines = []
+    for string in strings:
+        line = Line(text=string)
+        lines.append(line)
+
     bow_elmo_embedder = BowElmoEmbedder(layer_aggregation=layer_aggregation)
-    return bow_elmo_embedder, iter_dict
+    return bow_elmo_embedder, lines
 
 
 class TestBowElmoEncoder:
     def test_dimension(self, setup_bow_elmo_encoder):
-        bow_elmo_embedder, iter_dict = setup_bow_elmo_encoder
-        embedding = bow_elmo_embedder(iter_dict)
-        assert embedding.size() == (2, 10, 1024)
+        bow_elmo_embedder, lines = setup_bow_elmo_encoder
+        embedding = bow_elmo_embedder(lines)
+        assert embedding.size(0) == 2
+        assert embedding.size(2) == 1024
+
+    def test_token_embeddings(self, setup_bow_elmo_encoder):
+        bow_elmo_embedder, lines = setup_bow_elmo_encoder
+        _ = bow_elmo_embedder(lines)
+
+        for line in lines:
+            tokens = line.tokens["tokens"]
+            for token in tokens:
+                assert isinstance(token.get_embedding("elmo"), torch.FloatTensor)
+                assert token.get_embedding("elmo").size(0) == 1024
diff --git a/tests/modules/embedders/test_char_embedder.py b/tests/modules/embedders/test_char_embedder.py
new file mode 100644
index 0000000..8b2e2fc
--- /dev/null
+++ b/tests/modules/embedders/test_char_embedder.py
@@ -0,0 +1,69 @@
+import pytest
+from sciwing.modules.embedders.char_embedder import CharEmbedder
+from sciwing.data.line import Line
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
+from sciwing.datasets.classification.text_classification_dataset import (
+    TextClassificationDatasetManager,
+)
+import torch
+
+
+@pytest.fixture(scope="session")
+def clf_dataset_manager(tmpdir_factory):
+    train_file = tmpdir_factory.mktemp("train_data").join("train_file.txt")
+    train_file.write("train_line1###label1\ntrain_line2###label2")
+
+    dev_file = tmpdir_factory.mktemp("dev_data").join("dev_file.txt")
+    dev_file.write("dev_line1###label1\ndev_line2###label2")
+
+    test_file = tmpdir_factory.mktemp("test_data").join("test_file.txt")
+    test_file.write("test_line1###label1\ntest_line2###label2")
+
+    clf_dataset_manager = TextClassificationDatasetManager(
+        train_filename=str(train_file),
+        dev_filename=str(dev_file),
+        test_filename=str(test_file),
+        batch_size=1,
+    )
+
+    return clf_dataset_manager
+
+
+@pytest.fixture(params=[(10, 100)])
+def setup_char_embedder(request, clf_dataset_manager):
+    char_embedding_dim, hidden_dim = request.param
+    datset_manager = clf_dataset_manager
+    embedder = CharEmbedder(
+        char_embedding_dimension=char_embedding_dim,
+        hidden_dimension=hidden_dim,
+        datasets_manager=datset_manager,
+    )
+    texts = ["This is sentence", "This is another sentence"]
+    lines = []
+    for text in texts:
+        line = Line(
+            text=text,
+            tokenizers={"tokens": WordTokenizer(), "char_tokens": CharacterTokenizer()},
+        )
+        lines.append(line)
+
+    return embedder, lines
+
+
+class TestCharEmbedder:
+    def test_encoding_dimension(self, setup_char_embedder):
+        embedder, lines = setup_char_embedder
+        embedded = embedder(lines)
+        assert embedded.size(0) == 2
+        assert embedded.size(2) == embedder.hidden_dimension * 2
+
+    def test_embedding_set_lines(self, setup_char_embedder):
+        embedder, lines = setup_char_embedder
+        _ = embedder(lines)
+        for line in lines:
+            tokens = line.tokens["tokens"]
+            for token in tokens:
+                assert isinstance(
+                    token.get_embedding("char_embedding"), torch.FloatTensor
+                )
diff --git a/tests/modules/embedders/test_concat_embedders.py b/tests/modules/embedders/test_concat_embedders.py
index 48ee24e..77b33f8 100644
--- a/tests/modules/embedders/test_concat_embedders.py
+++ b/tests/modules/embedders/test_concat_embedders.py
@@ -1,96 +1,38 @@
 import pytest
 from sciwing.modules.embedders.concat_embedders import ConcatEmbedders
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-from sciwing.modules.embedders.bow_elmo_embedder import BowElmoEmbedder
-import torch.nn as nn
-import copy
-import torch
-
-
-@pytest.fixture()
-def vanilla_embedder():
-    embedding = nn.Embedding(1000, 100)
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=100)
-    return embedder, {"EMBEDDING_DIM": 100, "VOCAB_SIZE": 1000}
-
-
-@pytest.fixture()
-def bow_elmo_embedder():
-    embedder = BowElmoEmbedder()
-    instance = ["This is a string"]
-    return embedder, {"instance": instance}
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.data.line import Line
+from sciwing.tokenizers.word_tokenizer import WordTokenizer
+from sciwing.tokenizers.character_tokenizer import CharacterTokenizer
 
 
 @pytest.fixture
-def concat_vanilla_embedders(vanilla_embedder):
-    first_embedder, options = vanilla_embedder
-    second_embedder = copy.deepcopy(first_embedder)
-    third_embedder = copy.deepcopy(second_embedder)
-    EMBEDDING_DIM = options["EMBEDDING_DIM"]
-    VOCAB_SIZE = options["VOCAB_SIZE"]
-    BATCH_SIZE = 32
-    MAX_TIME_STEPS = 10
-    EXPECTED_EMBEDDING_DIM = EMBEDDING_DIM * 3
-    concat_embedder = ConcatEmbedders([first_embedder, second_embedder, third_embedder])
-    iter_dict = {
-        "tokens": torch.randint(0, VOCAB_SIZE, size=(BATCH_SIZE, MAX_TIME_STEPS))
-    }
-    return (
-        concat_embedder,
-        {
-            "EXPECTED_EMBEDDING_DIM": EXPECTED_EMBEDDING_DIM,
-            "BATCH_SIZE": BATCH_SIZE,
-            "MAX_TIME_STEPS": MAX_TIME_STEPS,
-            "iter_dict": iter_dict,
-        },
-    )
+def setup_lines():
+    texts = ["First sentence", "Second Sentence"]
+    lines = []
+    for text in texts:
+        line = Line(
+            text=text,
+            tokenizers={"tokens": WordTokenizer(), "char_tokens": CharacterTokenizer()},
+        )
+        lines.append(line)
+    return lines
 
 
 @pytest.fixture
-def concat_vanilla_bow_elmo_embedders(vanilla_embedder, bow_elmo_embedder):
-    vanilla_word_embedder, vanilla_embedder_options = vanilla_embedder
-    bow_elmo_word_embedder, bow_elmo_embedder_options = bow_elmo_embedder
-    embedders = [vanilla_word_embedder, bow_elmo_word_embedder]
-    VANILLA_EMBEDDING_DIM = vanilla_embedder_options["EMBEDDING_DIM"]
-    VOCAB_SIZE = vanilla_embedder_options["VOCAB_SIZE"]
-    BATCH_SIZE = 1
-    MAX_TIME_STEPS = 4
-    instance = bow_elmo_embedder_options["instance"]
-    EXPECTED_EMBEDDING_DIM = VANILLA_EMBEDDING_DIM + 1024
-    concat_embedder = ConcatEmbedders(embedders)
-    iter_dict = {
-        "tokens": torch.randint(0, VOCAB_SIZE, size=(BATCH_SIZE, MAX_TIME_STEPS)),
-        "instance": instance,
-    }
-
-    return (
-        concat_embedder,
-        {
-            "iter_dict": iter_dict,
-            "EXPECTED_EMBEDDING_DIM": EXPECTED_EMBEDDING_DIM,
-            "BATCH_SIZE": BATCH_SIZE,
-            "MAX_TIME_STEPS": MAX_TIME_STEPS,
-        },
-    )
+def setup_concat_vanilla_embedders():
+    word_embedder_1 = WordEmbedder(embedding_type="glove_6B_50")
+    word_embedder_2 = WordEmbedder(embedding_type="glove_6B_100")
+    embedder = ConcatEmbedders([word_embedder_1, word_embedder_2])
+    return embedder
 
 
 class TestConcatEmbedders:
-    def test_concat_vanilla_embedders_dim(self, concat_vanilla_embedders):
-        embedder, options = concat_vanilla_embedders
-        iter_dict = options["iter_dict"]
-        batch_size = options["BATCH_SIZE"]
-        time_steps = options["MAX_TIME_STEPS"]
-        expected_emb_dim = options["EXPECTED_EMBEDDING_DIM"]
-        embedding = embedder(iter_dict=iter_dict)
-        assert embedding.size() == (batch_size, time_steps, expected_emb_dim)
-
-    def test_concat_vanilla_bow_elmo_embedders_dim(
-        self, concat_vanilla_bow_elmo_embedders
+    def test_concat_vanilla_embedders_dim(
+        self, setup_concat_vanilla_embedders, setup_lines
     ):
-        embedder, options = concat_vanilla_bow_elmo_embedders
-        iter_dict = options["iter_dict"]
-        batch_size = options["BATCH_SIZE"]
-        time_steps = options["MAX_TIME_STEPS"]
-        expected_emb_dim = options["EXPECTED_EMBEDDING_DIM"]
-        embedding = embedder(iter_dict=iter_dict)
-        assert embedding.size() == (batch_size, time_steps, expected_emb_dim)
+        concat_embedder = setup_concat_vanilla_embedders
+        lines = setup_lines
+        embedding = concat_embedder(lines)
+        assert embedding.size(0) == 2
+        assert embedding.size(2) == 150
diff --git a/tests/modules/embedders/test_vanilla_embedder.py b/tests/modules/embedders/test_vanilla_embedder.py
deleted file mode 100644
index 1484731..0000000
--- a/tests/modules/embedders/test_vanilla_embedder.py
+++ /dev/null
@@ -1,43 +0,0 @@
-import pytest
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import torch.nn as nn
-import torch
-from sciwing.utils.class_nursery import ClassNursery
-
-
-@pytest.fixture
-def embedder():
-    batch_size = 32
-    time_steps = 10
-    vocab_size = 3000
-    embedding_dim = 300
-
-    embedding = nn.Embedding.from_pretrained(
-        embeddings=torch.rand(vocab_size, embedding_dim), freeze=False
-    )
-    tokens = torch.LongTensor(
-        torch.randint(0, vocab_size, size=(batch_size, time_steps))
-    )
-    options = {
-        "tokens": tokens,
-        "embedding_size": embedding_dim,
-        "batch_size": batch_size,
-        "time_steps": time_steps,
-    }
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=embedding_dim)
-    return embedder, options
-
-
-class TestVanillaEmbedder:
-    def test_dimension(self, embedder):
-        embedder, options = embedder
-        iter_dict = {"tokens": options["tokens"]}
-        embedding_size = options["embedding_size"]
-        batch_size = options["batch_size"]
-        time_steps = options["time_steps"]
-
-        embedding = embedder(iter_dict=iter_dict)
-        assert embedding.size() == (batch_size, time_steps, embedding_size)
-
-    def test_vanilla_embedder_in_class_nursery(self):
-        assert ClassNursery.class_nursery["VanillaEmbedder"] is not None
diff --git a/tests/modules/embedders/test_word_embedder.py b/tests/modules/embedders/test_word_embedder.py
new file mode 100644
index 0000000..19c71b0
--- /dev/null
+++ b/tests/modules/embedders/test_word_embedder.py
@@ -0,0 +1,44 @@
+import pytest
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.data.line import Line
+from sciwing.utils.class_nursery import ClassNursery
+import torch
+
+
+@pytest.fixture(
+    params=["glove_6B_50", "glove_6B_100", "glove_6B_200", "glove_6B_300", "parscit"]
+)
+def setup_embedder(request):
+    embedding_type = request.param
+    embedder = WordEmbedder(embedding_type)
+    return embedder
+
+
+@pytest.fixture
+def setup_lines():
+    texts = ["first line", "second line"]
+    lines = []
+    for text in texts:
+        line = Line(text=text)
+        lines.append(line)
+    return lines
+
+
+class TestWordEmbedder:
+    def test_dimension(self, setup_embedder, setup_lines):
+        embedder = setup_embedder
+        lines = setup_lines
+        _ = embedder(lines)
+        for line in lines:
+            for token in line.tokens["tokens"]:
+                embedding = token.get_embedding(name=embedder.embedding_type)
+                assert isinstance(embedding, torch.FloatTensor)
+
+    def test_final_embedding_size(self, setup_embedder, setup_lines):
+        embedder = setup_embedder
+        lines = setup_lines
+        embeddings = embedder(lines)
+        assert embeddings.size(0) == 2
+
+    def test_vanilla_embedder_in_class_nursery(self):
+        assert ClassNursery.class_nursery["WordEmbedder"] is not None
diff --git a/tests/modules/test_bow_encoder.py b/tests/modules/test_bow_encoder.py
index 6fcefa0..7758890 100644
--- a/tests/modules/test_bow_encoder.py
+++ b/tests/modules/test_bow_encoder.py
@@ -1,67 +1,31 @@
 import numpy as np
 import pytest
-import torch
-import torch.nn as nn
 from sciwing.modules.bow_encoder import BOW_Encoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import itertools
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.data.line import Line
 from sciwing.utils.class_nursery import ClassNursery
 
-aggregation_types = ["sum", "average"]
-embedding_type = ["zeros", "ones"]
 
-params = itertools.product(aggregation_types, embedding_type)
+@pytest.fixture(params=["sum", "average"])
+def setup_bow_encoder(request):
+    aggregation_type = request.param
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
+    encoder = BOW_Encoder(embedder=embedder, aggregation_type=aggregation_type)
+    texts = ["First sentence", "second sentence"]
+    lines = []
+    for text in texts:
+        line = Line(text=text)
+        lines.append(line)
 
-
-@pytest.fixture(params=params)
-def setup_zero_embeddings(request):
-    EMB_DIM = 300
-    VOCAB_SIZE = 10
-    BATCH_SIZE = 10
-    aggregation_type = request.param[0]
-    embedding_type = request.param[1]
-    embedding = None
-    if embedding_type == "zeros":
-        embedding = torch.zeros([VOCAB_SIZE, EMB_DIM])
-    elif embedding_type == "ones":
-        embedding = torch.ones([VOCAB_SIZE, EMB_DIM])
-    embedding = nn.Embedding.from_pretrained(embedding)
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMB_DIM)
-    encoder = BOW_Encoder(
-        emb_dim=EMB_DIM, embedder=embedder, aggregation_type=aggregation_type
-    )
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, EMB_DIM))
-    tokens = torch.LongTensor(tokens)
-    iter_dict = {"tokens": tokens}
-    options = {
-        "EMB_DIM": EMB_DIM,
-        "VOCAB_SIZE": VOCAB_SIZE,
-        "BATCH_SIZE": BATCH_SIZE,
-        "EMBEDDING_TYPE": embedding_type,
-        "AGGREGATION_TYPE": aggregation_type,
-    }
-    return encoder, iter_dict, options
+    return encoder, lines
 
 
 class TestBOWEncoder:
-    def test_bow_encoder_zeros(self, setup_zero_embeddings):
-        encoder, iter_dict, options = setup_zero_embeddings
-        BATCH_SIZE = options["BATCH_SIZE"]
-        EMB_DIM = options["EMB_DIM"]
-        EMBEDDING_TYPE = options["EMBEDDING_TYPE"]
-        AGGREGATION_TYPE = options["AGGREGATION_TYPE"]
-        embeddings = encoder(iter_dict=iter_dict)
-        if EMBEDDING_TYPE == "zeros" and AGGREGATION_TYPE == "average":
-            expected_embedding = torch.zeros([BATCH_SIZE, EMB_DIM])
-        elif EMBEDDING_TYPE == "zeros" and AGGREGATION_TYPE == "sum":
-            expected_embedding = torch.zeros([BATCH_SIZE, EMB_DIM])
-        elif EMBEDDING_TYPE == "ones" and AGGREGATION_TYPE == "average":
-            expected_embedding = torch.ones([BATCH_SIZE, EMB_DIM])
-        elif EMBEDDING_TYPE == "ones" and AGGREGATION_TYPE == "sum":
-            expected_embedding = torch.ones([BATCH_SIZE, EMB_DIM]) * EMB_DIM
-
-        assert embeddings.size() == (BATCH_SIZE, EMB_DIM)
-        assert torch.all(torch.eq(embeddings, expected_embedding)).item()
+    def test_bow_encoder_dimensions(self, setup_bow_encoder):
+        encoder, lines = setup_bow_encoder
+        encoded_lines = encoder(lines)
+        assert encoded_lines.size(0) == 2
+        assert encoded_lines.size(1) == 50
 
     def test_bow_encoder_in_class_nursery(self):
         assert ClassNursery.class_nursery.get("BOW_Encoder") is not None
diff --git a/tests/modules/test_elmo_lstm_encoder.py b/tests/modules/test_elmo_lstm_encoder.py
deleted file mode 100644
index c752214..0000000
--- a/tests/modules/test_elmo_lstm_encoder.py
+++ /dev/null
@@ -1,126 +0,0 @@
-import pytest
-from sciwing.modules.elmo_lstm_encoder import ElmoLSTMEncoder
-from sciwing.modules.embedders.elmo_embedder import ElmoEmbedder
-import torch
-import torch.nn as nn
-import numpy as np
-
-
-@pytest.fixture
-def setup_test_elmo_lstm_encoder_concat():
-    elmo_embedder = ElmoEmbedder()
-    elmo_emb_dim = 1024
-    emb_dim = 100
-    VOCAB_SIZE = 10
-    BATCH_SIZE = 1
-    NUM_TOKENS = 5
-    hidden_dim = 1024
-    BIDIRECTIONAL = True
-    COMBINE_STRATEGY = "concat"
-    RNN_BIAS = True
-    instances = ["i like to test this".split()]
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, NUM_TOKENS))
-    tokens = torch.LongTensor(tokens)
-    labels = torch.LongTensor([[1]])
-    embedding = torch.nn.Embedding(VOCAB_SIZE, emb_dim)
-
-    elmo_lstm_encoder = ElmoLSTMEncoder(
-        elmo_emb_dim=elmo_emb_dim,
-        elmo_embedder=elmo_embedder,
-        emb_dim=emb_dim,
-        embedding=embedding,
-        dropout_value=0.0,
-        hidden_dim=hidden_dim,
-        bidirectional=BIDIRECTIONAL,
-        combine_strategy=COMBINE_STRATEGY,
-        rnn_bias=RNN_BIAS,
-    )
-
-    return (
-        tokens,
-        labels,
-        instances,
-        elmo_lstm_encoder,
-        {
-            "ELMO_EMB_DIM": 1024,
-            "EMB_DIM": emb_dim,
-            "VOCAB_SIZE": VOCAB_SIZE,
-            "BATCH_SIZE": BATCH_SIZE,
-            "BIDIRECTIONAL": BIDIRECTIONAL,
-            "COMBINE_STRATEGY": COMBINE_STRATEGY,
-            "RNN_BIAS": RNN_BIAS,
-            "HIDDEN_DIM": hidden_dim,
-        },
-    )
-
-
-@pytest.fixture
-def setup_test_elmo_lstm_encoder_sum():
-    elmo_embedder = ElmoEmbedder()
-    elmo_emb_dim = 1024
-    emb_dim = 100
-    VOCAB_SIZE = 10
-    BATCH_SIZE = 1
-    NUM_TOKENS = 5
-    hidden_dim = 1024
-    BIDIRECTIONAL = True
-    COMBINE_STRATEGY = "sum"
-    RNN_BIAS = True
-    instances = ["i like to test this".split()]
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, NUM_TOKENS))
-    tokens = torch.LongTensor(tokens)
-    labels = torch.LongTensor([[1]])
-    embedding = torch.nn.Embedding(VOCAB_SIZE, emb_dim)
-
-    elmo_lstm_encoder = ElmoLSTMEncoder(
-        elmo_emb_dim=elmo_emb_dim,
-        elmo_embedder=elmo_embedder,
-        emb_dim=emb_dim,
-        embedding=embedding,
-        dropout_value=0.0,
-        hidden_dim=hidden_dim,
-        bidirectional=BIDIRECTIONAL,
-        combine_strategy=COMBINE_STRATEGY,
-        rnn_bias=RNN_BIAS,
-    )
-
-    return (
-        tokens,
-        labels,
-        instances,
-        elmo_lstm_encoder,
-        {
-            "ELMO_EMB_DIM": 1024,
-            "EMB_DIM": emb_dim,
-            "VOCAB_SIZE": VOCAB_SIZE,
-            "BATCH_SIZE": BATCH_SIZE,
-            "BIDIRECTIONAL": BIDIRECTIONAL,
-            "COMBINE_STRATEGY": COMBINE_STRATEGY,
-            "RNN_BIAS": RNN_BIAS,
-            "HIDDEN_DIM": hidden_dim,
-        },
-    )
-
-
-class TestElmoLSTMEncoder:
-    def test_encoding_dimension_concat(self, setup_test_elmo_lstm_encoder_concat):
-        tokens, labels, instances, elmo_lstm_encoder, options = (
-            setup_test_elmo_lstm_encoder_concat
-        )
-        encoding = elmo_lstm_encoder(x=tokens, instances=instances)
-        batch_size = options["BATCH_SIZE"]
-        hidden_dim = options["HIDDEN_DIM"]
-
-        # bidirectional concat embedding
-        assert encoding.size() == (batch_size, 2 * hidden_dim)
-
-    def test_encoding_dimension_sum(self, setup_test_elmo_lstm_encoder_sum):
-        tokens, labels, instances, elmo_lstm_encoder, options = (
-            setup_test_elmo_lstm_encoder_sum
-        )
-        encoding = elmo_lstm_encoder(x=tokens, instances=instances)
-        batch_size = options["BATCH_SIZE"]
-        hidden_dim = options["HIDDEN_DIM"]
-
-        # bidirectional concat embedding
-        assert encoding.size() == (batch_size, hidden_dim)
diff --git a/tests/modules/test_lstm2seqencoder.py b/tests/modules/test_lstm2seqencoder.py
index ec8218b..6977e1d 100644
--- a/tests/modules/test_lstm2seqencoder.py
+++ b/tests/modules/test_lstm2seqencoder.py
@@ -1,9 +1,7 @@
 import pytest
 from sciwing.modules.lstm2seqencoder import Lstm2SeqEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import torch
-import torch.nn as nn
-import numpy as np
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.data.line import Line
 import itertools
 
 is_additional_embedding = [True, False]
@@ -13,22 +11,12 @@ lstm2encoder_options = list(lstm2encoder_options)
 
 @pytest.fixture(params=lstm2encoder_options)
 def setup_lstm2seqencoder(request):
-    EMBEDDING_DIM = 100
-    VOCAB_SIZE = 1000
-    BATCH_SIZE = 2
     HIDDEN_DIM = 1024
-    NUM_TIME_STEPS = 10
     BIDIRECTIONAL = request.param[0]
     COMBINE_STRATEGY = request.param[1]
     NUM_LAYERS = request.param[2]
-    EMBEDDING = nn.Embedding.from_pretrained(torch.zeros([VOCAB_SIZE, EMBEDDING_DIM]))
-    tokens = np.random.randint(0, VOCAB_SIZE - 1, size=(BATCH_SIZE, NUM_TIME_STEPS))
-    tokens = torch.LongTensor(tokens)
-
-    embedder = VanillaEmbedder(embedding=EMBEDDING, embedding_dim=EMBEDDING_DIM)
-
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
     encoder = Lstm2SeqEncoder(
-        emb_dim=EMBEDDING_DIM,
         embedder=embedder,
         dropout_value=0.0,
         hidden_dim=HIDDEN_DIM,
@@ -38,21 +26,24 @@ def setup_lstm2seqencoder(request):
         num_layers=NUM_LAYERS,
     )
 
+    lines = []
+    texts = ["First sentence", "second sentence"]
+    for text in texts:
+        line = Line(text=text)
+        lines.append(line)
+
     return (
         encoder,
         {
-            "EMBEDDING_DIM": EMBEDDING_DIM,
-            "VOCAB_SIZE": VOCAB_SIZE,
-            "BATCH_SIZE": BATCH_SIZE,
             "HIDDEN_DIM": HIDDEN_DIM,
             "COMBINE_STRATEGY": COMBINE_STRATEGY,
             "BIDIRECTIONAL": BIDIRECTIONAL,
-            "tokens": tokens,
             "EXPECTED_HIDDEN_DIM": 2 * HIDDEN_DIM
             if COMBINE_STRATEGY == "concat" and BIDIRECTIONAL
             else HIDDEN_DIM,
-            "TIME_STEPS": NUM_TIME_STEPS,
             "NUM_LAYERS": NUM_LAYERS,
+            "LINES": lines,
+            "TIME_STEPS": 2,
         },
     )
 
@@ -60,22 +51,9 @@ def setup_lstm2seqencoder(request):
 class TestLstm2SeqEncoder:
     def test_hidden_dim(self, setup_lstm2seqencoder):
         encoder, options = setup_lstm2seqencoder
-        tokens = options["tokens"]
-        batch_size = options["BATCH_SIZE"]
+        lines = options["LINES"]
         num_time_steps = options["TIME_STEPS"]
         expected_hidden_size = options["EXPECTED_HIDDEN_DIM"]
-        encoding = encoder({"tokens": tokens})
+        encoding = encoder(lines=lines)
+        batch_size = len(lines)
         assert encoding.size() == (batch_size, num_time_steps, expected_hidden_size)
-
-    def test_encoder_produces_zero_encoding(self, setup_lstm2seqencoder):
-        encoder, options = setup_lstm2seqencoder
-        tokens = options["tokens"]
-        batch_size = options["BATCH_SIZE"]
-        num_time_steps = options["TIME_STEPS"]
-        expected_hidden_size = options["EXPECTED_HIDDEN_DIM"]
-        encoding = encoder({"tokens": tokens})
-        assert torch.all(
-            torch.eq(
-                encoding, torch.zeros(batch_size, num_time_steps, expected_hidden_size)
-            )
-        )
diff --git a/tests/modules/test_lstm2vecencoder.py b/tests/modules/test_lstm2vecencoder.py
index 83cf2b1..6c8ea7c 100644
--- a/tests/modules/test_lstm2vecencoder.py
+++ b/tests/modules/test_lstm2vecencoder.py
@@ -2,8 +2,8 @@ import pytest
 import torch
 import torch.nn as nn
 from sciwing.modules.lstm2vecencoder import LSTM2VecEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import numpy as np
+from sciwing.modules.embedders.word_embedder import WordEmbedder
+from sciwing.data.line import Line
 import itertools
 
 directions = [True, False]  # True for bi-directions
@@ -15,22 +15,12 @@ directions_combine_strategy = list(directions_combine_strategy)
 
 @pytest.fixture(params=directions_combine_strategy)
 def setup_lstm2vecencoder(request):
-    emb_dim = 300
-    time_steps = 10
-    vocab_size = 100
-    batch_size = 32
-    embedding = nn.Embedding.from_pretrained(torch.zeros([vocab_size, emb_dim]))
     hidden_dimension = 1024
     combine_strategy = request.param[1]
     bidirectional = request.param[0]
-    tokens = np.random.randint(0, vocab_size - 1, size=(batch_size, time_steps))
-    tokens = torch.LongTensor(tokens)
-
-    iter_dict = {"tokens": tokens}
-    embedder = VanillaEmbedder(embedding=embedding, embedding_dim=emb_dim)
+    embedder = WordEmbedder(embedding_type="glove_6B_50")
 
     encoder = LSTM2VecEncoder(
-        emb_dim=emb_dim,
         embedder=embedder,
         dropout_value=0.0,
         hidden_dim=hidden_dimension,
@@ -39,19 +29,21 @@ def setup_lstm2vecencoder(request):
         rnn_bias=False,
     )
 
+    texts = ["First sentence", "second sentence"]
+    lines = []
+    for text in texts:
+        line = Line(text=text)
+        lines.append(line)
+
     return (
         encoder,
         {
-            "emb_dim": emb_dim,
-            "vocab_size": vocab_size,
             "hidden_dim": 2 * hidden_dimension
             if bidirectional and combine_strategy == "concat"
             else hidden_dimension,
             "bidirectional": False,
             "combine_strategy": combine_strategy,
-            "tokens": tokens,
-            "batch_size": batch_size,
-            "iter_dict": iter_dict,
+            "lines": lines,
         },
     )
 
@@ -61,24 +53,13 @@ class TestLstm2VecEncoder:
         with pytest.raises(AssertionError):
 
             encoder = LSTM2VecEncoder(
-                emb_dim=300,
-                embedder=VanillaEmbedder(nn.Embedding(10, 1024), embedding_dim=1024),
-                combine_strategy="add",
+                embedder=WordEmbedder("glove_6B_50"), combine_strategy="add"
             )
 
-    def test_zero_embedding_dimension(self, setup_lstm2vecencoder):
+    def test_encoding_dimension(self, setup_lstm2vecencoder):
         encoder, options = setup_lstm2vecencoder
         hidden_dim = options["hidden_dim"]
-        batch_size = options["batch_size"]
-        encoding = encoder(iter_dict=options["iter_dict"])
+        lines = options["lines"]
+        batch_size = len(lines)
+        encoding = encoder(lines=lines)
         assert encoding.size() == (batch_size, hidden_dim)
-
-    def test_lstm2vec_encoder(self, setup_lstm2vecencoder):
-        encoder, options = setup_lstm2vecencoder
-        iter_dict = options["iter_dict"]
-        hidden_dim = options["hidden_dim"]
-        batch_size = options["batch_size"]
-        encoding = encoder(iter_dict=iter_dict)
-        assert torch.all(
-            torch.eq(encoding, torch.zeros([batch_size, hidden_dim]))
-        ).item()
diff --git a/tests/modules/test_lstmchar_encoder.py b/tests/modules/test_lstmchar_encoder.py
deleted file mode 100644
index 7664d62..0000000
--- a/tests/modules/test_lstmchar_encoder.py
+++ /dev/null
@@ -1,54 +0,0 @@
-import pytest
-from sciwing.modules.charlstm_encoder import CharLSTMEncoder
-from sciwing.modules.embedders.vanilla_embedder import VanillaEmbedder
-import torch.nn as nn
-import torch
-
-
-@pytest.fixture
-def char_lstm_encoder():
-    VOCAB_SIZE = 150
-    EMBEDDING_DIM = 25
-    BATCH_SIZE = 32
-    NUM_TIME_STEPS = 10
-    MAX_CHAR_SIZE = 5
-    HIDDEN_DIM = 100
-    embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
-    vanilla_embedder = VanillaEmbedder(embedding=embedding, embedding_dim=EMBEDDING_DIM)
-    tokens = torch.randint(
-        0, VOCAB_SIZE, size=(BATCH_SIZE, NUM_TIME_STEPS, MAX_CHAR_SIZE)
-    )
-    iter_dict = {"char_tokens": tokens}
-
-    char_lstm_encoder = CharLSTMEncoder(
-        char_embedder=vanilla_embedder,
-        char_emb_dim=EMBEDDING_DIM,
-        hidden_dim=HIDDEN_DIM,
-        bidirectional=True,
-        combine_strategy="concat",
-    )
-
-    return (
-        char_lstm_encoder,
-        {
-            "iter_dict": iter_dict,
-            "VOCAB_SIZE": VOCAB_SIZE,
-            "EMBEDDING_DIM": EMBEDDING_DIM,
-            "BATCH_SIZE": BATCH_SIZE,
-            "NUM_TIME_STEPS": NUM_TIME_STEPS,
-            "MAX_CHAR_SIZE": MAX_CHAR_SIZE,
-            "HIDDEN_DIM": HIDDEN_DIM,
-            "EXPECTED_HIDDEN_DIM": HIDDEN_DIM * 2,
-        },
-    )
-
-
-class TestCharLSTMEncoder:
-    def test_char_lstm_encoder_dim(self, char_lstm_encoder):
-        encoder, options = char_lstm_encoder
-        iter_dict = options["iter_dict"]
-        BATCH_SIZE = options["BATCH_SIZE"]
-        NUM_TIME_STEPS = options["NUM_TIME_STEPS"]
-        EXPECTED_HIDDEN_DIM = options["EXPECTED_HIDDEN_DIM"]
-        embedding = encoder(iter_dict=iter_dict)
-        assert embedding.size() == (BATCH_SIZE, NUM_TIME_STEPS, EXPECTED_HIDDEN_DIM)
diff --git a/tests/numericalizer/test_numericalizer.py b/tests/numericalizer/test_numericalizer.py
index 037ebb4..91f2c04 100644
--- a/tests/numericalizer/test_numericalizer.py
+++ b/tests/numericalizer/test_numericalizer.py
@@ -1,5 +1,5 @@
 import pytest
-from sciwing.numericalizer.numericalizer import Numericalizer
+from sciwing.numericalizers.numericalizer import Numericalizer
 from sciwing.vocab.vocab import Vocab
 
 
@@ -34,10 +34,26 @@ class TestNumericalizer:
 
     def test_tokens_are_integers(self, single_instance_setup):
         """
-        Just to test that something untoward is happening
+        Just to test that nothing untoward is  happening
         """
         single_instance, numericalizer, vocabulary = single_instance_setup
         numerical_tokens = numericalizer.numericalize_instance(single_instance[0])
 
         for each_token in numerical_tokens:
             assert type(each_token) == int
+
+    def test_pad_instances(self, single_instance_setup):
+        single_instance, numericalizer, vocab = single_instance_setup
+        numerical_tokens = numericalizer.numericalize_instance(single_instance[0])
+        padded_numerical_tokens = numericalizer.pad_instance(
+            numerical_tokens, max_length=10
+        )
+        assert [isinstance(token, int) for token in padded_numerical_tokens]
+        assert padded_numerical_tokens[0] == vocab.get_idx_from_token(vocab.start_token)
+        assert padded_numerical_tokens[-1] == vocab.get_idx_from_token(vocab.pad_token)
+
+    def test_pad_batch_instances(self, single_instance_setup):
+        single_instance, numericalizer, vocab = single_instance_setup
+        numerical_tokens = numericalizer.numericalize_batch_instances(single_instance)
+        for instance in numerical_tokens:
+            assert isinstance(instance, list)
diff --git a/tests/numericalizer/test_transformer_numericalizer.py b/tests/numericalizer/test_transformer_numericalizer.py
new file mode 100644
index 0000000..9592c9e
--- /dev/null
+++ b/tests/numericalizer/test_transformer_numericalizer.py
@@ -0,0 +1,43 @@
+import pytest
+from sciwing.tokenizers.bert_tokenizer import TokenizerForBert
+from sciwing.numericalizers.transformer_numericalizer import NumericalizerForTransformer
+
+
+@pytest.fixture
+def instances():
+    return ["I  like transformers.", "I like python libraries."]
+
+
+@pytest.fixture(
+    params=[
+        "bert-base-uncased",
+        "bert-base-cased",
+        "scibert-base-uncased",
+        "scibert-base-cased",
+    ]
+)
+def numericalizer(instances, request):
+    bert_type = request.param
+    tokenizer = TokenizerForBert(bert_type=bert_type)
+    numericalizer = NumericalizerForTransformer(tokenizer=tokenizer)
+    return numericalizer
+
+
+class TestNumericalizeForTransformer:
+    def test_token_types(self, numericalizer, instances):
+        tokenizer = numericalizer.tokenizer
+        for instance in instances:
+            tokens = tokenizer.tokenize(instance)
+            ids = numericalizer.numericalize_instance(tokens)
+            assert all([isinstance(token_id, int) for token_id in ids])
+
+    @pytest.mark.parametrize("padding_length", [10, 100])
+    def test_padding_length(self, numericalizer, instances, padding_length):
+        tokenizer = numericalizer.tokenizer
+        for instance in instances:
+            tokens = tokenizer.tokenize(instance)
+            ids = numericalizer.numericalize_instance(tokens)
+            padded_ids = numericalizer.pad_instance(
+                numericalized_text=ids, max_length=padding_length
+            )
+            assert len(padded_ids) == padding_length
diff --git a/tests/utils/test_common.py b/tests/utils/test_common.py
index 42a650a..914e325 100644
--- a/tests/utils/test_common.py
+++ b/tests/utils/test_common.py
@@ -1,12 +1,5 @@
 import sciwing.constants as constants
-from sciwing.utils.common import convert_sectlabel_to_json
-from sciwing.utils.common import merge_dictionaries_with_sum
-from sciwing.utils.common import pack_to_length
-from sciwing.utils.common import convert_generic_sect_to_json
-from sciwing.utils.common import convert_parscit_to_conll
-from sciwing.utils.common import write_cora_to_conll_file
-from sciwing.utils.common import write_parscit_to_conll_file
-from sciwing.utils.common import create_class
+from sciwing.utils.common import *
 import pytest
 import pathlib
 from sciwing.engine.engine import Engine
@@ -61,6 +54,68 @@ class TestCommon:
         file_numbers = set(file_numbers)
         assert len(file_numbers) == 40  # number of files expected
 
+    def test_sectlabel_sciwing_clf_format_lines_labels_not_empty(self, tmp_path):
+        tmp_dir = tmp_path / "temp"
+        tmp_dir.mkdir()
+        try:
+            convert_sectlabel_to_sciwing_clf_format(SECTLABEL_FILENAME, tmp_dir)
+            with open(tmp_dir / "sectLabel.train", "r") as fp:
+                for line in fp:
+                    text, label = line.split("###")
+                    assert text.strip() != ""
+                    assert label.strip() != ""
+        except:
+            pytest.fail(
+                "Failed to write SectLabel file in Sciwing classification format"
+            )
+
+    def test_genericsect_sciwing_clf_format_lines_labels_not_empty(self, tmp_path):
+        tmp_dir = tmp_path / "temp"
+        tmp_dir.mkdir()
+
+        try:
+            convert_generic_sect_to_sciwing_clf_format(
+                GENERIC_SECTION_TRAIN_FILE, tmp_dir
+            )
+            with open(tmp_dir / "genericSect.train", "r") as fp:
+                for line in fp:
+                    text, label = line.split("###")
+                    assert bool(text.strip())
+                    assert bool(label.strip())
+
+            with open(tmp_dir / "genericSect.dev", "r") as fp:
+                for line in fp:
+                    text, label = line.split("###")
+                    assert bool(text.strip())
+                    assert bool(label.strip())
+
+            with open(tmp_dir / "genericSect.test", "r") as fp:
+                for line in fp:
+                    text, label = line.split("###")
+                    assert bool(text.strip())
+                    assert bool(label.strip())
+        except:
+            pytest.fail(
+                f"Failed to write GenericSect file in SciWING classification format"
+            )
+
+    @pytest.mark.parametrize("random_state", [1729, None])
+    def test_stratified_split_returns_right_split(self, random_state):
+        lines = ["a"] * 100
+        labels = ["label"] * 100
+
+        (
+            (train_lines, train_labels),
+            (dev_lines, dev_labels),
+            (test_lines, test_labels),
+        ) = get_train_dev_test_stratified_split(
+            lines=lines, labels=labels, random_state=random_state
+        )
+
+        assert len(train_lines) == len(train_labels) == 80
+        assert len(dev_lines) == len(dev_labels) == 10
+        assert len(test_lines) == len(test_labels) == 10
+
     def test_merge_dictionaries_empty(self):
         a = {}
         b = {"a": 0, "b": 1}
@@ -275,6 +330,26 @@ class TestCommon:
         else:
             parscit_train_path.unlink()
 
+    def test_parscit_sciwing_seqlabel_format_works(self, tmp_path):
+        tmp_dir = tmp_path / "temp"
+        tmp_dir.mkdir()
+
+        try:
+            convert_parscit_to_sciwing_seqlabel_format(
+                parscit_train_filepath=PARSCIT_TRAIN_FILE, output_dir=tmp_dir
+            )
+            with open(tmp_dir / "parscit.train") as fp:
+                for line in fp:
+                    word_tags = line.split()
+                    for word_tag in word_tags:
+                        word, tag = word_tag.split("###")
+                        assert bool(word.strip())
+                        assert bool(tag.split())
+        except:
+            pytest.fail(
+                f"SciWING parscit sequential labelling formatting does not work"
+            )
+
     @pytest.mark.parametrize(
         "classname, modulename", [(Engine.__name__, Engine.__module__)]
     )
diff --git a/tests/vocab/test_vocab.py b/tests/vocab/test_vocab.py
index e1b4ef1..e34a3c5 100644
--- a/tests/vocab/test_vocab.py
+++ b/tests/vocab/test_vocab.py
@@ -14,19 +14,32 @@ system_mem = int(get_system_mem_in_gb())
 
 
 class TestVocab:
-    def test_build_vocab_single_instance_has_words(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_build_vocab_single_instance_has_words(
+        self, instances, include_special_vocab
+    ):
         single_instance = instances["single_instance"]
-        vocab_builder = Vocab(instances=single_instance, max_num_tokens=1000)
+        vocab_builder = Vocab(
+            instances=single_instance,
+            max_num_tokens=1000,
+            include_special_vocab=include_special_vocab,
+        )
         vocab = vocab_builder.map_tokens_to_freq_idx()
 
         assert "i" in vocab.keys()
         assert "like" in vocab.keys()
         assert "nlp" in vocab.keys()
 
-    def test_build_vocab_single_instance_descending_order(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_build_vocab_single_instance_descending_order(
+        self, instances, include_special_vocab
+    ):
         single_instance = instances["single_instance"]
         vocab_builder = Vocab(
-            instances=single_instance, max_num_tokens=1000, min_count=1
+            instances=single_instance,
+            max_num_tokens=1000,
+            min_count=1,
+            include_special_vocab=include_special_vocab,
         )
         vocab = vocab_builder.map_tokens_to_freq_idx()
 
@@ -78,12 +91,16 @@ class TestVocab:
 
         assert vocab_len == MAX_NUM_WORDS + len(vocab_builder.special_vocab)
 
-    def test_single_instance_build_vocab(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_single_instance_build_vocab(self, instances, include_special_vocab):
         single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
+        MAX_NUM_WORDS = None
         MIN_FREQ = 1
         vocab_builder = Vocab(
-            instances=single_instance, max_num_tokens=MAX_NUM_WORDS, min_count=MIN_FREQ
+            instances=single_instance,
+            max_num_tokens=MAX_NUM_WORDS,
+            min_count=MIN_FREQ,
+            include_special_vocab=include_special_vocab,
         )
 
         vocab = vocab_builder.build_vocab()
@@ -159,10 +176,15 @@ class TestVocab:
         len_vocab = vocab_builder.get_vocab_len()
         assert len_vocab == 1 + len(vocab_builder.special_vocab)
 
-    def test_save_vocab(self, instances, tmpdir):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_save_vocab(self, instances, tmpdir, include_special_vocab):
         single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab_builder = Vocab(instances=single_instance, max_num_tokens=MAX_NUM_WORDS)
+        MAX_NUM_WORDS = None
+        vocab_builder = Vocab(
+            instances=single_instance,
+            max_num_tokens=MAX_NUM_WORDS,
+            include_special_vocab=include_special_vocab,
+        )
 
         vocab_builder.build_vocab()
         vocab_file = tmpdir.mkdir("tempdir").join("vocab.json")
@@ -262,62 +284,45 @@ class TestVocab:
         indices = sorted(indices)
         assert indices == list(range(len_indices))
 
-    def test_orig_vocab_len(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_orig_vocab_len(self, instances, include_special_vocab):
         single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 0
-        vocab_builder = Vocab(instances=single_instance, max_num_tokens=MAX_NUM_WORDS)
+        MAX_NUM_WORDS = None
+        vocab_builder = Vocab(
+            instances=single_instance,
+            max_num_tokens=MAX_NUM_WORDS,
+            include_special_vocab=include_special_vocab,
+        )
         vocab_builder.build_vocab()
         vocab_len = vocab_builder.get_orig_vocab_len()
         assert vocab_len == 3 + len(vocab_builder.special_vocab)
 
-    def test_get_topn(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_get_topn(self, instances, include_special_vocab):
         single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab_builder = Vocab(instances=single_instance, max_num_tokens=MAX_NUM_WORDS)
+        MAX_NUM_WORDS = None
+        vocab_builder = Vocab(
+            instances=single_instance,
+            max_num_tokens=MAX_NUM_WORDS,
+            include_special_vocab=include_special_vocab,
+        )
         vocab_builder.build_vocab()
         words_freqs = vocab_builder.get_topn_frequent_words(n=1)
 
         assert words_freqs[0][0] == "i"
         assert words_freqs[0][1] == 3
 
-    def test_print_stats_works(self, instances):
-        single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab_builder = Vocab(instances=single_instance, max_num_tokens=MAX_NUM_WORDS)
-        vocab_builder.build_vocab()
-        vocab_builder.print_stats()
-
-    @pytest.mark.skipif(
-        system_mem < 16, reason="Cannot loading embeddings because memory is low"
-    )
-    @pytest.mark.parametrize(
-        "embedding_type",
-        ["glove_6B_50", "glove_6B_100", "glove_6B_200", "glove_6B_300", "random"],
-    )
-    def test_load_embedding_has_all_words(self, instances, embedding_type):
-        single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab = Vocab(
-            instances=single_instance,
-            max_num_tokens=MAX_NUM_WORDS,
-            embedding_type=embedding_type,
-        )
-        vocab.build_vocab()
-        embedding = vocab.load_embedding()
-        assert embedding.size(0) == vocab.get_vocab_len()
-
-    def test_random_embeddinng_has_2dimensions(self, instances):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_print_stats_works(self, instances, include_special_vocab):
         single_instance = instances["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab = Vocab(
+        MAX_NUM_WORDS = None
+        vocab_builder = Vocab(
             instances=single_instance,
             max_num_tokens=MAX_NUM_WORDS,
-            embedding_type=None,
-            embedding_dimension=300,
+            include_special_vocab=include_special_vocab,
         )
-        vocab.build_vocab()
-        embeddings = vocab.load_embedding()
-        assert embeddings.ndimension() == 2
+        vocab_builder.build_vocab()
+        vocab_builder.print_stats()
 
     @pytest.mark.parametrize("save_vocab", [True, False])
     def test_add_token(self, instances, tmpdir, save_vocab):
@@ -328,8 +333,6 @@ class TestVocab:
         vocab = Vocab(
             instances=single_instance,
             max_num_tokens=MAX_NUM_WORDS,
-            embedding_type=None,
-            embedding_dimension=300,
             store_location=vocab_file,
         )
         vocab.build_vocab()
@@ -348,8 +351,6 @@ class TestVocab:
         vocab = Vocab(
             instances=single_instance,
             max_num_tokens=MAX_NUM_WORDS,
-            embedding_type=None,
-            embedding_dimension=300,
             store_location=vocab_file,
         )
         vocab.build_vocab()
@@ -363,18 +364,35 @@ class TestVocab:
         assert vocab.get_idx_from_token("very") == 7
         assert vocab.get_idx_from_token("much") == 8
 
-    def test_disp_sentences_from_indices(self, instances, tmpdir):
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_disp_sentences_from_indices(
+        self, instances, tmpdir, include_special_vocab
+    ):
         instance_dict = instances
         single_instance = instance_dict["single_instance"]
-        MAX_NUM_WORDS = 100
-        vocab_file = tmpdir.mkdir("tempdir").join("vocab.json")
+        MAX_NUM_WORDS = None
         vocab = Vocab(
             instances=single_instance,
             max_num_tokens=MAX_NUM_WORDS,
-            embedding_type=None,
-            embedding_dimension=300,
-            store_location=vocab_file,
+            include_special_vocab=include_special_vocab,
         )
         vocab.build_vocab()
-        sent = vocab.get_disp_sentence_from_indices([0, 1, 2, 3])
+        sent = vocab.get_disp_sentence_from_indices([0, 1, 2])
         assert type(sent) is str
+
+    @pytest.mark.parametrize("include_special_vocab", [True, False])
+    def test_max_num_tokens_unset(self, instances, include_special_vocab):
+        single_instance = instances["single_instance"]
+        MAX_NUM_WORDS = None
+        vocab = Vocab(
+            instances=single_instance,
+            max_num_tokens=MAX_NUM_WORDS,
+            include_special_vocab=include_special_vocab,
+        )
+        vocab.build_vocab()
+        assert vocab.max_num_tokens == 3 + len(vocab.special_vocab.keys())
+
+    def test_max_instance_length(self, instances):
+        single_instance = instances["single_instance"]
+        vocab_builder = Vocab(instances=single_instance, max_num_tokens=1000)
+        assert vocab_builder.max_instance_length == 100
diff --git a/tests/vocab/test_word_emb_loader.py b/tests/vocab/test_word_emb_loader.py
index aed00ee..85490f3 100644
--- a/tests/vocab/test_word_emb_loader.py
+++ b/tests/vocab/test_word_emb_loader.py
@@ -1,16 +1,18 @@
 import pytest
 from sciwing.vocab.embedding_loader import EmbeddingLoader
-from sciwing.vocab.vocab import Vocab
+import numpy as np
 import os
 from sciwing.utils.common import get_system_mem_in_gb
 
 
-@pytest.fixture()
-def setup_word_emb_loader():
-    instances = [["load", "vocab"]]
-    vocab = Vocab(instances=instances, max_num_tokens=1000)
-    vocab.build_vocab()
-    return vocab
+@pytest.fixture(
+    params=["glove_6B_50", "glove_6B_100", "glove_6B_200", "glove_6B_300", "parscit"],
+    scope="session",
+)
+def setup_word_emb_loader(request):
+    embedding_type = request.param
+    embedding_loader = EmbeddingLoader(embedding_type=embedding_type)
+    return embedding_loader
 
 
 memory_available = int(get_system_mem_in_gb())
@@ -20,48 +22,19 @@ memory_available = int(get_system_mem_in_gb())
     memory_available < 16, reason="Memory is too low to run the word emb loader tests"
 )
 class TestWordEmbLoader:
-    def test_invalid_embedding_type(self, setup_word_emb_loader):
-        vocab = setup_word_emb_loader
+    def test_invalid_embedding_type(self):
         with pytest.raises(AssertionError):
-            emb_loader = EmbeddingLoader(
-                token2idx=vocab.get_token2idx_mapping(), embedding_type="notexistent"
-            )
-
-    @pytest.mark.parametrize(
-        "embedding_type",
-        ["glove_6B_50", "glove_6B_100", "glove_6B_200", "glove_6B_300", "parscit"],
-    )
-    def test_preloaded_file_exists(self, setup_word_emb_loader, embedding_type):
-        vocab = setup_word_emb_loader
+            loader = EmbeddingLoader(embedding_type="nonexistent")
 
-        emb_loader = EmbeddingLoader(
-            token2idx=vocab.get_token2idx_mapping(), embedding_type=embedding_type
-        )
+    def test_preloaded_file_exists(self, setup_word_emb_loader):
+        emb_loader = setup_word_emb_loader
         preloaded_filename = emb_loader.get_preloaded_filename()
 
         assert os.path.isfile(preloaded_filename)
 
-    @pytest.mark.parametrize(
-        "embedding_type",
-        [
-            "glove_6B_50",
-            "glove_6B_100",
-            "glove_6B_200",
-            "glove_6B_300",
-            "parscit",
-            "random",
-        ],
-    )
-    def test_all_vocab_words_have_embedding(
-        self, setup_word_emb_loader, embedding_type
-    ):
-        vocab = setup_word_emb_loader
-        emb_loader = EmbeddingLoader(
-            token2idx=vocab.get_token2idx_mapping(), embedding_type=embedding_type
-        )
-
-        vocab_embedding = emb_loader.vocab_embedding
-
-        words = vocab_embedding.keys()
+    def test_embeddings_are_np_arrays(self, setup_word_emb_loader):
 
-        assert len(words) == vocab.get_vocab_len()
+        emb_loader = setup_word_emb_loader
+        if emb_loader.embedding_type != "parscit":
+            for word, embedding in emb_loader._embeddings.items():
+                assert isinstance(embedding, np.ndarray)
